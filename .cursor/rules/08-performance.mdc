---
description: Performance optimization guidelines
globs:
  - "src/**/*.py"
  - "tests/benchmark/**/*.py"
  - "tests/performance/**/*.py"
alwaysApply: false
---

# Performance Optimization Guidelines

## Performance Philosophy
1. **Correctness first**: Working code before fast code
2. **Measure, don't guess**: Profile before optimizing
3. **Focus on bottlenecks**: Optimize what matters
4. **Maintain readability**: Clear code is maintainable code

## Profiling

### Tools
```python
# CPU profiling with cProfile
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()
# ... code to profile ...
profiler.disable()

stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # Top 20 functions
```

```python
# Memory profiling with memory_profiler
from memory_profiler import profile

@profile
def memory_intensive_function():
    # ... code ...
    pass
```

```bash
# Line profiling with line_profiler
kernprof -l -v script.py
```

### Benchmarking
Use pytest-benchmark for consistent benchmarks:

```python
import pytest

@pytest.mark.benchmark
def test_search_performance(benchmark):
    """Benchmark search query performance."""
    engine = SearchEngine()
    result = benchmark(engine.search, "test query")
    assert len(result) > 0
```

## Optimization Strategies

### Algorithmic Optimization
- Choose appropriate data structures
- Reduce time complexity where possible
- Consider space-time trade-offs
- Use caching for expensive operations

Example:
```python
# ❌ O(n²) for membership testing
def slow_lookup(items: list[str], targets: list[str]) -> list[bool]:
    return [item in items for item in targets]

# ✅ O(n) with set
def fast_lookup(items: list[str], targets: list[str]) -> list[bool]:
    item_set = set(items)
    return [item in item_set for item in targets]
```

### Generator Usage
Use generators for large datasets:

```python
# ❌ Loads all results into memory
def get_all_results(query: str) -> list[dict]:
    return [process(item) for item in fetch_items(query)]

# ✅ Yields results one at a time
def stream_results(query: str) -> Iterator[dict]:
    for item in fetch_items(query):
        yield process(item)
```

### Caching

#### Function-Level Caching
```python
from functools import lru_cache, cache

@lru_cache(maxsize=128)
def expensive_computation(x: int) -> int:
    """Cache results for frequently called function."""
    return sum(i ** 2 for i in range(x))

@cache  # Unbounded cache for Python 3.9+
def immutable_lookup(key: str) -> str:
    """Cache all results."""
    return fetch_from_database(key)
```

#### Application-Level Caching
```python
from cachetools import TTLCache

class SearchEngine:
    def __init__(self):
        self._cache = TTLCache(maxsize=1000, ttl=300)  # 5 min TTL
    
    def search(self, query: str) -> list[dict]:
        if query in self._cache:
            return self._cache[query]
        
        results = self._execute_search(query)
        self._cache[query] = results
        return results
```

### Database Optimization

#### Batch Operations
```python
# ❌ Multiple individual operations
for item in items:
    db.insert(item)

# ✅ Single batch operation
db.insert_many(items)
```

#### Connection Pooling
```python
from contextlib import contextmanager

class DatabasePool:
    def __init__(self, max_connections: int = 10):
        self._pool = create_pool(max_connections)
    
    @contextmanager
    def get_connection(self):
        conn = self._pool.get()
        try:
            yield conn
        finally:
            self._pool.put(conn)
```

#### Query Optimization
```python
# ❌ N+1 queries
users = db.query("SELECT * FROM users")
for user in users:
    posts = db.query(f"SELECT * FROM posts WHERE user_id = {user.id}")

# ✅ Single join query
results = db.query("""
    SELECT users.*, posts.*
    FROM users
    LEFT JOIN posts ON posts.user_id = users.id
""")
```

### I/O Optimization

#### Async I/O
```python
import asyncio
from typing import List

# ❌ Sequential I/O
def fetch_all_sequential(urls: list[str]) -> list[str]:
    return [fetch(url) for url in urls]

# ✅ Concurrent I/O
async def fetch_all_concurrent(urls: list[str]) -> list[str]:
    tasks = [fetch_async(url) for url in urls]
    return await asyncio.gather(*tasks)
```

#### Buffering
```python
# ❌ Unbuffered writes
for line in lines:
    file.write(line + "\n")

# ✅ Buffered writes
with open("file.txt", "w", buffering=8192) as f:
    f.writelines(line + "\n" for line in lines)
```

### Memory Optimization

#### Avoid Memory Leaks
```python
# ❌ Circular reference
class Node:
    def __init__(self):
        self.children = []
        self.parent = None  # Creates cycle

# ✅ Use weak references
import weakref

class Node:
    def __init__(self):
        self.children = []
        self._parent = None
    
    @property
    def parent(self):
        return self._parent() if self._parent else None
    
    @parent.setter
    def parent(self, value):
        self._parent = weakref.ref(value) if value else None
```

#### Lazy Loading
```python
class Document:
    def __init__(self, doc_id: str):
        self.doc_id = doc_id
        self._content = None  # Lazy load
    
    @property
    def content(self) -> str:
        if self._content is None:
            self._content = self._load_content()
        return self._content
```

### Parallel Processing

#### CPU-Bound Tasks
```python
from multiprocessing import Pool

def process_items_parallel(items: list[Any]) -> list[Any]:
    with Pool() as pool:
        return pool.map(process_item, items)
```

#### With Ray (Distributed)
```python
import ray

@ray.remote
def process_item(item: Any) -> Any:
    return expensive_computation(item)

def process_items_distributed(items: list[Any]) -> list[Any]:
    futures = [process_item.remote(item) for item in items]
    return ray.get(futures)
```

## Performance Anti-Patterns

### Premature Optimization
```python
# ❌ Overly complex optimization without measurement
def hyper_optimized_function():
    # 200 lines of micro-optimizations
    pass

# ✅ Clear code first, optimize if needed
def clear_function():
    # Simple, readable implementation
    pass
```

### Micro-Optimization Over Algorithm
```python
# ❌ Optimizing a bad algorithm
for i in range(len(items)):  # O(n²)
    for j in range(len(items)):
        if items[i] == items[j]:
            duplicates.append(items[i])

# ✅ Better algorithm
seen = set()
duplicates = [x for x in items if x in seen or seen.add(x)]
```

## Performance Testing

### Benchmark Tests
```python
@pytest.mark.benchmark(group="search")
def test_search_small_corpus(benchmark):
    """Benchmark with small corpus."""
    result = benchmark(search_engine.search, "query", corpus_size=1000)
    assert benchmark.stats["mean"] < 0.1  # Under 100ms

@pytest.mark.benchmark(group="search")
def test_search_large_corpus(benchmark):
    """Benchmark with large corpus."""
    result = benchmark(search_engine.search, "query", corpus_size=1_000_000)
    assert benchmark.stats["mean"] < 1.0  # Under 1 second
```

### Performance Regression Tests
```python
def test_no_performance_regression(baseline_results):
    """Ensure performance doesn't degrade."""
    current = measure_performance()
    baseline = baseline_results["search_performance"]
    
    # Allow 10% degradation
    assert current <= baseline * 1.1
```

## Monitoring

### Logging Performance Metrics
```python
import time
import logging

logger = logging.getLogger(__name__)

def timed(func):
    """Decorator to log function execution time."""
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        try:
            return func(*args, **kwargs)
        finally:
            duration = time.perf_counter() - start
            logger.info(
                f"{func.__name__} took {duration:.3f}s",
                extra={"function": func.__name__, "duration": duration}
            )
    return wrapper
```

### Performance Dashboards
- Track key metrics over time
- Monitor query latency percentiles (p50, p95, p99)
- Track memory usage
- Monitor cache hit rates
- Alert on performance degradation

## Best Practices Summary

1. **Profile first**: Measure before optimizing
2. **Optimize algorithms**: Choose the right data structure
3. **Cache wisely**: Cache expensive, deterministic operations
4. **Use async**: For I/O-bound operations
5. **Parallelize**: For CPU-bound, independent tasks
6. **Stream data**: Use generators for large datasets
7. **Batch operations**: Reduce overhead
8. **Test performance**: Regression tests prevent degradation
9. **Monitor production**: Track real-world performance
10. **Document**: Explain optimization decisions and trade-offs
