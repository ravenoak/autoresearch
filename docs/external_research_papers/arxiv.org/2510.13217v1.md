# Citation Key: 2510.13217v1

---

## LLM- GUIDED H IERARCHICAL R ETRIEVAL

**Nilesh Gupta** [♠][♣] **Wei-Cheng Chang** [♣] **Ngot Bui** [♣] **Cho-Jui Hsieh** [♦] **Inderjit S. Dhillon** [♠][♣]
♠ UT Austin ♦ UCLA ♣ Google
� [https://github.com/nilesh2797/lattice](https://github.com/nilesh2797/lattice)


A BSTRACT


Modern IR systems are increasingly tasked with answering complex, multi-faceted
queries that require deep reasoning rather than simple keyword or semantic matching. While LLM based IR has shown great promise, the current retrieve-then-rerank
paradigm inherits the limits of embedding-based retrieval, parametric generative approaches are difficult to adapt to new information, and long-in-context approaches
that put the entire corpus in context are computationally infeasible for large document corpora. To this end, we introduce a hierarchical retrieval framework
LATTICE that enables an LLM to reason and navigate a large corpus with search
complexity that is logarithmic in the number of documents, achieved by imposing
a semantic tree structure on the corpus. Our approach comprises two stages: (1) an
offline process where we organize the document collection into a semantic hierarchy – we explore two LLM-driven strategies for this, a bottom-up agglomerative
approach and a top-down divisive approach using multi-level summaries; and (2)
an online traversal stage where a "search LLM" navigates this tree. A central
challenge in using LLMs for this search is that the LLM’s relevance judgments
are _noisy, context-dependent, and unaware of the underlying hierarchy_, making
it difficult to compare nodes across different branches and levels of the tree. To
solve this, our traversal algorithm estimates calibrated latent relevance scores from
the LLM’s local outputs, which are combined into a path relevance metric to guide
the search globally across the tree. Our training-free framework achieves state-ofthe-art zero-shot performance on the reasoning-intensive BRIGHT (Su et al., 2024)
benchmark (with up to 420K corpus size), demonstrating improvements of up to 9%
in Recall@100 and 5% in nDCG@10 over the next zero-shot baseline. Moreover,
compared to the highly specialized and fine-tuned SOTA method DIVER-v2 (Long
et al., 2025), it achieves comparable results on BRIGHT subsets that use a static
corpus for evaluation.





![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-0-3.png)














































|40<br>30<br>nDCG@10<br>20<br>10|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|10<br>20<br>30<br>40<br>nDCG@10|||||||
|10<br>20<br>30<br>40<br>nDCG@10|||||||
|10<br>20<br>30<br>40<br>nDCG@10||||~~LAT~~<br>BM<br>Rea|~~TICE~~<br>25 rerank<br>sonIR~~-~~8B reran|k|




avg. number of input tokens given to LLM) vs. ranking quality (nDCG@10) on the Robotics subset. Reranking
baselines (BM25+rerank, ReasonIR-8B+rerank) with varying top-k shortlist use same Gemini-2.5-flash as
reranker exhibit early gains but quickly plateau. LATTICE starts with a shallow flat region (cost of traversing
tree levels) but then scales more effectively—surpassing the baselines and continuing to improve to a higher
final nDCG—demonstrating that guided hierarchical traversal using LLM can be more compute efficient.


1


LLM-guided Tree Traversal (LATTICE)


Search LLM



Query



![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-1-3.png)


```
How to do simple takeoff

then do yaw rotation: ..

```


Cluster LLM


Convert to Hierarchical


Semantic Tree



Offline Tree Construction


ROOT















Traverse Tree on User


Query







Database











Figure 2: A high-level overview of our proposed framework, LATTICE. The process consists of two
stages. (Left) In the offline stage, we organize an unstructured document corpus into a semantic
tree. (Right) In the online stage, a search LLM performs a best-first traversal over calibrated path
relevance scores to find documents relevant to a user query. The path relevance score is defined as the
exponentially moving average of calibrated scores of nodes on the path. Score calibration is achieved
by comparing nodes against high-relevance candidates from sibling branches and previously seen
leaves, ensuring a globally coherent search.


1 I NTRODUCTION


The proliferation of Large Language Models (LLMs) has catalyzed a paradigm shift in Information
Retrieval (IR), moving beyond simple fact-finding towards complex problem solving that demands
nuanced understanding and reasoning. Modern user queries often require not just keyword or semantic
matching, but a deeper level of inference, categorized as reasoning-based retrieval (Su et al., 2024).
For instance, a user might seek a solution to a coding bug by describing its behavior, or ask for math
problems that require applying a specific theorem. Answering such queries effectively requires
retrieval of documents that help reason _through_ the problem, a task for which traditional IR systems
are poorly equipped.


Current LLM-based IR systems primarily fall into three paradigms, each with inherent drawbacks.
The first, _Retrieve-then-Rerank_, employs a computationally cheap retriever (e.g., BM25 or dense
retrieval) to fetch a broad set of candidate documents, which are then re-ordered by a more powerful
but expensive LLM. Although scalable, this approach is constrained with the _limits of the initial_
_retrieval stage_ (Weller et al., 2025); if a crucial document is not captured in the initial candidate set,
even a perfect reranker cannot recover it. Furthermore, the initial retrieval often relies on shallow
semantic similarity, failing to perform the multi-step reasoning needed to identify relevant documents
for complex queries. The second paradigm, _Generative Retrieval (GenIR)_, uses the LLM itself to
synthesize an answer. This can be _parametric_ (Tay et al., 2022), where the corpus is stored implicitly
in the model weights, making the system prone to hallucinations and difficult to update with new
information. Alternatively, _long-context GenIR_ (Lee et al., 2024a) places the entire corpus (or a
large subset) explicitly into the LLM’s context. While this allows the LLM to reason over the full
text, it is computationally infeasible for a typical retrieval corpora, as the self-attention mechanism’s
quadratic/super-linear complexity leads to prohibitive costs and latency.


To this end, we propose an LLM-guided hierarchical retrieval framework **LATTICE**, a framework that
combines the logarithmic search efficiency of hierarchical structures with the sophisticated reasoning
capabilities of modern LLMs. Our method first organizes a document corpus into a semantic tree
offline, with internal nodes represented by rich, LLM-generated textual summaries. Then, at query
time, a search LLM navigates this semantic hierarchy using a greedy, best-first traversal, processing a
beam of top candidates at each step. To ensure the search remains globally coherent, the traversal
algorithm computes a path relevance score for each node by aggregating calibrated local scores from
the LLM along the path from the root, allowing our method to robustly compare nodes across different
branches and levels and efficiently reach the most relevant documents. Our main contributions are:


 - We introduce a novel retrieval framework where an LLM directly performs the traversal of a
semantic hierarchy, using its reasoning capabilities to guide the search path at each step, achieving
state-of-the-art zero-shot results on the reasoning-intensive BRIGHT benchmark with improvements of up to 9% in Recall@100 and 5% in nDCG@10.


2


 - We propose a robust LLM-guided search algorithm that reliably performs greedy search on a
semantic tree using noisy LLM judgments.


 - We design and compare two distinct LLM-driven strategies for corpus organization: a bottom-up
agglomerative clustering method and a top-down divisive summarization approach.



![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-2-0.png)



2 R ELATED W ORK


2.1 LLM S FOR I NFORMATION R ETRIEVAL


**Retrieve-then-Rerank Paradigm.** The dominant paradigm in modern IR is a two-stage retrievethen-rerank pipeline (Zhu et al., 2023). LLMs have excelled as powerful rerankers in this framework,
applied in either pointwise (score each document independently) or listwise fashion (rank a list of
documents) (Reddy et al., 2024; Sun et al., 2024). However, the overall performance is irreversibly
bottlenecked by the quality of the initial retrieval stage (Rathee et al., 2025). In the retrieval stage,
LLMs are increasingly used as backbones for dense embedding models (Luo et al., 2024; Lee et al.,
2025), though this often involves adapting their autoregressive architecture for representation learning
which is not directly aligned with their pre-training task.


**Generative Paradigms.** To overcome the limitations of the cascading pipeline, alternative
paradigms have emerged. **Generative Retrieval**, such as the Differentiable Search Index (DSI) (Tay
et al., 2022; Li et al., 2024), reframes IR as a sequence-to-sequence task, mapping a query directly
to a document identifier. While conceptually elegant, these methods face challenges in scaling and
updating the index (Pradeep et al., 2023). **Long-Context Retrieval** proposes placing the entire corpus
into the LLM’s context window (Lee et al., 2024a), but this remains computationally infeasible for
even moderate-scale applications. Our work offers a middle ground by using a semantic hierarchy
to structure the corpus, thus enabling an LLM to navigate it efficiently without the scalability /
updatability issues of generative retrieval or the computational cost of long-context models.


2.2 H IERARCHICAL R ETRIEVAL


**Vector Hierarchies.** Hierarchical structures have been long used to improve computational efficiency in tasks with large output spaces, such as hierarchical softmax for language modeling (Morin
& Bengio, 2005) and tree-based methods for extreme multi-label classification (Prabhu & Varma,
2014; Yu et al., 2022; Gupta et al., 2022). In vector search, algorithms such as Hierarchical Navigable
Small World (HNSW) (Malkov & Yashunin, 2018) use a multilevel graph for an efficient approximate
nearest-neighbor search, although this hierarchy is geometric rather than semantic.


**Textual Hierarchies.** More recently, models like RAPTOR (Sarthi et al., 2024) construct a semantic
hierarchy by recursively clustering and summarizing text chunks from the bottom up. This creates a
tree with nodes representing different levels of abstraction. However, RAPTOR relies on conventional
embedding-based similarity search to traverse this tree. Our work differs fundamentally by employing
an LLM as an _active traversal agent_ during the online retrieval phase. Instead of a static vector
comparison, our model uses in-context reasoning at each node to decide the optimal path, transforming
the retrieval into an intelligent navigation process.


2.3 A GENTIC AND R EASONING - BASED IR


**Reasoning as a Pre-processing Step.** A common approach to incorporate reasoning in IR is
through query expansion (QE) (Wang et al., 2023; Gao et al., 2023). In this setup, an LLM enriches
the query with generated text or a chain-of-thought analysis before it is passed to a standard retrieval
system. While effective, this treats reasoning as a discrete pre-retrieval step, leaving the core search


3


User Query R Relevance 1 Search LLM



Relevance
1



![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-3-2.png)























0

|Path: (6, 2, 1)<br>5 6 7 P8ath Relevance<br>Calibrated Rele<br>Local Relevanc<br>Child Relevanc<br>Description:<br>Provides compre<br>facilitates 3D spa<br>4 5 6 7 0 1 2 o3f 4x4 homogen<br>for converting be<br>for key functions<br>Children nodes:<br>Reasoning:<br>The user is attem<br>drone control scr<br>2 13 14 15 16 17 18 19 20 0 1 2 3 4 5 0 1 c2orrectly apply y<br>- **Candidate 0**<br>"robust support f<br>angles, with func<br>explicitly uses `q<br>rotation. This do<br>0 0 - 1**Candidate 1**<br>description focus<br>and `translation_<br>rotation, which is<br>- **Candidate 2**<br>`tf.transformation<br>representations<br>012345678910 11 12 13 14 15012345678910 11 12 13 14 `1q5uaternion_from<br>parent node that|Col2|Search LLM|
|---|---|---|
|5<br>6<br>7<br>8<br>4<br>5<br>6<br>7<br>0<br>1<br>2<br>3<br>2<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>0<br>1<br>2<br>3<br>4<br>5<br>0<br>1<br>2<br><br>0<br>0<br>1<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10 11 12 13 14 15<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10 11 12 13 14 15<br>**Path:** (6, 2, 1)<br>**Path Relevance**<br>**Calibrated Rele**<br>**Local Relevanc**<br>**Child Relevanc**<br>**Description:**<br>Provides compre<br>facilitates 3D spa<br>of 4x4 homogen<br>for converting be<br>for key functions<br>Children nodes:<br>**Reasoning:**<br>The user is attem<br>drone control scr<br>correctly apply y<br>- **Candidate 0**<br>"robust support f<br>angles, with func<br>explicitly uses `q<br>rotation. This do<br>- **Candidate 1**<br>description focus<br>and `translation_<br>rotation, which is<br>- **Candidate 2**<br>`tf.transformation<br>representations<br>`quaternion_from<br>parent node that|**:** 0<br>**va**<br>**e:** <br>**e:** 0<br>he<br>tia<br>eou<br>tw<br> lik<br>0,<br>pt<br>ipt<br>aw<br>: T<br>or<br>tio<br>uat<br>cum<br>: T<br>es<br>ma<br> th<br>: T<br>s`<br>suc<br>_m<br> en|0.2<br>0.4<br>0.6<br>0.8<br>.891<br>**nce:** 0.794<br>0.950<br>: 0.95; 1: 0.5<br>nsive guides and API reference documentation for the `tf.transformations` Python library, which<br>l transformations using `Numpy`. This cluster covers the creation, manipulation, and decomposition<br>s transformation matrices for operations like rotation, translation, and scaling. It details functions<br>een rotation representations such as matrices, quaternions, and Euler angles, and includes reference<br>e `quaternion_from_matrix`, `scale_matrix`, `superimposition_matrix`, and `compose_matrix`.<br>16<br>ing to perform yaw rotation using `quaternion_from_euler` from the `tf.transformations` library in a<br>and is seeking guidance on how to do so correctly. The essential problem is understanding how to<br>rotation through Euler to quaternion conversion.<br>his document is highly relevant because it directly describes the `tf.transformations` library's<br>converting between different 3D rotation representations, including matrices, quaternions, and Euler<br>ns like `euler_from_matrix`, `quaternion_from_matrix`, and `quaternion_matrix`." The user's code<br>ernion_from_euler`, which is precisely a function for converting Euler angles to quaternions for<br>ent provides critical concepts and theories on the exact mechanism the user is struggling with.<br>his document is less relevant. While it pertains to the `tf.transformations` library, its<br> on general 3D geometric transformations and vector math, mentioning functions like `scale_matrix`<br>trix`. It does not specifically highlight the conversion between Euler angles and quaternions for<br>e core of the user's query.<br>his document is also highly relevant. It serves as a comprehensive guide and API reference for the<br>library and explicitly states that it "details functions for converting between rotation<br>h as matrices, quaternions, and Euler angles, and includes reference for key functions like<br>atrix`." This directly addresses the user's use of `quaternion_from_euler` for yaw rotation. As a<br>compasses the content of node 0, it offers a broader context and foundational understanding.|


Figure 3: An illustration of the search process of LATTICE for a real query from the BRIGHT
benchmark. The color of each node corresponds to its computed path relevance; the highlighted
yellow path shows the path to ground-truth documents. The search LLM makes a step-by-step
decision at each internal node to determine which branch to explore next. The expanded callout
provides a "glass box" view into one such decision, detailing the LLM’s explicit reasoning process as
it scores the child nodes.


mechanism unchanged, often resulting in complex multi-component pipelines (Long et al., 2025;
Shao et al., 2025) and ill-suited for lengthy and complex queries.


**Agentic Frameworks.** The emerging field of Agentic IR (Jin et al., 2025; Zhang et al., 2024)
conceptualizes retrieval as a multi-step, goal-oriented process. However, current implementations
typically involve an LLM agent calling an external, black-box search tool, making its success
contingent on the tool’s effectiveness. Similarly, Graph-RAG (Edge et al., 2024; Zhang et al., 2025)
leverages LLMs to reason over pre-structured knowledge graphs, but the role of LLMs to retrieve
information from these graphs are limited. Our work integrates the reasoning agent more deeply into
the retrieval process itself. The LLM is not just a pre-processor or a tool-caller but the core search
mechanism, more specifically, is an agent whose environment is the corpus’ semantic tree. The tree
provides essential scaffolding, constraining the agent’s action space to make the search tractable,
while the agent’s reasoning enables intelligent traversal decisions, offering a more fundamental fusion
of reasoning and retrieval.


3 M ETHODOLOGY


We begin by formalizing the task setup and notations in Section 3.1, followed by a detailed description
of the search procedure in Section 3.2, and ending with the tree construction procedures in Section 3.3.


3.1 S ETUP


The fundamental task is **retrieval** : given a large corpus of _|D|_ documents, _D_ = _{d_ 1 _, d_ 2 _, . . ., d_ _|D|_ _}_,
and a complex natural language query _q_, the objective is to retrieve a ranked list of documents
_D_ _rel_ _⊆_ _D_ . We define the core components and notations of our framework as follows:


- **Semantic Tree:** The corpus is organized into a tree _T_ = ( _V, E_ ), with a single root node, _v_ _root_ .

- **Nodes (** _v ∈_ _V_ **):** The set of nodes _V_ is partitioned into leaf nodes _V_ _L_ (corresponding to documents)
and internal nodes _V_ _I_ (representing document clusters).

- **Edges (** _E_ **):** The set of directed edges _E ⊂_ _V × V_ consists of ordered pairs ( _u, v_ ), where
_u_ = parent( _v_ ). The set of immediate children of a node _u_ is denoted as _C_ ( _u_ ).

- **Node Representation (** _ϕ_ ( _v_ ) **):** Every node _v ∈_ _V_ has a textual representation _ϕ_ ( _v_ ) . For _v_ _l_ _∈_ _V_ _L_,
_ϕ_ ( _v_ _l_ ) is its document’s content. For _v_ _i_ _∈_ _V_ _I_, _ϕ_ ( _v_ _i_ ) is an LLM-generated summary of its children.


4


- **Search LLM (** _L_ **):** For the purpose of this paper we assume that the search LLM can be abstracted
out as a **listwise scoring function** . Given a query _q_ and a list of _k_ candidate nodes [ _v_ 1 _, . . ., v_ _k_ ], it
returns a list of real-valued scores (along with a reasoning trace):


_L_ ( _q,_ [ _ϕ_ ( _v_ 1 ) _, . . ., ϕ_ ( _v_ _k_ )]) = [ _s_ 1 _, . . ., s_ _k_ ]


where _s_ _i_ _∈_ [0 _,_ 1] _, i_ = 1 _, . . ., k_ . A higher score implies higher preference. The prompt used in our
experiments to prompt an LLM as _L_ is detailed in Figure 7.


3.2 O NLINE LLM- GUIDED H IERARCHICAL S EARCH


The core challenge in using an LLM for hierarchical search is that its relevance judgments are
inherently noisy, context-dependent and unaware of the underlying hierarchy. The score assigned to a
node depends on the query as well as on the other nodes present in the list of options provided to the
LLM. On top of this, these scores are inherently noisy due to the non-deterministic reasoning chain /
inference of LLMs. This makes it difficult to compare the promise of a node in one branch against a
node in a completely different branch or at a different level of the tree. Given a search query, the goal
of our traversal algorithm is to prioritize the exploration of relevant nodes in the tree by predicting
a **path relevance score**, ˆ _p_ _rel_ ( _v_ ), which converts these noisy, local signals into a globally coherent
signal. The algorithm, depicted in Figure 2 and formalized in Algorithm 1, proceeds in following
steps.


**1. Initialization.** The search begins with a max-priority queue, the **frontier (** _F_ **)**, which is initialized
with the root node _v_ _root_ . Its score is set to ˆ _p_ _rel_ ( _v_ _root_ ) _←_ 1 _._ 0 . We also initialize an empty **prediction**
**set (Pred)** to store candidate leaf nodes and a history of all observed scores, ScoreHistory _←∅_ .


**2. Beam Expansion.** The search runs for _N_ iterations, where in each iteration we expand (i.e.
evaluate the children nodes of the chosen node) a beam of the top _B_ most promising nodes from the
frontier _F_ . These nodes are selected based on their current path relevance scores ˆ _p_ _rel_ .


**3. Slate Construction with Calibration.** For each node _v_ in the beam, we construct a slate for the
search LLM to evaluate. This slate consists of the children of the current node _C_ ( _v_ ), augmented with
a set _Aug_ ( _v_ ). The composition of _Aug_ ( _v_ ) depends on the type of nodes being evaluated:


    - If _C_ ( _v_ ) are **internal nodes**, _Aug_ ( _v_ ) consists of the highest scoring sibling of _v_ to provide a
cross-reference between different branches.

    - If _C_ ( _v_ ) are **leaf nodes**, _Aug_ ( _v_ ) consists of _ℓ_ (a hyperparameter) leaf nodes sampled from
Pred according to a probability distribution proportional to _e_ _[p]_ [ˆ] _[rel]_ [(] _[u]_ [)], anchoring the evaluation
against the best candidates found so far and giving a chance for best scoring candidates to
be evaluated again in a different context. In Figure 4, we show that this is essential for the
final ranking.


**4. Latent Score Estimation and Path Relevance Update.** After the search LLM _L_ evaluates the
slate and produces local scores, we perform a global calibration step before updating path relevance.
We model the observed score _s_ _[i]_ _v_ [for a node] _[ v]_ [ in a given slate] _[ i]_ [ as a linear transformation of an]
underlying, slate-independent **latent relevance score** ˆ _s_ _v_ :


_s_ _[i]_ _v_ _[≈]_ _[a][ ·]_ [ ˆ] _[s]_ _[v]_ [+] _[ b]_ _[i]_


where _a_ is a single global scale parameter and _b_ _[i]_ is a per-slate bias parameter. After each new slate is
evaluated, we update our estimates for all latent scores _{s_ ˆ _v_ _}_, _a_, and biases _{b_ _[i]_ _}_ by treating this as a
Maximum Likelihood Estimation (MLE) problem. We find the parameters that minimize the Mean
Squared Error (MSE) across all scores observed thus far:



� ( _s_ _[i]_ _v_ _[−]_ [(] _[a][ ·]_ [ ˆ] _[s]_ _[v]_ [+] _[ b]_ _[i]_ [))] [2] _[.]_

_v∈_ slate _[i]_



min
_a,{s_ ˆ _v_ _},{b_ _[i]_ _}_



�


_i_



Note that without the _a, b_ _[i]_ parameters ˆ _s_ _v_ reduces to the mean of all the scores seen so far for node
_v_ in ScoreHistory, we notice improved performance with this formulation as it can account for
noise in scoring. While other objectives such as margin-based losses or probabilistic models like


5


**Algorithm 1** LLM-guided Hierarchical Search


1: **Parameters:** _q, T, L, B, N, K, α_
2: **Initialize:**
3: Frontier _F ←_ new MaxPriorityQueue(), Pred _←∅_
4: ScoreHistory _←∅_, LatentScores _←∅_
5: ˆ _p_ _rel_ ( _v_ _root_ ) _←_ 1 _._ 0, _F._ push( _v_ _root_ _,_ ˆ _p_ _rel_ ( _v_ _root_ ))
6: **for** _i_ = 1 **to** _N_ **do**
7: Beam _←_ Extract top _B_ nodes from _F_
8: **for all** _v_ in Beam **do**
9: Slate _←_ _C_ ( _v_ ) + _Aug_ ( _v_ )
10: LocalScores [ _s_ _v_ _′_ ] _v_ _′_ _∈_ Slate _←L_ ( _q,_ [ _ϕ_ ( _v_ _[′]_ )] _v_ _′_ _∈_ Slate )
11: Add _{_ (slate_id _i_ _, v_ _[′]_ _, s_ _v_ _′_ ) _| v_ _[′]_ _∈_ Slate _}_ to ScoreHistory
12: **end for**
13: LatentScores _←_ SolveMLE(ScoreHistory) {Minimize MSE to find all ˆ _s_ _v_ }
14: **for all** _v_ in Beam that were just expanded **do**
15: **for all** _v_ _[′]_ in Slate **do**

ˆ
16: _s_ _v_ _′_ _←_ LatentScores[ _v_ _[′]_ ]

ˆ
17: _p_ _rel_ ( _v_ _[′]_ ) _←_ _α ·_ ˆ _p_ _rel_ (parent( _v_ _[′]_ )) + (1 _−_ _α_ ) _·_ ˆ _s_ _v_ _′_
18: **end for**
19: **for all** _v_ _[′]_ in _C_ ( _v_ ) **do**
20: **if** _v_ _[′]_ is a leaf node **then**
21: Add _v_ _[′]_ to Pred

22: **else**
23: _F._ push( _v_ _[′]_ _,_ ˆ _p_ _rel_ ( _v_ _[′]_ ))
24: **end if**

25: **end for**

26: **end for**

27: **end for**
28: **return** Top- _K_ nodes from Pred sorted by ˆ _p_ _rel_


Plackett-Luce could be applied, we found the simple modified MSE optimization to be the most
consistent. The resulting latent score ˆ _s_ _v_ is used to define the path relevance:


ˆ
_p_ _rel_ ( _v_ ) = _α ·_ ˆ _p_ _rel_ (parent( _v_ )) + (1 _−_ _α_ ) _·_ ˆ _s_ _v_


Here _α_ is a hyperparameter in [0 _,_ 1] . After scoring, the newly evaluated internal nodes are added to
the frontier _F_, and leaf nodes are added to the prediction set Pred.


**5. Termination.** The algorithm terminates after _N_ iterations. The final output is the set of top- _K_
documents from Pred, ranked by their final path relevance scores.


3.3 O FFLINE T REE C ONSTRUCTION


The objective is to create a tree _T_ = ( _V, E_ ) where every leaf node _v ∈_ _V_ _L_ is connected to the
root node _v_ _root_ via a single path and each node _v ∈_ _V_ is annotated with a textual representation
_ϕ_ ( _v_ ) . The maximum branching factor of any node is constrained by a hyperparameter _M_, i.e.,
_|C_ ( _v_ ) _| ≤_ _M ∀_ _v ∈_ _V_ . While our traversal algorithm can be adapted for more general Directed
Acyclic Graph (DAG) structures, we focus on a tree for simplicity. We now describe our bottom-up
construction approach, which is conceptually similar to recursive clustering and summarization
methods like RAPTOR (Sarthi et al., 2024).


3.3.1 A PPROACH 1: B OTTOM -U P C LUSTERING AND S UMMARIZATION


This approach constructs the tree layer by layer, starting from the leaf nodes and iteratively clustering
and summarizing them until a single root node is formed. To do this, we require two main components:


    - An **embedding function**, _E_ : text _→_ R _[d]_, which maps a textual representation _ϕ_ ( _v_ ) to a
_d_ -dimensional vector. We use Gecko embeddings (Lee et al., 2024b) in our experiments.


6


    - A **clustering function**, _C_ . Given a set of _n_ vectors _X_ = _{_ **x** 1 _, . . .,_ **x** _n_ _}_, the function
produces a partition _{K_ 1 _, . . ., K_ _m_ _}_ of _X_, such that for all _j ∈{_ 1 _, . . ., m}_, _|K_ _j_ _| ≤_ _M_ and
_K_ _i_ _∩_ _K_ _j_ = _∅_ for _i ̸_ = _j_ . This can be implemented via iterative application of standard
clustering algorithms like spectral clustering.


The construction process, formalized in Algorithm 2, proceeds as follows:


**1. Initial Layer Formation.** The process begins with the set of leaf nodes, _V_ _L_ . We form an initial
set of parent nodes, _V_ current, one level above the leaves. This can be done in two ways:


    - **From Scratch:** Apply the embedding and clustering functions to all documents to form the
initial parent nodes.

    - **Using Metadata:** For datasets where documents are passages from a smaller set of source
articles (stackexchange sub-datasets in BRIGHT), we leverage this inherent structure. We
form initial clusters by grouping all passages belonging to the same source document. If
any of the resulting cluster contains more than _M_ passages, we further group nodes in the
cluster based on location proximity in the source document until all sub-clusters satisfy the
branching factor constraint. This metadata-driven approach often yields more semantically
coherent initial groupings. Further implementation details are provided in Appendix B.3.


**2. Iterative Clustering and Summarization.** Starting with the initial set of parent nodes, _V_ current,
we iteratively repeat a summarize-embed-cluster cycle. In each iteration, we first generate a textual
summary _ϕ_ ( _v_ ) for each node in _V_ current, embed these new summaries, and cluster them to form the
next, higher level of the tree.


**3. Termination.** We repeat this process until the number of nodes at the current level is less than or
equal to _M_ . These final nodes are assigned as the children of the root node, _v_ _root_, completing the

tree.


3.3.2 A PPROACH 2: T OP -D OWN D IVISIVE C LUSTERING


As an alternative to the agglomerative bottom-up method, we also explore a top-down divisive
approach. Conceptually, this method is similar to hierarchical k-means, where we begin with a
single cluster containing the entire document corpus and recursively partition it. The standard
implementation would use an embedding and clustering function at each step. However, we observed
that this can produce noisy, suboptimal clusters at the higher levels of the tree where partitions should
be based on broad conceptual similarities rather than keyword overlap.


To address this, we employ an LLM as a more powerful clustering function. Since providing the entire
corpus to an LLM is infeasible due to context limits, we introduce a prerequisite step: **hierarchical**
**summarization** . For each leaf node _v_ _l_, we prompt an LLM to generate five summaries in increasing
order of complexity (we quantify the complexity of a summary by its length, for e.g. first level of
summary is 1-2 word, next is 3-4 words, and so on, more details in Section B.3.2), yielding a set of
multi-level representations _{ϕ_ ( _v_ _l_ ) _[i]_ _}_ [5] _i_ =1 [.]


The top-down construction, detailed in Algorithm 4, proceeds as a recursive partitioning process:


**1. Initialization.** The process begins with a work queue containing the root node _v_ _root_, whose
children are initially all leaf nodes _V_ _L_ .


**2. Recursive Partitioning.** We iteratively process nodes from the queue. For each node _v_ to be
partitioned, we first select an appropriate summary level _i_ for its leaf descendants (details in the
Section B.3.2). We then provide the set of unique summaries at that level to an LLM, prompting it to
group them into _M_ conceptual topics.


**3. Node Creation and Re-assignment.** The LLM returns a description for each of the _M_ topics
and a mapping from the unique input summaries to these topics. We create _M_ new internal nodes,
assign them the topic descriptions, and partition the leaf descendants of _v_ among these new nodes
according to the LLM’s mapping. These _M_ new nodes become the children of _v_ . Any new node that
still contains more than _M_ leaves is added to the queue for further partitioning.


7


**4. Termination.** The process terminates when the queue is empty, meaning all internal nodes in the
tree satisfy the maximum branching factor constraint.


4 E XPERIMENTS


4.1 E XPERIMENTAL S ETUP


**Benchmark.** All experiments are conducted on the BRIGHT benchmark (Su et al., 2024), a
collection of 12 reasoning-intensive retrieval tasks. The benchmark is specifically designed to evaluate
deep reasoning and is composed of complex questions from diverse sources, including StackExchange,
Leetcode, and TheoremQA, spanning topics from biology and economics to programming and
mathematics.


**Evaluation Metrics.** We use two standard IR metrics to measure performance: **nDCG@10** (Normalized Discounted Cumulative Gain at 10) to evaluate the ranking quality of the top 10 results, and
**Recall@100** to measure the comprehensiveness of the retrieval within the top 100 results.


**Baselines.** We compare LATTICE against several strong baselines.


- **SOTA Systems:** We compare against state-of-the-art systems like **DIVER-v1/v2** (Long et al.,
2025), **RaDeR** (Das et al., 2025), **ReasonRank** (Liu et al., 2025) and **ReasonIR** (Shao et al., 2025),
which are trained and highly specialized for the BRIGHT benchmark.


- **Controlled Reranking Baseline:** To ensure a fair, apples-to-apples comparison, we include
a strong retrieve-then-rerank baseline **XRR2** [1] (BM25 + Rerank) that uses the _same base LLM_
(Gemini-2.5-flash) as our method. XRR2 first retrieves 100 candidates using BM25 with a GPT-4
expanded query and then reranks them using Gemini-2.5-flash model for total 5 iterations. This
allows us to isolate the performance gains attributable to directly using an LLM to search the space
versus just reranking a small retrieved corpus.


**Implementation Details.** For all LLM-driven components of our method (tree construction, summarization, and online search), we use **Gemini-2.5-flash** (Comanici et al., 2025). For the online
traversal, we set the path relevance momentum to _α_ = 0 _._ 5, the number of iterations to _N_ = 20,
_ℓ_ = 10 and the beam size to _B_ = 2 . This configuration results in approximately 250 documents
being evaluated by the LLM per query. For tree construction, the maximum branching factor was set
to _M ∼_ 10 _−_ 20. For datasets derived from StackExchange, we employed the bottom-up clustering
method; for all others, we used the top-down divisive approach. Our method, LATTICE, is evaluated
in a strictly zero-shot setting, without any fine-tuning or ensembling with any other method for the
BRIGHT benchmark tasks. Further details are provided in Appendix B.


4.2 P ERFORMANCE ON THE BRIGHT B ENCHMARK


**Ranking Performance (nDCG@10)** We present main ranking results on the BRIGHT benchmark in Table 1. On the seven StackExchange datasets, which use a standard static corpus, LATTICE achieves an average nDCG@10 of **51.6**, significantly outperforming the controlled reranking
baseline’s score of **47.4** . Furthermore, our zero-shot performance is highly competitive with the
fine-tuned SOTA, Diver-v2 ( **52.2** ), and even achieves the best results in several sub-domains like Economics and Robotics. On the 3/5 Coding and Theorem-based tasks (LeetCode, AoPS & TheoremQ),
our method’s performance is noticably lower than the baselines. This is attributable to a specific
benchmark artifact: the use of a query-dependent dynamic corpus, where a unique large list (can
be > 10K) of documents (which are potential positives) is excluded from the search space. While
we prune the excluded leaf nodes at query time, the pre-computed summaries ( _ϕ_ ( _v_ ) ) of their parent
nodes do not update dynamically. Consequently, these summaries often misguide the traversal (please
see Figure 6, Section C.2). In contrast, retrieve-then-rerank pipelines can simply filter excluded
documents from their candidate list post-retrieval without penalty. We would like to note that most
real-world IR systems operate on a query-independent corpus.


1 [https://github.com/jataware/XRR2/tree/main](https://github.com/jataware/XRR2/tree/main)


8


![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-8-0.png)











Table 1: nDCG@10 performance of various retrievers and rankers on the BRIGHT benchmark. **Bold**
represents overall best numbers, underline represents best numbers among zero-shot methods, _[∗]_
denotes subsets with dynamic corpus.


**Retrieval Performance (Recall@100)** As illustrated in Figure 1, our method demonstrates superior overall retrieval comprehensiveness. On average, LATTICE achieves a Recall@100 of **74.8**,
outperforming both the BM25 baseline (65.3) and the specialized ReasonIR-8B model (70.8). This
strong performance is consistent across the majority of subsets, with our method achieving the highest
recall in four of the seven domains, including Economics and Psychology.


**Cost-Performance Analysis.** To analyze the computational cost of our method, we compare the
trade-off between performance (nDCG@10) and cost (measure in number of tokens processed by the
LLM) against two retreive-then-rerank baselines using the gemini-2.5-flash as the ranking LLM and
varying top-k predictions from the retriever. Figure 1 plots this relationship for the Robotics subset.
While the reranking baselines exhibit diminishing returns, LATTICE’s performance scales far more
effectively on this subset. The performance initially remains flat as the model needs to take atleast
tree height number of slate comparisons to reach a leaf node. This shows promise that our guided
hierarchical search can be more efficient use of the LLM’s computational budget than reranking a
long, flat list of documents, where many of the tokens are spent on irrelevant candidates.



5 A NALYSIS


**Effect of # Cross-Branch Calibration (** _ℓ_ **).** Figure 4 shows
the impact of including _ℓ_ top-scoring nodes from sibling
branches in the leaf slates on bio subset. The results demon
strate that this calibration is critical for effective search. The
baseline with no calibration ( _ℓ_ = 0 ) performs significantly
worse and fails to improve with more search iterations. Performance consistently increases with _ℓ_, with substantial gains
from _ℓ_ = 1 to _ℓ_ = 5. The gains diminish after _ℓ_ = 5.





|70<br>60<br>50 nDCG@10<br>40<br>30<br>20<br>10<br>0 2 4|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|~~2~~<br>~~4~~<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10<br>|||||||~~60.6~~<br>63.8<br>~~64.4~~|
|~~2~~<br>~~4~~<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10<br>|||||||53.9<br>|
|~~2~~<br>~~4~~<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10<br>|||||||~~40.7~~|
|~~2~~<br>~~4~~<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10<br>||||||~~=~~<br>|~~ 0~~<br>|
|~~2~~<br>~~4~~<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10<br>||||||=<br>~~=~~<br>=<br>~~=~~|1<br>~~ 3~~<br> 5<br>~~ 10~~|
|~~2~~<br>~~4~~<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10<br>||||||||
|~~2~~<br>~~4~~<br>0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10<br>|||~~6~~<br>~~8~~<br>~~10~~<br>~~1~~<br>|~~6~~<br>~~8~~<br>~~10~~<br>~~1~~<br>|~~2~~<br>~~14~~<br>~~16~~<br>|~~18~~|~~20~~|


Figure 4: nDCG@10 vs. _ℓ_ .



**Impact of Method Components** To quantify the contribution of each component of LATTICE,
we conduct a detailed ablation study with results presented in Table 2. We compare our full method
against several variants: a version without score calibration (always taking the latest score given

|Configuration|Avg.|Bio. Earth. Econ. Psy. Rob. Stack. Su|
|---|---|---|
|**LATTICE (Full Method)**|51.57|64.38<br>62.36<br>45.37<br>57.35<br>47.57<br>37.58<br>46.|
|_−_No Score Calibration (ˆ_sv_ = last_ si_<br>_v_)<br>_−_No Path Relevance (_α_ = 0)<br>_−_No Reasoning (thinking_budget= 0)|49.36<br>48.62<br>49.33|64.45<br>58.98<br>44.27<br>54.41<br>46.70<br>32.93<br>43.<br>63.62<br>55.89<br>41.90<br>52.99<br>42.14<br>40.68<br>43.<br>63.69<br>57.32<br>43.77<br>57.33<br>45.73<br>33.16<br>43.|



Table 2: Ablation study on the core components of our traversal algorithm, evaluated across all
StackExchange subsets of the BRIGHT benchmark. All values are nDCG@10.


9


by the search LLM to a node), one without path relevance (disabling path smoothing with _α_ = 0 ),
and one with zero reasoning budget to the LLM (passing thinking_budget= 0 in search LLM
calls and strictly constraining it to output only the “scores” field in its output json). Disabling path
relevance smoothing causes the largest degradation, followed by removing either the LLM’s reasoning
or score calibration mechanism reducing the average score by over 2.2 nDCG points.



**Beam Size vs. Search Iterations.** Figure 5 presents a budgetmatched analysis of beam size ( _B_ ) versus search iterations ( _N_ ),
where the total number of node expansions ( _B × N_ ) is kept
roughly constant. The results indicate that for a fixed computational budget, prioritizing search depth (more iterations)
over breadth (a larger beam) is better. The configurations with
smaller beams, _B_ = 1 and _B_ = 2, achieve the highest final
nDCG@10 scores but are more sequential. This validates our
choice of using a small beam size ( _B_ = 2 ) with a moderate
number of iterations.






|Col1|Budget-matched B|Col3|(beam-size) ablation|Col5|Col6|
|---|---|---|---|---|---|
|0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10|~~60.9~~<br>|~~60.9~~<br>|62.2<br>64.4<br>64.9<br>|62.2<br>64.4<br>64.9<br>|62.2<br>64.4<br>64.9<br>|
|0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10||||||
|0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10||||||
|0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10||||||
|0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10|||B = 1, N = 40<br>|B = 1, N = 40<br>|B = 1, N = 40<br>|
|0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10||||~~B = 2, ~~<br>B = 4, <br>|~~N = 20~~<br>N = 10<br>|
|0<br>10<br>20<br>30<br>40<br>50<br>60<br>70<br>nDCG@10|||~~B = 8, N = 5~~|~~B = 8, ~~|~~B = 8, ~~|
|Iteration<br>|Iteration<br>|Iteration<br>|10~~1~~<br> (log~~-~~scale)|10~~1~~<br> (log~~-~~scale)|10~~1~~<br> (log~~-~~scale)|


Figure 5: nDCG@10 vs. beam-size.



**Impact of Tree Construction Strategy** We investigate
the impact of the tree construction strategy on two representative datasets in Table 3. The results show that nDCG@10 R@100 nDCG@10 R@100
aligning the tree construction method with the corpus’s Bottom-Up Tree
underlying structure is critical for zero-shot performance. **64.38** **87.53** 35.89 61.82
For the Biology dataset, which is composed of passages Top-Down Tree
from larger source documents, the bottom-up approach 55.22 67.31 **47.85** **73.91**
is superior, improving nDCG@10 by over 9 points. We

![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-9-0.png)

Table 3: Tree construction comparison.

hypothesize that this is because it leverages the inherent
part-whole relationships in the data. Conversely, for the TheoT. dataset, which is a collection of
distinct documents under a high-level topic, the top-down approach excels, improving nDCG@10 by
nearly 12 points. We hypothesize that this method is better suited to discovering the latent conceptual
clusters among independent documents.















Table 3: Tree construction comparison.



A CKNOWLEDGEMENTS


This research was supported in part by NSF award #2439754. We would also like to express our
gratitude to Divy Thakkar for generously helping with additional gemini-api credits, which were
essential for conducting our experiments.


R EFERENCES


Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit
Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier
with advanced reasoning, multimodality, long context, and next generation agentic capabilities.
_arXiv preprint arXiv:2507.06261_, 2025.


Debrup Das, Sam O’ Nuallain, and Razieh Rahimi. Rader: Reasoning-aware dense retrieval models.
_arXiv preprint arXiv:2505.18405_, 2025.


Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt,
Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A
graph rag approach to query-focused summarization. _arXiv preprint arXiv:2404.16130_, 2024.


Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without
relevance labels. In _Proceedings of the 61st Annual Meeting of the Association for Computational_
_Linguistics (Volume 1: Long Papers)_, pp. 1762–1777, 2023.


Nilesh Gupta, Patrick Chen, Hsiang-Fu Yu, Cho-Jui Hsieh, and Inderjit Dhillon. Elias: End-to-end
learning to index and search in large output spaces. _Advances in Neural Information Processing_
_Systems_, 35:19798–19809, 2022.


10


Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and
Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement
learning. _arXiv preprint arXiv:2503.09516_, 2025.


Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko,
Yi Luan, Sébastien MR Arnold, Vincent Perot, Siddharth Dalmia, et al. Can long-context language
models subsume retrieval, rag, sql, and more? _arXiv preprint arXiv:2406.13121_, 2024a.


Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R Cole, Kai Hui, Michael
Boratko, Rajvi Kapadia, Wen Ding, et al. Gecko: Versatile text embeddings distilled from large
language models. _arXiv preprint arXiv:2403.20327_, 2024b.


Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gustavo Hernández Ábrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, et al. Gemini embedding:
Generalizable embeddings from gemini. _arXiv preprint arXiv:2503.07891_, 2025.


Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. Learning to rank in generative retrieval.
In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pp. 8716–8723, 2024.


Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, and Zhicheng Dou. Reasonrank: Empowering passage ranking with strong reasoning ability. _arXiv preprint arXiv:2508.07050_,
2025.


Meixiu Long, Duolin Sun, Dan Yang, Junjie Wang, Yue Shen, Jian Wang, Peng Wei, Jinjie Gu, and
Jiahai Wang. Diver: A multi-stage approach for reasoning-intensive information retrieval. _arXiv_
_preprint arXiv:2508.07995_, 2025.


Kun Luo, Minghao Qin, Zheng Liu, Shitao Xiao, Jun Zhao, and Kang Liu. Large language models as
foundations for next-gen dense retrieval: A comprehensive empirical assessment. _arXiv preprint_
_arXiv:2408.12194_, 2024.


Yu. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using
hierarchical navigable small world graphs, 2018. URL [https://arxiv.org/abs/1603.](https://arxiv.org/abs/1603.09320)
[09320.](https://arxiv.org/abs/1603.09320)


Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In
_International workshop on artificial intelligence and statistics_, pp. 246–252. PMLR, 2005.


Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm.
_Advances in neural information processing systems_, 14, 2001.


Yashoteja Prabhu and Manik Varma. Fastxml: A fast, accurate and stable tree-classifier for extreme
multi-label learning. In _Proceedings of the 20th ACM SIGKDD international conference on_
_Knowledge discovery and data mining_, pp. 263–272, 2014.


Ronak Pradeep, Kai Hui, Jai Gupta, Adam D Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler,
and Vinh Q Tran. How does generative retrieval scale to millions of passages? _arXiv preprint_
_arXiv:2305.11841_, 2023.


Mandeep Rathee, Sean MacAvaney, and Avishek Anand. Guiding retrieval using llm-based listwise
rankers. In _European Conference on Information Retrieval_, pp. 230–246. Springer, 2025.


Revanth Gangi Reddy, JaeHyeok Doo, Yifei Xu, Md Arafat Sultan, Deevya Swain, Avirup Sil, and
Heng Ji. First: Faster improved listwise reranking with single token decoding. _arXiv preprint_
_arXiv:2406.15657_, 2024.


Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.
Raptor: Recursive abstractive processing for tree-organized retrieval. In _The Twelfth International_
_Conference on Learning Representations_, 2024.


Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan
Kian Hsiang Low, Sewon Min, Wen-Tau Yih, Pang Wei Koh, and Luke Zettlemoyer. ReasonIR:
Training retrievers for reasoning tasks. _arXiv [cs.AI]_, April 2025.


11


Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-Yu Wang, Haisu Liu,
Quan Shi, Zachary S Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O Arik, Danqi Chen,
and Tao Yu. BRIGHT: A realistic and challenging benchmark for reasoning-intensive retrieval.
_arXiv [cs.CL]_, July 2024.


Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin,
and Zhaochun Ren. Is chatgpt good at search? investigating large language models as re-ranking
[agents, 2024. URL https://arxiv.org/abs/2304.09542.](https://arxiv.org/abs/2304.09542)


Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe
Zhao, Jai Gupta, et al. Transformer memory as a differentiable search index. _Advances in Neural_
_Information Processing Systems_, 35:21831–21843, 2022.


Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models.
_arXiv preprint arXiv:2303.07678_, 2023.


Orion Weller, Michael Boratko, Iftekhar Naim, and Jinhyuk Lee. On the theoretical limitations of
embedding-based retrieval. _arXiv preprint arXiv:2508.21038_, 2025.


Hsiang-Fu Yu, Kai Zhong, Jiong Zhang, Wei-Cheng Chang, and Inderjit S Dhillon. Pecos: Prediction
for enormous and correlated output spaces. _Journal of Machine Learning Research_, 23(98):1–32,
2022.


Weinan Zhang, Junwei Liao, Ning Li, Kounianhua Du, and Jianghao Lin. Agentic information
retrieval. _arXiv preprint arXiv:2410.09713_, 2024.


Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, and Yong Li. A survey of large
language model empowered agents for recommendation and search: Towards next-generation
information retrieval. _arXiv preprint arXiv:2503.05659_, 2025.


Yutao Zhu, Huaying Yuan, Zhengyi Liu, Chenxi Li, Ahmed Awadallah, Haolan Wang, and Ji-Rong
Wen. Large language models for information retrieval: A survey. _arXiv preprint arXiv:2308.07107_,
2023.


12


A L IMITATIONS AND F UTURE W ORK


Our work introduces a novel framework for hierarchical retrieval, but it also presents several avenues
for future research. One of the limitation of our current approach is the use of a **static semantic tree** .
As demonstrated in our experiments on dynamic corpora, the pre-computed summaries of internal
nodes do not update when leaf nodes are filtered, which can occasionally misguide the search. Future
work could explore methods for efficient, localized updates to the tree’s summaries, allowing the
hierarchy to adapt to a changing corpus without the need for a full reconstruction.


Second, the **offline tree construction process**, while a one-time cost, can be computationally intensive
for extremely large corpora due to the repeated use of LLMs for clustering and summarization.
Research into more efficient construction methods, perhaps by combining traditional clustering for
the lower levels with LLM-based summarization for only the top, most abstract layers, could further
improve scalability.


Finally, our traversal algorithm opens up new research directions. The score calibration method,
while effective, uses a simple linear model. More sophisticated probabilistic models, could be
explored for even more robust **latent score estimation** . Furthermore, while our greedy, best-first
traversal is effective in a zero-shot setting, the entire process could be framed as a reinforcement
learning problem, where the search LLM is an agent trained to optimize a policy for navigating the
tree to maximize retrieval rewards. We believe that exploring these directions will further establish
hierarchical, LLM-driven navigation as a powerful new paradigm in information retrieval.


B I MPLEMENTATION D ETAILS


B.1 H YPERPARAMETERS


This section provides a detailed list of all hyperparameters and implementation choices used in our
experiments to ensure full reproducibility.


B.1.1 O FFLINE T REE C ONSTRUCTION


    - **Maximum Branching Factor (** _M_ **):** We set the maximum number of children for any node
to _M_ = 10 _−_ 20.


    - **Embedding Model (** _E_ **):** We use gecko (Lee et al., 2024b) embeddings to generate vector
representations for the clustering steps.


    - **Clustering Algorithm (** _C_ **):** Our implementation uses an iterative spectral clustering (Ng
et al., 2001) algorithm to partition nodes into at most _M_ clusters at each level of the
hierarchy.


    - **Summarization LLM:** We use Gemini-2.5-flash for all summarization tasks (both
for internal nodes in the bottom-up method and for the multi-level document summaries in
the top-down method). The exact prompt template used is detailed in Appendix D.


    - **Top-Down Summary Levels:** For the top-down method, we generate 5 levels of hierarchical
summaries for each document.


B.1.2 O NLINE T RAVERSAL


    - **Search LLM (** _L_ **):** We use Gemini-2.5-flash as the search agent that performs the
listwise scoring. The prompt structure is provided in Appendix D.


    - **Number of Iterations (** _N_ **):** We run the search for _N_ = 20 iterations for all main experi
ments.


    - **Beam Size (** _B_ **):** We use a beam size of _B_ = 2 for parallel node expansion in each iteration.


    - **Path Relevance Momentum (** _α_ **):** The smoothing factor for the path relevance score is set
to _α_ = 0 _._ 5.


    - **Calibration Nodes (** _l_ **):** We augment each leaf slate with _ℓ_ = 10 cross-branch leaf nodes for
calibration, based on our ablation study.


13


    - **Reasoning Budget:** The default “thinking budget” for the LLM’s reasoning step is set to
-1, meaning the model gets to decide how long it wants to thin.


    - **MLE Solver:** The latent scores are updated after each batch of slate evaluations. The MSE
loss is minimized using the Adam optimizer with a learning rate of 10 _[−]_ [2] for 100 steps.


**Usage of LLMs** During the preparation of this manuscript, LLM were used as a collaborative
writing assistant to aid with drafting, refining prose for clarity and conciseness, and structuring
arguments; all core ideas, experiments, and analyses were conducted by the authors.


B.2 D ATASET D ETAILS


All experiments are conducted on the BRIGHT benchmark (Su et al., 2024), a comprehensive
collection of 12 datasets designed to evaluate reasoning-intensive retrieval. A summary of the
statistics for each subset is provided in Table 4.



![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-13-0.png)

Table 4: Statistics for the 12 subsets of the BRIGHT benchmark used in our experiments.


The datasets exhibit two key characteristics relevant to our work. First, the StackExchange subsets
are composed of passages derived from longer source documents. We leverage this structure for our
metadata-based initial clustering in the bottom-up tree construction method. Second, the Coding and
Theorem-based datasets (excluding Pony and TheoremQA Theorems) utilize a **query-dependent**
**corpus**, where a unique list of documents (often >10k) must be excluded from the search space for
each query. This feature, discussed in our main results analysis, poses a unique challenge for static
index structures like our semantic tree.


B.3 T REE C ONSTRUCTION


B.3.1 B OTTOM - UP


The Bottom-up tree constructions algorithms are defined in Alogirthm 2, 3.


B.3.2 T OP - DOWN


The Top-down tree constructions algorithm is defined in Algorithm 4, the two subroutines used are
described below.


The **SelectSummaryLevel** function implements a heuristic to find the optimal summary granularity
for a given set of leaf nodes. It begins with the most abstract summary level ( _i_ = 1 ) and iteratively


14


**Algorithm 2** Bottom-Up Tree Construction


1: **Parameters:** Corpus _D_, _E_, _C_, Summarize LLM, _M_, Optional InitialClusters
2: **Initialize:** _V_ _L_ _←{_ Node( _d_ ) _| d ∈_ _D}_, _V ←_ _V_ _L_, _E ←∅_
3: **if** InitialClusters is provided **then**
4: _V_ current _←_ CreateNodesFromClusters( _V_ _L_ _,_ InitialClusters _, V, E_ )
5: **else**
6: Embeddings _←{E_ ( _ϕ_ ( _v_ )) : _v ∈_ _V_ _L_ _}_
7: Clusters _←C_ (Embeddings)
8: _V_ current _←_ CreateNodesFromClusters( _V_ _L_ _,_ Clusters _, V, E_ )
9: **end if**
10: **while** _|_ V current _| > M_ **do**
11: {Summarize the current layer before clustering}
12: **for all** _v_ in _V_ current **do**
13: _ϕ_ ( _v_ ) _←_ Summarize( _{ϕ_ ( _c_ ) _| c ∈_ _C_ ( _v_ ) _}_ )
14: **end for**
15: _V_ next_layer _←∅_
16: Embeddings _←{E_ ( _ϕ_ ( _v_ )) : _v ∈_ _V_ current _}_
17: Clusters _←C_ (Embeddings)
18: _V_ next_layer _←_ CreateNodesFromClusters( _V_ current _,_ Clusters _, V, E_ )
19: _V_ current _←_ _V_ next_layer
20: **end while**
21: _v_ _root_ _←_ NewInternalNode(), _ϕ_ ( _v_ _root_ ) _←_ ””
22: _C_ ( _v_ _root_ ) _←_ _V_ current
23: _V ←_ _V ∪{v_ _root_ _}_, _E ←_ _E ∪{_ ( _v_ _root_ _, c_ ) _| c ∈_ _C_ ( _v_ _root_ ) _}_
24: **return** Tree _T_ = ( _V, E_ )


**Algorithm 3** CreateNodesFromClusters Subroutine


1: **function** CreateNodesFromClusters( _V_ source, Clusters, _V_, _E_ )
2: **Input:**
3: _V_ source : The set of nodes in the layer to be clustered.
4: Clusters: The partition of _V_ source ’s embeddings from _C_ .
5: _V, E_ : The global node and edge sets for the tree (passed by reference).
6: **Initialize:** _V_ new_layer _←∅_
7: **for all** cluster _K_ in Clusters **do**
8: _v_ _new_ _←_ NewInternalNode()
9: _C_ ( _v_ _new_ ) _←{v ∈_ _V_ source _| v ∈_ _K}_
10: _V ←_ _V ∪{v_ _new_ _}_
11: _E ←_ _E ∪{_ ( _v_ _new_ _, c_ ) _| c ∈_ _C_ ( _v_ _new_ ) _}_
12: _V_ new_layer _←_ _V_ new_layer _∪{v_ _new_ _}_
13: **end for**
14: **return** _V_ new_layer


checks the number of unique summaries, selecting the first level _i_ where the count of unique
summaries is sufficient for meaningful clustering (e.g., greater than _M_ ) while remaining under a
maximum token limit for the LLM context.


The **ClusterLLM** function is realized via a structured prompt (see 9. The LLM is provided with the
list of unique summaries and tasked with grouping them into _M_ coherent conceptual clusters. The
prompt instructs the model to first generate a short, descriptive title for each of the _M_ clusters, and
then to output a mapping from each input summary to one of these cluster titles. The final output is a
structured object containing the _M_ topic descriptions (which become the _ϕ_ ( _v_ ) for the new nodes)
and the mapping.


15


**Algorithm 4** Top-Down Divisive Tree Construction


1: **Parameters:** Corpus _D_, Summarize LLM, Cluster LLM, Max branching factor _M_
2: **Initialize:**
3: For each document _d_ _l_ _∈_ _D_, generate multi-level summaries _{ϕ_ ( _v_ _l_ ) _[i]_ _}_ [5] _i_ =1 [.]
4: _V_ _L_ _←{_ Node( _d_ ) _| d ∈_ _D}_, _V ←_ _V_ _L_
5: _v_ _root_ _←_ NewInternalNode(), _C_ ( _v_ _root_ ) _←_ _V_ _L_
6: _V ←_ _V ∪{v_ _root_ _}_, _E ←{_ ( _v_ _root_ _, c_ ) _| c ∈_ _V_ _L_ _}_
7: PartitionQueue _←_ new Queue()
8: **if** _|V_ _L_ _| > M_ **then**
9: PartitionQueue.enqueue( _v_ _root_ )
10: **end if**
11: **while** PartitionQueue is not empty **do**
12: _v ←_ PartitionQueue.dequeue()
13: LeafDescendants _←_ GetLeafDescendants( _v, T_ )
14: _i ←_ SelectSummaryLevel(LeafDescendants)
15: UniqueSummaries _←_ unique( _{ϕ_ ( _c_ ) _[i]_ _| c ∈_ LeafDescendants _}_ )
16: TopicDescs, Mapping _←_ ClusterLLM(UniqueSummaries _, M_ )
17: NewChildren _←∅_
18: **for** _j_ = 1 **to** _M_ **do**
19: _v_ _j_ _[′]_ _[←]_ [NewInternalNode][()][,] _[ ϕ]_ [(] _[v]_ _j_ _[′]_ [)] _[ ←]_ [TopicDescs][[] _[j]_ []]
20: _V ←_ _V ∪{v_ _j_ _[′]_ _[}]_ [, NewChildren] _[ ←]_ [NewChildren] _[ ∪{][v]_ _j_ _[′]_ _[}]_
21: **end for**
22: ReassignChildren(LeafDescendants, Mapping, NewChildren, T)
23: _E ←_ _E \ {_ ( _v, c_ ) _| c ∈_ _C_ ( _v_ ) _}_ {Disconnect old children}
24: _C_ ( _v_ ) _←_ NewChildren
25: _E ←_ _E ∪{_ ( _v, c_ ) _| c ∈_ NewChildren _}_ {Connect new children}
26: **for all** _v_ _j_ _[′]_ [in NewChildren] **[ do]**
27: **if** _|C_ ( _v_ _j_ _[′]_ [)] _[|][ > M]_ **[ then]**
28: PartitionQueue.enqueue( _v_ _j_ _[′]_ [)]
29: **end if**

30: **end for**

31: **end while**
32: **return** Tree _T_ = ( _V, E_ )


C S UBJECTIVE A NALYSIS


C.1 S AMPLE SCORING RESPONSE FROM LLM


To provide a more intuitive understanding of our method, Figure 3 presents a qualitative case study
of the search process for a real query from the BRIGHT benchmark. The user query is a code snippet
asking about “yaw rotation,” a complex 3D graphics problem. The figure visualizes the semantic tree
and the traversal path taken by LATTICE (highlighted in yellow) to successfully locate a relevant
document deep within the hierarchy.


The expanded callout provides a "glass box" view into the search LLM’s reasoning at a critical
decision point. The LLM’s generated **Reasoning** explicitly connects the user’s query to the node’s
topic, noting that the user is “attempting to perform yaw rotation using quaternion_from_euler.”
It then performs a detailed, comparative evaluation of the children nodes. It correctly identifies
Candidate 1 as highly relevant because it discusses “support for converting between different 3D
rotation representations, including matrices, quaternions, and Euler angles,” which directly addresses
the user’s problem. This example demonstrates that our method does not rely on shallow semantic
similarity; instead, the search is an active process guided by the LLM’s deep, step-by-step reasoning
about the query in the context of the corpus hierarchy.


16


**Interactive Prediction Tree** for **Query** : Let $s_k$ denote the sum of the $\textit{k}$th powers of the roots of the polynomial $x^3-5x^2+8x-13$. In particular, $s_0=3$, $s_1=5$, and
$s_2=9$. Let $a$, $b$, and $c$ be real numbers such that $s_{k+1} = a \, s_k + b \, s_{k-1} + c \, s_{k-2}$ for $k = 2$, $3$, $....$ What is $a+b+c$?
$\textbf{(A)} \; -6 \qquad \textbf{(B)} \; 0 \qquad \textbf{(C)} \; 6 \qquad \textbf{(D)} \; 10 \qquad \textbf{(E)} \; 26$



Relevance
1


0.8


0.6


0.4


0.2


0



![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-16-0.png)

Figure 6: Search failing due to dynamically excluded search corpus, red edges denote excluded leaf
nodes, gold edges denote ground-truth path


C.2 S EARCH FAILURE ON DYNAMIC CORPUS


Figure 6 provides a qualitative case study of a search failure, visually demonstrating the primary
challenge our method faces on datasets with a dynamic corpus. The figure shows the search tree for a
random query from the AoPS dataset. Red edges indicate leaf nodes that were dynamically excluded
for this specific query, while the yellow path highlights the ideal traversal route to the ground-truth
document.


As the figure shows, the search agent correctly follows the ground-truth path for the first two levels.
However, it then reaches an internal node whose pre-computed summary is now misleading; the
summary was generated based on all of its children, including the large number that have since been
pruned from the search space (the red nodes). This inaccurate, stale summary causes the search LLM
to make an incorrect judgment, deviating from the correct path and ultimately failing to retrieve the
relevant document. This example visually confirms the specific failure mode of a static hierarchical
index when faced with a dynamic corpus, reinforcing the quantitative analysis in our main results
section.


D P ROMPTS


17


![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-17-0.png)



Figure 7: Prompt template used in our experiments for scoring a list of nodes for _L_ .


18


![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-18-0.png)





Figure 8: Prompt template used in our experiments for generating multi-level keywords to be used in
top-down tree construction.


19


![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-19-0.png)



Figure 9: Prompt template used for ClusterLLM to be used in top-down tree construction i.e.
clustering a given set of keywords into [ _M_ _min_ _, M_ _max_ ] clusters.


20


![](/Users/caitlyn/Documents/2510.13217v1-bib4llm/2510.13217v1/2510.13217v1.pdf-20-0.png)



Figure 10: Prompt template for generating bottom-up summaries of a group of nodes.


21


