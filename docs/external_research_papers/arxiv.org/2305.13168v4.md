# Citation Key: 2305.13168v4

---

## LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities

Yuqi Zhu [1,2] **[†]**, Xiaohan Wang [1,2] **[†]**, Jing Chen [1,2] **[†]**, Shuofei Qiao [1,2],
Yixin Ou [1,2], Yunzhi Yao [1,2], Shumin Deng [3], Huajun Chen [1,2],
Ningyu Zhang [1,2*]

1*
Zhejiang University, China.
2
ZJU-Ant Group Joint Research Center for Knowledge Graphs, China.
3
National University of Singapore, NUS-NCS Joint Lab, Singapore.

*Corresponding author(s). E-mail(s): zhangningyu@zju.edu.cn;
Contributing authors: zhuyuqi@zju.edu.cn;

**†**
These authors contributed equally to this work.

**Abstract**

This paper presents an exhaustive quantitative and qualitative evaluation of
Large Language Models (LLMs) for Knowledge Graph (KG) construction and
reasoning. We engage in experiments across eight diverse datasets, focusing
on four representative tasks encompassing entity and relation extraction, event
extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs’ performance in the domain of construction and inference. Empirically,
our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while
GPT-4 exhibits good performance in tasks related to KG construction, it excels
further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs
for information extraction, leading to the proposition of a Virtual Knowledge
Extraction task and the development of the corresponding VINE dataset. Based
on these empirical findings, we further propose **AutoKG**, a multi-agent-based
approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future
undertakings in the field of knowledge graphs.

**Keywords:** Knowledge Graph, Information Extraction, GPT-4, Large Language Model

1

### **1 Introduction**

Knowledge Graph (KG) is a semantic network comprising entities, concepts, and
relations [1–6], which can catalyse applications across various scenarios. Constructing KGs [7, 8] typically involves multiple tasks such as Named Entity Recognition
(NER) [9, 10], Relation Extraction (RE) [11, 12], Event Extraction (EE) [13, 14], and
Entity Linking (EL) [15]. Additionally, Link Prediction (LP) [16, 17] is a crucial step
for KG reasoning, essential for understanding constructed KGs. These KGs also hold
a central position in Question Answering (QA) tasks [18, 19], especially in conducting inference based on question context, involving the construction and application of
relation subgraphs. This paper empirically investigates the potential applicability of
LLMs in the KG domain, taking ChatGPT and GPT-4 [20] as examples. The research
begins with an examination of the fundamental capabilities of LLMs [21–24], progressing to explore possible future developments, aiming to enhance our understanding of
LLMs and introduce new perspectives and methods to the field of knowledge graphs.


![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2305.13168v4/2305.13168v4.pdf-1-3.png)




![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2305.13168v4/2305.13168v4.pdf-1-0.png)








![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2305.13168v4/2305.13168v4.pdf-1-2.png)


![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2305.13168v4/2305.13168v4.pdf-1-1.png)












**Fig. 1** The overview of our work. There are three main components: 1) _**Basic Evaluation**_ : detailing our assessment of large models (text-davinci-003, ChatGPT, and GPT-4), in both zero-shot and
one-shot settings, using performance from fully supervised state-of-the-art models as benchmarks;
2) _**Virtual Knowledge Extraction**_ : an examination of LLMs’ virtual knowledge capabilities on the
constructed VINE dataset; and 3) _**Automatic KG**_ : the proposal of utilizing multiple agents to facilitate the construction and reasoning of KGs.

2

_**Recent Capabilities.**_ Entity and Relation Extraction, along with Event Extraction, are pivotal for Knowledge Graph (KG) construction tasks [25–28]. They play
a critical role in organizing vast amounts of entity, relation, and event data into
structured representations, forming the foundational elements that underpin the construction and enrichment of KGs. Meanwhile, Link Prediction, as a core task of KG
reasoning [29], aims to uncover latent relationships between entities, thereby enriching
the knowledge graph. Additionally, we delve into the utilization of LLMs in knowledgebased Question Answering tasks [30, 31] to gain a thorough insight into their reasoning
capabilities. Given these considerations, we select these tasks as representatives for
evaluating both the construction and reasoning of KGs. As illustrated in Figure 1, our
initial investigation targets the zero-shot and one-shot abilities of large language models across the aforementioned tasks. This analysis serves to assess the potential usage
of such models in the field of knowledge graphs. The empirical findings reveal that
LLMs like GPT-4 exhibit limited effectiveness as _a few-shot information extractor_, yet
demonstrate considerable proficiency as _an inference assistant_ .
_**Generalizability Analysis.**_ To delve deeper into the behavior of LLMs in
information extraction tasks, we devise a unique task termed **Virtual Knowledge**
**Extraction**, targeting LLMs’ ability to generalize and extract unfamiliar knowledge.
This undertaking aims to discern whether the observed performance enhancements on
these tasks are attributed to the extensive internal knowledge repositories of LLMs
or to their potent generalization capabilities facilitated by instruction tuning [32] and
Reinforcement Learning from Human Feedback (RLHF) [33]. And our experiments on
a newly constructed dataset, **VINE**, indicate that large language models like GPT-4
can acquire new knowledge from instructions and effectively execute extraction tasks,
thereby affording a more nuanced understanding of large models to a certain extent.
_**Future Opportunities.**_ In light of the preceding experiments, we further examine prospective directions for knowledge graphs. Given the remarkable generalization
capabilities of large models [34, 35], we opt to employ them to aid in the construction
of KG. Compared to smaller models, these LLMs mitigate potential resource wastage
and demonstrate notable adaptability in novel or data-scarce situations. However, it’s
important to recognize their strong dependence on prompt engineering [36] and the
inherent limitations of their knowledge cutoff. Consequently, researchers are exploring
interactive mechanisms that allow LLMs to access and leverage external resources,
aiming to enhance their performance further [37].
On this basis, we introduce the concept of AutoKG - autonomous KG construction
and reasoning via multi-agent communication. In this framework, the human role is
diminished, with multiple communicative agents each playing their respective roles.
These agents interact with external sources, collaboratively accomplishing the task.
We summarize our contributions as follows [1] :

- We evaluate LLMs, including ChatGPT and GPT-4, offering an initial understanding of their capabilities by evaluating their zero-shot and one-shot performance on
KG construction and reasoning on eight benchmark datasets.

1
[The code and datasets are in https://github.com/zjunlp/AutoKG.](https://github.com/zjunlp/AutoKG)

3

- We design a novel Virtual Knowledge Extraction task and construct the **VINE**
dataset. By evaluating the performance of LLMs on it, we further demonstrate that
LLMs such as GPT-4 possess strong generalization abilities.

- We introduce the concept of automatic KG construction and reasoning, known as
**AutoKG** . Leveraging LLMs’ inner knowledge, we enable multiple agents of LLMs
to assist in the process through iterative dialogues, providing insights for future
research.
### **2 Recent Capabilities of LLMs for KG Construction** **and Reasoning**

The release of large language models like GPT-4, recognized for their remarkable
general capabilities, has been considered by researchers as the spark of artificial general
intelligence (AGI) [38]. To facilitate an in-depth understanding of their performance in
KG-related tasks, a series of evaluations are conducted. §2.1 introduces the evaluation
principles, followed by a detailed analysis in §2.2 on the performance of LLMs in the
construction and reasoning tasks, highlighting variations across different datasets and
domains. Moreover, §2.3 delves into the reasons underlying the subpar performance of
LLMs in certain tasks. And finally, §2.4 discusses whether the models’ performance is
genuinely indicative of generalization abilities or influenced by inherent advantages of
the knowledge base.

**2.1 Evaluation Principle**

In this study, we conduct a comprehensive assessment of LLMs, represented by GPT4, and specifically analyze the performance disparities and enhancements between
GPT-4 and other models in the GPT series, such as ChatGPT. A primary area of
investigation is the models’ performance in zero-shot and one-shot tasks, as these tasks
illuminate the models’ generalization capabilities under data-limited conditions. Given
that some experiments in our study rely on randomly sampled subsets of datasets, it
is important to note that there may be inherent variability in the results due to this
sampling approach. We deliberately choose zero-shot and one-shot tasks over those
requiring more examples, as they better test the models’ adaptability and practical
application in scenarios with sparse data. The experimental prompt used is detailed
in Appendix D. Utilizing the evaluation results, our objective is to explore the reasons
behind the models’ exemplary performance in specific tasks and identify potential
areas of improvement. Ultimately, our goal is to derive valuable insights for future
advancements in such models.

**2.2 KG Construction and Reasoning**

**2.2.1 Settings**

_**Datasets.**_ During the task of Entity and Relation Extraction, Event Extraction, we
employ DuIE2.0 [39], SciERC [40], Re-TACRED [41], and MAVEN [42] datasets. For
Link Prediction, we utilized FB15K-237 [43] and ATOMIC 2020 [44] datasets. Finally,

4

FreebaseQA [45] and MetaQA [16] datasets are used in the Question Answering task.
The dataset used is described in detail in Appendix B.


![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2305.13168v4/2305.13168v4.pdf-4-0.png)



![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2305.13168v4/2305.13168v4.pdf-4-1.png)


![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2305.13168v4/2305.13168v4.pdf-4-2.png)


**Fig. 2** Examples of ChatGPT and GPT-4 on the RE datasets. (1) Zero-shot on the SciERC
dataset (2) Zero-shot on the Re-TACRED dataset (3) One-shot on the DuIE2.0 dataset

**2.2.2 Overall Results**

_**Entity and Relation Extraction.**_ We conduct experiments on DuIE2.0, ReTACRED, and SciERC, each involving 20 samples in the test/valid sets, encompassing
all types of relationships present within the datasets. Here we use PaddleNLP LIC2021
2
IE, PL-Marker [46] and EXOBRAIN [47] as baselines on each dataset, respectively.
Concurrently, for evaluation purposes, the results are reported utilizing the standard
micro F1 score. As shown in Table 1, GPT-4 performs relatively well in both zero-shot
and one-shot manners compared to ChatGPT, even though its performance has not
yet surpassed that of fully supervised small models.

  - **Zero-shot** GPT-4’s zero-shot performance significantly improves across all
tested datasets, especially in DuIE2.0, scoring 31.03, compared to ChatGPT’s 10.3.
Specifically, in the example of Re-TACRED in Figure 2, ChatGPT fails to extract
the target triple, possibly due to the close proximity of head and tail entities

2
[https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/information](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/information_extraction/DuIE) ~~e~~ xtraction/DuIE.

5

and the ambiguity of predicates. In contrast, GPT-4 gives the correct answer “
org:alternate names”, highlighting its superior language comprehension.

  - **One-shot** Simultaneously, the optimization of text instructions has been shown
to enhance the performance of LLMs. In the context of DuIE2.0 shown in Figure 2,
GPT-4 discerns an implicit relation from a statement about George Wilcombe’s association with the Honduras national team. This precision is attributed to GPT-4’s
extensive knowledge base, which facilitates the inference of George Wilcombe’s nationality. However, it is also observed that GPT-4 encounters challenges with complex
sentences, with factors such as prompt quality and relational ambiguity affecting the

outcomes.





|1) Event Extraction<br>ZH<br>The final medal tally was led by Indonesia, followed by Thailand and host Philippines.<br>Zero-shot:<br>Comparison, Earnings_and_losses, Ranking<br>One-shot:<br>Achieve, Competition, Presence, Ranking, Rewards_and_punishments|2) Link Prediction<br>ZH<br>Zero-shot<br>Predict the tail entity [MASK] from the given (Academy Award for<br>Best Film Editing, award award category category of, [MASK]) by<br>completing the sentence "what is the category of of Academy Award<br>for Best Film Editing? The answer is ".<br>The answer is Academy Awards, so the [MASK] is Academy Awards.<br>One-shot<br>Predict the tail entity [MASK] from the given (Primetime Emmy<br>Award for Outstanding Guest Actress - Comedy Series, award award<br>category category of, [MASK]) by completing the sentence "what is<br>the category of of Primetime Emmy Award for Outstanding Guest<br>Actress - Comedy Series? The answer is".<br>Zero-shot:<br>The answer is "Comedy Series".<br>One-shot:<br>The answer is Primetime Emmy Awards. So, the [MASK] is Primetime<br>Emmy Awards.|
|---|---|
|||
|3) Question Answering<br>ZH<br>When did the films written by [A Gathering of Old Men] writers release ?<br>Answer: 1983|1987|1984|1993|2009|2010|1995<br>Answer: 1999|1974|1987|1995|1984|3) Question Answering<br>ZH<br>When did the films written by [A Gathering of Old Men] writers release ?<br>Answer: 1983|1987|1984|1993|2009|2010|1995<br>Answer: 1999|1974|1987|1995|1984|


**Fig. 3** Here are examples of task Event Extraction, Link Prediction and Question Answering.

_**Event Extraction.**_ For simplification, we conduct event detection experiments
on 20 random samples from MAVEN, encompassing all event types. Using the Fscore metric, GPT-4’s performance is benchmarked against the existing state-of-theart (SOTA) [48] model, as well as to other models in the GPT family. Based on our
results, GPT-4 shows inconsistent superiority over the SOTA, with both GPT-4 and
ChatGPT outperforming each other in different scenarios.

  - **Zero-shot** As shown in Table 1, GPT-4 outperforms ChatGPT. For the sentence “Now an established member of the line-up, he agreed to sing it more often.”,
ChatGPT generates the result _Becoming a_ ~~_m_~~ _ember_, while GPT-4 identifies two more:
_Agree_ ~~_o_~~ _r_ ~~_r_~~ _efuse to_ ~~_a_~~ _ct, Performing_ . It is worth noting that in this experiment, ChatGPT frequently provides answers with a single event type. In contrast, GPT-4’s ability
to grasp complex contextual information enables it to identify multiple event types
within these sentences.

  - **One-shot** In this configuration, ChatGPT’s performance improves notably, while
GPT-4 experiences a slight decline. Figure 3 illustrates that GPT-4 incorrectly identifies five event types where the correct answers are _Process_ ~~_e_~~ _nd_ and _Come_ ~~_t_~~ _ogether_ .
Despite detecting underlying ranking and comparison information, GPT-4 misses the

6

trigger words _final_ and _host_ . Simultaneously, we observe that under one-shot setup,
GPT-4 tends to produce a higher number of erroneous responses when it is unable to
identify the correct ones. We theorize this could stem from implicit type indications
of the dataset.

**Table 1** KG Construction and KG Reasoning tasks.

|Knowledge Graph Construction Knowledge Graph Reasoning<br>Model<br>DuIE2.0 Re-TACRED SciERC MAVEN FB15K-237 ATOMIC2020 FreebaseQA MetaQA|Col2|Col3|
|---|---|---|
|Fine-Tuned SOTA|69.42 91.4 53.2 68.8|32.4 46.9 79.0 100|



**Zero-shot**

text-davinci-003 11.43 9.8 4.0 30.0 16.0 15.1 ~~95~~ . ~~0~~ 33.9

ChatGPT 10.26 15.2 4.4 26.5 24.0 10.6 ~~95~~ . ~~0~~ 52.7

GPT-4 31.03 15.5 7.2 ~~34~~ . ~~2~~ 32.0 16.3 ~~95~~ . ~~0~~ ~~63~~ . ~~8~~

**One-shot**

text-davinci-003 30.63 12.8 4.8 25.0 32.0 14.1 ~~95~~ . ~~0~~ 49.5

ChatGPT 25.86 14.2 5.3 34.1 32.0 11.1 ~~95~~ . ~~0~~ 50.0

GPT-4 ~~41~~ . ~~91~~ ~~22~~ . ~~5~~ ~~9~~ . ~~1~~ 30.4 ~~40~~ . ~~0~~ ~~19~~ . ~~1~~ ~~95~~ . ~~0~~ 56.0

_**Link Prediction.**_ Task link prediction involves experiments on two distinct
datasets, FB15k-237 and ATOMIC2020. The former is a random sample set comprising 25 instances, whereas the latter encompasses 23 instances on behalf of all
possible relations. Among various approaches, the best performing fine-tuned models
are C-LMKE (BERT-base) [49] and COMET (BART) [50] for each.

  - **Zero-shot** In Table 1, GPT-4 on the FB15k-237 demonstrates that its hits@1
score is nearing the SOTA level. Regarding the ATOMIC2020, while GPT-4 still
exceeds the other two models, there remains a considerable discrepancy in terms of
bleu1 score between GPT-4’s performance and the fine-tuned SOTA achieved. In the
zero-shot context, it is observable that ChatGPT often refrains from providing immediate answers when faced with link prediction ambiguity, opting instead to seek further
contextual data. This cautious approach contrasts with GPT-4’s propensity to offer
direct responses, suggesting possible differences in their reasoning and decision-making
strategies.

  - **One-shot** Instructional text optimization has proven beneficial in enhancing
GPT series’ performance in link prediction tasks. Empirical evaluations demonstrate
one-shot GPT-4 improves results on both datasets, supporting accurate tail entity prediction in triples. In the example of Figure 3, the target _[MASK]_ is _Primetime Emmy_
_Award_ . In zero-shot setting, GPT-4 fails to comprehend the relation, leading to an
incorrect response _Comedy Series_ . However, when the demonstration is incorporated,
GPT-4 successfully identifies the target.
_**Question Answering.**_ We conduct the evaluation using two prevalent Knowledge Base Question Answering datasets, FreebaseQA and MetaQA, with 20 random
instances sampled from each. In MetaQA, we sample proportional to their dataset
representation. Yu et al. [51] and Madani and Joseph [52] represent the SOTA models employed. And for both datasets, AnswerExactMatch is adopted as the metric of
evaluation.

7

  - **Zero-shot** As shown in Table 1, ChatGPT and GPT-4 demonstrate identical
performance on FreebaseQA, surpassing preceding fully supervised SOTA by 16%.
Yet, no advantage of GPT-4 over ChatGPT is observed. For MetaQA, there is still a
large gap between LLMs and supervised SOTA, possibly due to multi-answer questions and LLM input token constraints. Nevertheless, GPT-4 outperforms ChatGPT
by 11.1 points, which indicates the superiority of GPT-4 against ChatGPT on more
challenging QA tasks. Specifically, in the example of Figure 3, GPT-4 correctly answers
a multi-hop question from MetaQA, yielding both _1999_ and _1974_ release dates,
highlighting its superior performance in multi-hop QA tasks over ChatGPT.

  - **One-shot** We also conduct experiments under one-shot setting by randomly
sampling one example from the train set as the in-context exemplar. Results in
Table 1 demonstrate that only text-davinci-003 benefits from the prompt, while both
ChatGPT and GPT-4 encounter a performance drop. This can be attributed to the
notorious alignment tax where models sacrifice some of their in-context learning ability
for aligning with human feedback.

**2.2.3 KG Construction vs. Reasoning**

Our experiments on KG construction and reasoning reveal that LLMs exhibit superior reasoning skills compared to their construction capabilities. Given the challenge
of quantifying reasoning and construction abilities, we assess the comparative capabilities of LLMs in these tasks by measuring the performance differential between LLMs
and the current SOTA methodologies. Larger performance disparities indicate poorer
performance. Despite the exemplary performance of LLMs, they do not surpass the
current state-of-the-art models in KG construction under zero-shot and one-shot set
tings, indicating limitations in extracting information from sparse data. Conversely,
all LLMs in one-shot, and GPT-4 in zero-shot, match or near SOTA performance
on the FreebaseQA and FB15K-237 datasets. Moreover, they exhibit relatively good
performance across the remaining datasets, which underscores their adaptability in
KG reasoning tasks as well. The intrinsic complexity of KG construction tasks may
account for this discrepancy in performance. Furthermore, the robust reasoning performance of LLMs might be attributed to their exposure to relevant knowledge during
pre-training.

**2.2.4 General vs. Specific Domain**

In our study, we evaluate the performance of large language models, exemplified by
GPT-4, across diverse knowledge domains, ensuring a balanced assessment in both
generic and specialized contexts. We employed a consistent method of evaluating relative task capabilities, similar to the performance disparity assessment described in
§2.2.3. The chosen benchmarks, SciERC and Re-TACRED, represent scientific and
general domains, respectively. While Re-TACRED exhibits a broader range of relation
types compared to the seven in SciERC, both GPT-4 and ChatGPT underperform on
the specialized SciERC dataset, indicating their limitations in domain-specific data.
Interestingly, GPT-4’s performance boost on SciERC is less pronounced than on ReTACRED when given one demonstration. Specifically, during our experiments, we note
challenges in LLMs’ recognition and understanding of specialized terms within the

8

SciERC dataset. We hypothesize that the subpar performance on specialized datasets
may stem from these models being predominantly trained on vast general corpora,
thereby lacking sufficient domain-specific expertise.

**2.3 Discussion: Why LLMs do not present satisfactory**
**performance on some tasks?**

Our experiments underscore GPT-4’s ability to extract knowledge across diverse
domains, albeit not surpassing the performance of fine-tuned models. This observation
also aligns with findings from previous research [25, 53]. Our experiment, conducted
in March-April 2023, uses an interactive interface rather than an API to evaluate the
GPT models on a randomly selected subset of datasets.
Notably, in assessing large models across eight datasets, we identify that the
outcomes may be subject to various factors. _**Dataset Quality**_ : Using the KG construction task as an illustration, dataset noise could lead to ambiguities. Complex
contexts and potential label inaccuracies may also negatively impact model evaluation. _**Instruction Quality**_ : Model performance is notably influenced by the semantic
depth of instructions. Finding optimal instructions through prompt engineering [3]

can enhance performance. An In-context Learning [54] approach with relevant samples can further improve outcomes. _**Evaluation Methods**_ : Current methods may
not be entirely apt for assessing the capabilities of large models like ChatGPT and
GPT-4. Dataset labels may not capture all correct responses, and answers involving
synonymous terms might not be accurately recognized.

**2.4 Discussion: Do LLMs have memorized knowledge or truly**
**have the generalization ability?**

Leveraging insights from prior studies, it is apparent that large models are adept at
swiftly extracting structured knowledge from minimal information. This observation
raises a question regarding the origin of the performance advantage in LLMs: is it due
to the substantial volume of textual data used in pre-training phases, enabling the
models to acquire pertinent knowledge, or is it attributed to their robust inference
and generalization capabilities?
To explore this, we design the **Virtual Knowledge Extraction** task, targeting
LLMs’ ability to generalize and extract unfamiliar knowledge. Unlike conventional
benchmarks, the task focuses on evaluating how models perform when confronted
with information they have not previously encountered, rather than relying solely on
knowledge accumulated during pre-training. Existing datasets largely comprise entities
familiar to LLMs, potentially sourced from their pre-training corpora, thereby possibly
including relationships already encoded within these corpora during extraction tasks.
Addressing these dataset constraints, we introduce **VINE**, a novel dataset specifically
crafted for Virtual Knowledge Extraction.
In VINE, we fabricate entities and relations not found in reality, structuring them
into knowledge triples. We then instruct the models to extract this synthetic knowledge, using the efficiency of this process as an indicator of LLMs’ capacity to manage

3 [https://www.kdnuggets.com/publications/sheets/ChatGPT](https://www.kdnuggets.com/publications/sheets/ChatGPT_Cheatsheet_Costa.pdf) ~~C~~ heatsheet ~~C~~ osta.pdf

9

![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2305.13168v4/2305.13168v4.pdf-9-0.png)



**Fig. 4** Prompts used in Virtual Knowledge Extraction. The blue box is the demonstration and the
pink box is the corresponding answer.

virtual knowledge. It is worth noting that we construct VINE based on the test set
of Re-TACRED. The primary idea behind this process is to replace existing entities
and relations in the original dataset with unseen ones, thereby creating unique virtual
knowledge scenarios.

**2.4.1 Data Collection**

Considering the vast training datasets of large models like GPT-4, it is challenging
for us to find the knowledge that they do not recognize. Using GPT-4 data up to
September 2021 as a basis, we select a portion of participants’ responses from two
competitions organized by _the New York Times_ in 2022 and 2023 as part of our data

sources.

However, due to the limited number of responses in the above contests and to
enhance data source diversity, we also create new words by randomly generating letter sequences. This is accomplished by generating random sequences between 7 to 9
characters in length (including 26 letters of the alphabet and the symbol “ ”) and
appending common noun suffixes at random to finalize the construction. More details
could be found in Appendix C.

**2.4.2 Preliminary Results**

In our experiment, we conduct a random selection of ten sentences for evaluation,
ensuring they encompass all relationships. We assess the performance of ChatGPT and
GPT-4 on these test samples after learning two demonstrations of the same relation.
Notably, GPT-4 successfully extracted 80% of the virtual triples, while the accuracy
of ChatGPT is only 27%.

10

In Figure 4, we provide large models with a triple composed of virtual relation types
and virtual head and tail entities— _[Schoolnogo, decidiaster, Reptance]_ and _[Intran-_
_guish, decidiaster, Nugculous]_ —along with the respective demonstrations. The results
demonstrate that GPT-4 effectively completed the extraction of the virtual triple.
Consequently, we tentatively conclude that GPT-4 exhibits a relatively strong generalization ability and can rapidly acquire the capability to extract new knowledge
through instructions, rather than relying solely on the memory of relevant knowledge.
Related work [55] has also confirmed that large models possess an exceptionally strong
generalization ability concerning instructions.
### **3 Future Opportunities: Automatic KG** **Construction and Reasoning**

In contemplating the trajectory of Knowledge Graph, the pronounced merits of large
language models become evident. They not only optimize resource utilization but also
outperform smaller models in adaptability, especially in varied application domains
and data-limited settings. Such strengths position them as primary tools for KG construction and reasoning. Yet, while the prowess of LLMs is impressive, researchers
have identified certain limitations, such as misalignment with human preferences and
the tendency for hallucinations. The efficacy of models like ChatGPT heavily leans on
human engagement in dialogue generation. Further refining model responses necessitates intricate user task descriptions and enriched interaction contexts, a process that
remains demanding and time-intensive in the development lifecycle. Consequently,
there is a growing interest in the realm of interactive natural language processing
(iNLP) [37]. In parallel, research efforts concerning intelligent agents continue to proliferate [56–58]. A notable example of this advancement is AutoGPT [4], which can
independently generate prompts and carry out tasks such as event analysis, programming, and mathematical operations. Concurrently, Li et al. [59] delves into the
potential for autonomous cooperation between communicative agents and introduces
a novel cooperative agent framework called _role-playing_ .
In light of our findings, we propose the use of communicative intelligent agents for
KG construction, leveraging different roles assigned to multiple agents to collaborate
on KG tasks based on their mutual knowledge. Considering the knowledge cutoff
prevalent in large models during the pre-training phase, we suggest the incorporation
of external sources to assist task completion. These sources can include knowledge
bases, existing KGs, and internet retrieval systems, among others. Here we name this
**AutoKG** .

For a simple demonstration of the concept, we utilize the _role-playing_ method
in CAMEL [59]. As depicted in Figure 5, we designate the _KG assistant_ agent as a
_Consultant_ and the _KG user_ agent as a _KG domain expert_ . Upon receipt of the prompt
and assigned roles, the task-specifier agent provides an elaborate description to clarify
the concept. Following this, the _KG assistant_ and _KG user_ collaborate in a multiparty setting to complete the specified task until the _KG user_ confirms its completion.
Concurrently, a _web searcher_ role is introduced to aid the _KG assistant_ in internet

4 [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)

11

![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2305.13168v4/2305.13168v4.pdf-11-0.png)










**Fig. 5** Illustration of **AutoKG**, that integrates KG construction and reasoning by employing GPT4 and communicative agents based on ChatGPT. The figure omits the specific operational process,
providing the results directly.

knowledge retrieval. When the _KG assistant_ receives a dialogue from the _KG user_, it
initially consults the _web searcher_ on whether to browse information online based on
the content. Guided by the _web searcher_ ’s response, the _KG assistant_ then continues
to address the _KG user_ ’s command. The experimental example indicates that the
knowledge graph related to the film _Spider-Man: Across the Spider-Verse_ released in
2023 is more effectively and comprehensively constructed using the multi-agent and
internet-augmented approach.
_**Remark.**_ By combining the efforts of artificial intelligence and human expertise,
AutoKG could speed up the creation of specialized KGs, fostering a collaborative environment with language models. This system leverages domain and internet knowledge
to produce high-quality KGs, augmenting the factual accuracy of LLMs in domainspecific tasks, thereby increasing their practical utility. AutoKG not only simplifies
the construction process but also improves LLMs’ transparency, facilitating a deeper

12

understanding of their internal workings. As a cooperative human-machine platform,
it bolsters the understanding and guidance of LLMs’ decision-making, increasing their
efficiency in complex tasks. However, it is noteworthy that despite the assistance
of AutoKG, the current results of the constructed knowledge graph still necessitate
manual evaluation and validation.

Furthermore, three significant challenges remain when utilizing AutoKG, necessitating further research and resolution: **The utilization of the API is constrained**
**by a maximum token limit.** Currently, the gpt-3.5-turbo in use is subjected to
a max token restriction. This constraint impacts the construction of KGs. **AutoKG**
**now exhibits shortcomings in facilitating efficient human-machine interac-**
**tion.** In fully autonomous machine operations, human oversight for immediate error
correction is lacking, yet incorporating human involvement in every step will increase
time and labor costs substantially. **Hallucination problem of LLMs.** Given the
known propensity of LLMs to generate non-factual information, it’s imperative to
scrutinize outputs from them. This can be achieved via comparison with standard
answers, expert review, or through semi-automatic algorithms.
### **4 Conclusion and Future Work**

In this paper, we investigate LLMs for KG construction and reasoning. We question whether LLMs’ extraction abilities arise from their vast pre-training corpus or
their strong contextual learning capabilities. To investigate this, we conduct a Virtual Knowledge Extraction task using a novel dataset, with results highlighting the
LLMs’ robust contextual learning. Furthermore, we propose an innovative method of
AutoKG for accomplishing KG construction and reasoning tasks by employing multiple agents. In the future, we would like to extend our work to other LLMs and explore
additional KG-related tasks, such as multimodal reasoning.
### **Declarations**

- Funding. This work was supported by the National Natural Science Foundation of
China (No. 62206246, No. NSFCU23B2055, No. NSFCU19B2027), the Fundamental
Research Funds for the Central Universities (226-2023-00138), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Yongjiang Talent
Introduction Programme (2021A-156-G), Tencent AI Lab Rhino-Bird Focused
Research Program (RBFR2024003), Information Technology Center and State
Key Lab of CAD&CG, Zhejiang University, and NUS-NCS Joint Laboratory
(A-0008542-00-00).

- Ethics approval and consent to participate. This work did not involve any human
participants, their data, or biological materials, and therefore did not require ethical
approval.

- Data and Materials availability. Our data and materials are accessible in the
repository here [5] .

5 [The code and datasets are in https://github.com/zjunlp/AutoKG](https://github.com/zjunlp/AutoKG)

13

### **References**

[1] Cai, B., Xiang, Y., Gao, L., Zhang, H., Li, Y., Li, J.: Temporal knowledge graph
completion: A survey. In: Proceedings of the Thirty-Second International Joint
Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao,
[SAR, China, pp. 6545–6553 (2023). https://doi.org/10.24963/IJCAI.2023/734](https://doi.org/10.24963/IJCAI.2023/734)

[2] Zhu, X., Li, Z., Wang, X., Jiang, X., Sun, P., Wang, X., Xiao, Y., Yuan, N.J.:
Multi-modal knowledge graph construction and application: A survey. IEEE
Trans. Knowl. Data Eng. **36** [(2), 715–735 (2024) https://doi.org/10.1109/TKDE.](https://doi.org/10.1109/TKDE.2022.3224228)
[2022.3224228](https://doi.org/10.1109/TKDE.2022.3224228)

[3] Liang, K.Y., Meng, L., Liu, M., Liu, Y., Tu, W., Wang, S., Zhou, S., Liu, X., Sun,
F.: A survey of knowledge graph reasoning on graph types: Static, dynamic, and
multi-modal. IEEE transactions on pattern analysis and machine intelligence **PP**
(2022)

[4] Chen, X., Zhang, J., Wang, X., Wu, T., Deng, S., Wang, Y., Si, L., Chen, H.,
Zhang, N.: Continual multimodal knowledge graph construction. In: Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence,
IJCAI 2024 (2024)

[5] Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J., Wu, X.: Unifying large language
models and knowledge graphs: A roadmap. IEEE Trans. Knowl. Data Eng. **36** (7),
[3580–3599 (2024) https://doi.org/10.1109/TKDE.2024.3352100](https://doi.org/10.1109/TKDE.2024.3352100)

[6] Pan, J.Z., Razniewski, S., Kalo, J., Singhania, S., Chen, J., Dietze, S., Jabeen,
H., Omeliyanenko, J., Zhang, W., Lissandrini, M., Biswas, R., Melo, G., Bonifati,
A., Vakaj, E., Dragoni, M., Graux, D.: Large language models and knowledge
graphs: Opportunities and challenges. TGDK **1** [(1), 2–1238 (2023) https://doi.](https://doi.org/10.4230/.1.1.2)
[org/10.4230/.1.1.2](https://doi.org/10.4230/.1.1.2)

[7] Ye, H., Zhang, N., Chen, H., Chen, H.: Generative knowledge graph construction: A review. In: Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emi[rates, December 7-11, 2022, pp. 1–17 (2022). https://doi.org/10.18653/v1/2022.](https://doi.org/10.18653/v1/2022.emnlp-main.1)
[emnlp-main.1](https://doi.org/10.18653/v1/2022.emnlp-main.1)

[8] Ding, L., Zhou, S., Xiao, J., Han, J.: Automated construction of theme-specific
[knowledge graphs. CoRR (2024) https://doi.org/10.48550/ARXIV.2404.19146](https://doi.org/10.48550/ARXIV.2404.19146)

[9] Chiu, J.P.C., Nichols, E.: Named entity recognition with bidirectional lstm-cnns.
Trans. Assoc. Comput. Linguistics **4** [, 357–370 (2016) https://doi.org/10.1162/](https://doi.org/10.1162/tacl_a_00104)
tacl ~~a~~ [00104](https://doi.org/10.1162/tacl_a_00104)

[10] Gui, H., Yuan, L., Ye, H., Zhang, N., Sun, M., Liang, L., Chen, H.: Iepile:

14

Unearthing large-scale schema-based information extraction corpus. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers) (2024)

[11] Zeng, D., Liu, K., Chen, Y., Zhao, J.: Distant supervision for relation extraction via piecewise convolutional neural networks. In: M`arquez, L., Callison-Burch,
C., Su, J., Pighin, D., Marton, Y. (eds.) Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Por[tugal, September 17-21, 2015, pp. 1753–1762 (2015). https://doi.org/10.18653/](https://doi.org/10.18653/v1/d15-1203)
[v1/d15-1203](https://doi.org/10.18653/v1/d15-1203)

[12] Chen, X., Zhang, N., Xie, X., Deng, S., Yao, Y., Tan, C., Huang, F., Si, L., Chen,
H.: Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization
for relation extraction. In: Laforest, F., Troncy, R., Simperl, E., Agarwal, D.,
Gionis, A., Herman, I., M´edini, L. (eds.) WWW ’22: The ACM Web Conference
2022, Virtual Event, Lyon, France, April 25 - 29, 2022, pp. 2778–2788 (2022).
[https://doi.org/10.1145/3485447.3511998](https://doi.org/10.1145/3485447.3511998)

[13] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume
[1: Long Papers, pp. 167–176 (2015). https://doi.org/10.3115/v1/p15-1017](https://doi.org/10.3115/v1/p15-1017)

[14] Deng, S., Zhang, N., Kang, J., Zhang, Y., Zhang, W., Chen, H.: Meta-learning
with dynamic-memory-based prototypical network for few-shot event detection.
In: Caverlee, J., Hu, X.B., Lalmas, M., Wang, W. (eds.) WSDM ’20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston,
[TX, USA, February 3-7, 2020, pp. 151–159 (2020). https://doi.org/10.1145/](https://doi.org/10.1145/3336191.3371796)
[3336191.3371796](https://doi.org/10.1145/3336191.3371796)

[15] Shen, W., Wang, J., Han, J.: Entity linking with a knowledge base: Issues, techniques, and solutions. IEEE Trans. Knowl. Data Eng. **27** (2), 443–460 (2015)
[https://doi.org/10.1109/TKDE.2014.2327028](https://doi.org/10.1109/TKDE.2014.2327028)

[16] Zhang, Y., Dai, H., Kozareva, Z., Smola, A.J., Song, L.: Variational reasoning for
question answering with knowledge graph. In: McIlraith, S.A., Weinberger, K.Q.
(eds.) Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications of Artificial Intelligence
(IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial
Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018 (2018).
[https://doi.org/10.1609/aaai.v32i1.12057](https://doi.org/10.1609/aaai.v32i1.12057)

[17] Rossi, A., Barbosa, D., Firmani, D., Matinata, A., Merialdo, P.: Knowledge
graph embedding for link prediction: A comparative analysis. ACM Trans. Knowl.
Discov. Data **15** [(2), 14–11449 (2021) https://doi.org/10.1145/3424672](https://doi.org/10.1145/3424672)

15

[18] Karpukhin, V., Oguz, B., Min, S., Lewis, P.S.H., Wu, L., Edunov, S., Chen, D.,
Yih, W.: Dense passage retrieval for open-domain question answering. In: Webber,
B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November
[16-20, 2020 (2020). https://doi.org/10.18653/v1/2020.emnlp-main.550](https://doi.org/10.18653/v1/2020.emnlp-main.550)

[19] Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., Chua, T.: Retrieving and reading:
A comprehensive survey on open-domain question answering. CoRR (2021)

[20] OpenAI: GPT-4 technical report. CoRR **abs/2303.08774** [(2023) https://doi.](https://doi.org/10.48550/arXiv.2303.08774)
[org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)

[21] Liu, A., Hu, X., Wen, L., Yu, P.S.: A comprehensive evaluation of chatgpt’s zero[shot text-to-sql capability. CoRR (2023) https://doi.org/10.48550/arXiv.2303.](https://doi.org/10.48550/arXiv.2303.13547)
[13547](https://doi.org/10.48550/arXiv.2303.13547)

[22] Shakarian, P., Koyyalamudi, A., Ngu, N., Mareedu, L.: An independent evaluation
of chatgpt on mathematical word problems (MWP). In: Martin, A., Fill, H., Gerber, A., Hinkelmann, K., Lenat, D., Stolle, R., Harmelen, F. (eds.) Proceedings
of the AAAI 2023 Spring Symposium on Challenges Requiring the Combination
of Machine Learning and Knowledge Engineering (AAAI-MAKE 2023), Hyatt
Regency, San Francisco Airport, California, USA, March 27-29, 2023. CEUR
Workshop Proceedings, vol. 3433 (2023)

[23] Lai, V.D., Ngo, N.T., Veyseh, A.P.B., Man, H., Dernoncourt, F., Bui, T.,
Nguyen, T.H.: Chatgpt beyond english: Towards a comprehensive evaluation of
large language models in multilingual learning. In: Bouamor, H., Pino, J., Bali,
K. (eds.) Findings of the Association for Computational Linguistics: EMNLP
[2023, Singapore, December 6-10, 2023 (2023). https://doi.org/10.18653/v1/2023.](https://doi.org/10.18653/v1/2023.findings-emnlp.878)
[findings-emnlp.878](https://doi.org/10.18653/v1/2023.findings-emnlp.878)

[24] Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B.,
Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li,
Y., Tang, X., Liu, Z., Liu, P., Nie, J., Wen, J.: A survey of large language models.
[CoRR (2023) https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)

[25] Wei, X., Cui, X., Cheng, N., Wang, X., Zhang, X., Huang, S., Xie, P., Xu, J.,
Chen, Y., Zhang, M., Jiang, Y., Han, W.: Zero-shot information extraction via
chatting with chatgpt. CoRR **abs/2302.10205** [(2023) https://doi.org/10.48550/](https://doi.org/10.48550/arXiv.2302.10205)
[arXiv.2302.10205](https://doi.org/10.48550/arXiv.2302.10205)

[26] Li, B., Fang, G., Yang, Y., Wang, Q., Ye, W., Zhao, W., Zhang, S.: Evaluating chatgpt’s information extraction capabilities: An assessment of performance,
explainability, calibration, and faithfulness. CoRR (2023)

[27] Li, G., Wang, P., Ke, W.: Revisiting large language models as zero-shot relation
extractors. In: Bouamor, H., Pino, J., Bali, K. (eds.) Findings of the Association

16

for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023
[(2023). https://doi.org/10.18653/v1/2023.findings-emnlp.459](https://doi.org/10.18653/v1/2023.findings-emnlp.459)

[28] Wan, Z., Cheng, F., Mao, Z., Liu, Q., Song, H., Li, J., Kurohashi, S.: GPTRE: in-context learning for relation extraction using large language models. In:
Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore,
[December 6-10, 2023 (2023). https://doi.org/10.18653/v1/2023.emnlp-main.214](https://doi.org/10.18653/v1/2023.emnlp-main.214)

[29] Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., Yang, D.: Is chatgpt a
general-purpose natural language processing task solver? In: Bouamor, H., Pino,
J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023
[(2023). https://doi.org/10.18653/v1/2023.emnlp-main.85](https://doi.org/10.18653/v1/2023.emnlp-main.85)

[30] Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., Zhang, Y.: Evaluating the logical
[reasoning ability of chatgpt and GPT-4. CoRR (2023) https://doi.org/10.48550/](https://doi.org/10.48550/ARXIV.2304.03439)
[ARXIV.2304.03439](https://doi.org/10.48550/ARXIV.2304.03439)

[31] Jiang, J., Zhou, K., Zhao, W.X., Song, Y., Zhu, C., Zhu, H., Wen, J.: Kg-agent:
An efficient autonomous agent framework for complex reasoning over knowledge
[graph. CoRR (2024) https://doi.org/10.48550/ARXIV.2402.11163](https://doi.org/10.48550/ARXIV.2402.11163)

[32] Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.W., Tay, Y., Zhou, D., Le,
Q.V., Zoph, B., Wei, J., Roberts, A.: The flan collection: Designing data and
methods for effective instruction tuning. In: Krause, A., Brunskill, E., Cho, K.,
Engelhardt, B., Sabato, S., Scarlett, J. (eds.) International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA. Proceedings of
Machine Learning Research (2023)

[33] Christiano, P.F., Leike, J., Brown, T.B., Martic, M., Legg, S., Amodei, D.: Deep
reinforcement learning from human preferences. In: Guyon, I., Luxburg, U., Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Garnett, R. (eds.)
Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
CA, USA (2017)

[34] Leiter, C., Zhang, R., Chen, Y., Belouadi, J., Larionov, D., Fresen, V., Eger,
[S.: Chatgpt: A meta-analysis after 2.5 months. CoRR (2023) https://doi.org/10.](https://doi.org/10.48550/ARXIV.2302.13795)
[48550/ARXIV.2302.13795](https://doi.org/10.48550/ARXIV.2302.13795)

[35] Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., Zhong, S., Yin, B., Hu,
X.B.: Harnessing the power of llms in practice: A survey on chatgpt and beyond.
ACM Trans. Knowl. Discov. Data **18** [(6), 160–116032 (2024) https://doi.org/10.](https://doi.org/10.1145/3649506)
[1145/3649506](https://doi.org/10.1145/3649506)

[36] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.H., Le,

17

Q.V., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language
models. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A.
(eds.) Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022 (2022)

[37] Wang, Z., Zhang, G., Yang, K., Shi, N., Zhou, W., Hao, S., Xiong, G., Li, Y.,
Sim, M.Y., Chen, X., Zhu, Q., Yang, Z., Nik, A., Liu, Q., Lin, C., Wang, S.,
Liu, R., Chen, W., Xu, K., Liu, D., Guo, Y., Fu, J.: Interactive natural language
[processing. CoRR (2023) https://doi.org/10.48550/arXiv.2305.13246](https://doi.org/10.48550/arXiv.2305.13246)

[38] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E.,
Lee, P., Lee, Y.T., Li, Y., Lundberg, S.M., Nori, H., Palangi, H., Ribeiro, M.T.,
Zhang, Y.: Sparks of artificial general intelligence: Early experiments with GPT-4.
[CoRR (2023) https://doi.org/10.48550/ARXIV.2303.12712](https://doi.org/10.48550/ARXIV.2303.12712)

[39] Li, S., He, W., Shi, Y., Jiang, W., Liang, H., Jiang, Y., Zhang, Y., Lyu, Y., Zhu,
Y.: Duie: A large-scale chinese dataset for information extraction. In: Tang, J.,
Kan, M., Zhao, D., Li, S., Zan, H. (eds.) Natural Language Processing and Chinese
Computing - 8th CCF International Conference, NLPCC 2019, Dunhuang, China,
October 9-14, 2019, Proceedings, Part II. Lecture Notes in Computer Science,
[vol. 11839, pp. 791–800 (2019). https://doi.org/10.1007/978-3-030-32236-6](https://doi.org/10.1007/978-3-030-32236-6_72) ~~7~~ 2

[40] Luan, Y., He, L., Ostendorf, M., Hajishirzi, H.: Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In:
Riloff, E., Chiang, D., Hockenmaier, J., Tsujii, J. (eds.) Proceedings of the
2018 Conference on Empirical Methods in Natural Language Processing, Brus[sels, Belgium, October 31 - November 4, 2018, pp. 3219–3232 (2018). https:](https://doi.org/10.18653/v1/d18-1360)
[//doi.org/10.18653/v1/d18-1360](https://doi.org/10.18653/v1/d18-1360)

[41] Stoica, G., Platanios, E.A., P´oczos, B.: Re-tacred: Addressing shortcomings of the
TACRED dataset. In: Thirty-Fifth AAAI Conference on Artificial Intelligence,
AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial
Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances
in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp.
13843–13850 (2021)

[42] Wang, X., Wang, Z., Han, X., Jiang, W., Han, R., Liu, Z., Li, J., Li, P., Lin,
Y., Zhou, J.: MAVEN: A massive general domain event detection dataset. In:
Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
[November 16-20, 2020, pp. 1652–1671 (2020). https://doi.org/10.18653/v1/2020.](https://doi.org/10.18653/v1/2020.emnlp-main.129)
[emnlp-main.129](https://doi.org/10.18653/v1/2020.emnlp-main.129)

[43] Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., Gamon, M.: Representing text for joint embedding of text and knowledge bases. In: M`arquez,
L., Callison-Burch, C., Su, J., Pighin, D., Marton, Y. (eds.) Proceedings of

18

the 2015 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 1499–1509 (2015).
[https://doi.org/10.18653/v1/d15-1174](https://doi.org/10.18653/v1/d15-1174)

[44] Hwang, J.D., Bhagavatula, C., Bras, R.L., Da, J., Sakaguchi, K., Bosselut, A.,
Choi, Y.: (comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI
2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial
Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 6384–6392 (2021)

[45] Jiang, K., Wu, D., Jiang, H.: Freebaseqa: A new factoid QA data set matching trivia-style question-answer pairs with freebase. In: Burstein, J., Doran, C.,
Solorio, T. (eds.) Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol[ume 1 (Long and Short Papers), pp. 318–323 (2019). https://doi.org/10.18653/](https://doi.org/10.18653/v1/n19-1028)
[v1/n19-1028](https://doi.org/10.18653/v1/n19-1028)

[46] Ye, D., Lin, Y., Li, P., Sun, M.: Packed levitated marker for entity and relation
extraction. In: Muresan, S., Nakov, P., Villavicencio, A. (eds.) Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 4904–4917
[(2022). https://doi.org/10.18653/v1/2022.acl-long.337](https://doi.org/10.18653/v1/2022.acl-long.337)

[47] Park, S., Kim, H.: Improving sentence-level relation extraction through curriculum learning. CoRR (2021)

[48] Wang, S., Yu, M., Huang, L.: The art of prompting: Event detection based on type
specific prompts. In: Rogers, A., Boyd-Graber, J.L., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp.
[1286–1299 (2023). https://doi.org/10.18653/v1/2023.acl-short.111](https://doi.org/10.18653/v1/2023.acl-short.111)

[49] Wang, X., He, Q., Liang, J., Xiao, Y.: Language models as knowledge embeddings. In: Raedt, L.D. (ed.) Proceedings of the Thirty-First International Joint
Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July
[2022, pp. 2291–2297 (2022). https://doi.org/10.24963/ijcai.2022/318](https://doi.org/10.24963/ijcai.2022/318)

[50] Hwang, J.D., Bhagavatula, C., Bras, R.L., Da, J., Sakaguchi, K., Bosselut, A.,
Choi, Y.: (comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs. In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI
2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial
Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 6384–6392 (2021).
[https://doi.org/10.1609/aaai.v35i7.16792](https://doi.org/10.1609/aaai.v35i7.16792)

19

[51] Yu, D., Zhang, S., Ng, P., Zhu, H., Li, A.H., Wang, J., Hu, Y., Wang, W.Y., Wang,
Z., Xiang, B.: Decaf: Joint decoding of answers and logical forms for question
answering over knowledge bases. In: The Eleventh International Conference on
Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 (2023)

[52] Madani, N., Joseph, K.: Answering questions over knowledge graphs using logic
programming along with language models. In: Maughan, K., Liu, R., Burns, T.F.
(eds.) The First Tiny Papers Track at ICLR 2023, Tiny Papers @ ICLR 2023,
Kigali, Rwanda, May 5, 2023 (2023)

[53] Gao, J., Zhao, H., Yu, C., Xu, R.: Exploring the feasibility of chatgpt for event
[extraction. CoRR (2023) https://doi.org/10.48550/arXiv.2303.03836](https://doi.org/10.48550/arXiv.2303.03836)

[54] Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L.,
[Sui, Z.: A survey for in-context learning. CoRR (2023) https://doi.org/10.48550/](https://doi.org/10.48550/ARXIV.2301.00234)
[ARXIV.2301.00234](https://doi.org/10.48550/ARXIV.2301.00234)

[55] Wei, J.W., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H.,
Huang, D., Zhou, D., Ma, T.: Larger language models do in-context learning
differently. CoRR (2023)

[56] Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J.,
Chen, X., Lin, Y., Zhao, W.X., Wei, Z., Wen, J.: A survey on large language
model based autonomous agents. Frontiers Comput. Sci. **18** (6), 186345 (2024)
[https://doi.org/10.1007/S11704-024-40231-1](https://doi.org/10.1007/S11704-024-40231-1)

[57] Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J.,
Jin, S., Zhou, E., Zheng, R., Fan, X., Wang, X., Xiong, L., Zhou, Y., Wang, W.,
Jiang, C., Zou, Y., Liu, X., Yin, Z., Dou, S., Weng, R., Cheng, W., Zhang, Q.,
Qin, W., Zheng, Y., Qiu, X., Huan, X., Gui, T.: The rise and potential of large
[language model based agents: A survey. CoRR (2023) https://doi.org/10.48550/](https://doi.org/10.48550/arXiv.2309.07864)
[arXiv.2309.07864](https://doi.org/10.48550/arXiv.2309.07864)

[58] Zhao, P., Jin, Z., Cheng, N.: An in-depth survey of large language model-based
[artificial intelligence agents. CoRR (2023) https://doi.org/10.48550/arXiv.2309.](https://doi.org/10.48550/arXiv.2309.14365)
[14365](https://doi.org/10.48550/arXiv.2309.14365)

[59] Li, G., Hammoud, H.A.A.K., Itani, H., Khizbullin, D., Ghanem, B.: Camel: Communicative agents for ”mind” exploration of large language model society. In:
Thirty-seventh Conference on Neural Information Processing Systems (2023)

[60] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,
Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,
J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. In: Larochelle, H., Ranzato, M., Hadsell,

20

R., Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
[NeurIPS 2020, December 6-12, 2020, Virtual (2020). https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)
[cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)

[61] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama,
D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O.,
Liang, P., Dean, J., Fedus, W.: Emergent abilities of large language models. Trans.
Mach. Learn. Res. **2022** (2022)

[62] Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji,
Z., Yu, T., Chung, W., Do, Q.V., Xu, Y., Fung, P.: A multitask, multilingual,
multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.
In: Park, J.C., Arase, Y., Hu, B., Lu, W., Wijaya, D., Purwarianti, A., Krisnadhi, A.A. (eds.) Proceedings of the 13th International Joint Conference on
Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, IJCNLP 2023 -Volume
1: Long Papers, Nusa Dua, Bali, November 1 - 4, 2023, pp. 675–718 (2023).
[https://doi.org/10.18653/v1/2023.ijcnlp-main.45](https://doi.org/10.18653/v1/2023.ijcnlp-main.45)

[63] Nori, H., King, N., McKinney, S.M., Carignan, D., Horvitz, E.: Capabilities of
[GPT-4 on medical challenge problems. CoRR (2023) https://doi.org/10.48550/](https://doi.org/10.48550/ARXIV.2303.13375)
[ARXIV.2303.13375](https://doi.org/10.48550/ARXIV.2303.13375)

[64] Qiao, S., Ou, Y., Zhang, N., Chen, X., Yao, Y., Deng, S., Tan, C., Huang, F.,
Chen, H.: Reasoning with language model prompting: A survey. In: Rogers, A.,
Boyd-Graber, J.L., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), ACL
[2023, Toronto, Canada, July 9-14, 2023, pp. 5368–5393 (2023). https://doi.org/](https://doi.org/10.18653/v1/2023.acl-long.294)
[10.18653/v1/2023.acl-long.294](https://doi.org/10.18653/v1/2023.acl-long.294)

[65] S´anchez, R.J., Conrads, L., Welke, P., Cvejoski, K., Marin, C.O.: Hidden schema
networks. In: Rogers, A., Boyd-Graber, J.L., Okazaki, N. (eds.) Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 4764–4798
[(2023). https://doi.org/10.18653/v1/2023.acl-long.263](https://doi.org/10.18653/v1/2023.acl-long.263)

[66] Ma, Y., Cao, Y., Hong, Y., Sun, A.: Large language model is not a good few-shot
information extractor, but a good reranker for hard samples! In: Bouamor, H.,
Pino, J., Bali, K. (eds.) Findings of the Association for Computational Linguistics:
[EMNLP 2023, Singapore, December 6-10, 2023, pp. 10572–10601 (2023). https:](https://doi.org/10.18653/v1/2023.findings-emnlp.710)
[//doi.org/10.18653/v1/2023.findings-emnlp.710](https://doi.org/10.18653/v1/2023.findings-emnlp.710)

[67] Jeblick, K., Schachtner, B., Dexl, J., Mittermeier, A., St¨uber, A.T., Topalis, J.,
Weber, T., Wesp, P., Sabel, B.O., Ricke, J., Ingrisch, M.: Chatgpt makes medicine
easy to swallow: An exploratory case study on simplified radiology reports. CoRR
[(2022) https://doi.org/10.48550/arXiv.2212.14882](https://doi.org/10.48550/arXiv.2212.14882)

21

[68] Tan, Y., Min, D., Li, Y., Li, W., Hu, N., Chen, Y., Qi, G.: Can chatgpt
replace traditional KBQA models? an in-depth analysis of the question answering performance of the GPT LLM family. In: Payne, T.R., Presutti, V., Qi, G.,
Poveda-Villal´on, M., Stoilos, G., Hollink, L., Kaoudi, Z., Cheng, G., Li, J. (eds.)
The Semantic Web - ISWC 2023 - 22nd International Semantic Web Confer
ence, Athens, Greece, November 6-10, 2023, Proceedings, Part I. Lecture Notes
[in Computer Science, vol. 14265, pp. 348–367 (2023). https://doi.org/10.1007/](https://doi.org/10.1007/978-3-031-47240-4_19)
[978-3-031-47240-4](https://doi.org/10.1007/978-3-031-47240-4_19) ~~1~~ 9

[69] Jiao, W., Wang, W., Huang, J., Wang, X., Tu, Z.: Is chatgpt A good translator?
[A preliminary study. CoRR (2023) https://doi.org/10.48550/arXiv.2301.08745](https://doi.org/10.48550/arXiv.2301.08745)

[70] Kasai, J., Kasai, Y., Sakaguchi, K., Yamada, Y., Radev, D.: Evaluating GPT[4 and chatgpt on japanese medical licensing examinations. CoRR (2023) https:](https://doi.org/10.48550/ARXIV.2303.18027)
[//doi.org/10.48550/ARXIV.2303.18027](https://doi.org/10.48550/ARXIV.2303.18027)

[71] Sifatkaur, Singh, M., B, V.S., Malviya, N.: Mind meets machine: Unravelling
[gpt-4’s cognitive psychology. CoRR (2023) https://doi.org/10.48550/arXiv.2303.](https://doi.org/10.48550/arXiv.2303.11436)
[11436](https://doi.org/10.48550/arXiv.2303.11436)

[72] Nunes, D., Primi, R., Pires, R., Alencar Lotufo, R., Nogueira, R.F.: Evaluating GPT-3.5 and GPT-4 models on brazilian university admission exams. CoRR
[(2023) https://doi.org/10.48550/arXiv.2303.17003](https://doi.org/10.48550/arXiv.2303.17003)

[73] Lyu, Q., Tan, J., Zapadka, M.E., Ponnatapuram, J., Niu, C., Wang, G., Whitlow,
C.T.: Translating radiology reports into plain language using chatgpt and GPT-4
with prompt learning: Promising results, limitations, and potential. Vis. Comput.
[Ind. Biomed. Art 6, 9 (2023) https://doi.org/10.1186/s42492-023-00136-5](https://doi.org/10.1186/s42492-023-00136-5)

[74] Li, D., Tan, Z., Chen, T., Liu, H.: Contextualization distillation from large
language model for knowledge graph completion. In: Graham, Y., Purver, M.
(eds.) Findings of the Association for Computational Linguistics: EACL 2024,
[St. Julian’s, Malta, March 17-22, 2024, pp. 458–477 (2024). https://aclanthology.](https://aclanthology.org/2024.findings-eacl.32)
[org/2024.findings-eacl.32](https://aclanthology.org/2024.findings-eacl.32)

[75] Li, F., Lin, Z., Zhang, M., Ji, D.: A span-based model for joint overlapped and
discontinuous named entity recognition. In: Zong, C., Xia, F., Li, W., Navigli,
R. (eds.) Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural
Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual
[Event, August 1-6, 2021, pp. 4814–4828 (2021). https://doi.org/10.18653/v1/](https://doi.org/10.18653/v1/2021.acl-long.372)
[2021.acl-long.372](https://doi.org/10.18653/v1/2021.acl-long.372)

[76] Zhou, W., Zhang, S., Gu, Y., Chen, M., Poon, H.: Universalner: Targeted distillation from large language models for open named entity recognition. In:
The Twelfth International Conference on Learning Representations, ICLR 2024
[(2024). https://openreview.net/forum?id=r65xfUb76p](https://openreview.net/forum?id=r65xfUb76p)

22

[77] Jiang, P., Lin, J., Wang, Z., Sun, J., Han, J.: GenRES: Rethinking evaluation for generative relation extraction in the era of large language models. In:
Duh, K., Gomez, H., Bethard, S. (eds.) Proceedings of the 2024 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 2820–2837.
[Association for Computational Linguistics, Mexico City, Mexico (2024). https:](https://aclanthology.org/2024.naacl-long.155)
[//aclanthology.org/2024.naacl-long.155](https://aclanthology.org/2024.naacl-long.155)

[78] Wang, L., Zhao, W., Wei, Z., Liu, J.: Simkgc: Simple contrastive knowledge graph
completion with pre-trained language models. In: Muresan, S., Nakov, P., Villavicencio, A. (eds.) Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ire[land, May 22-27, 2022, pp. 4281–4294 (2022). https://doi.org/10.18653/v1/2022.](https://doi.org/10.18653/v1/2022.acl-long.295)
[acl-long.295](https://doi.org/10.18653/v1/2022.acl-long.295)

[79] Li, D., Zhu, B., Yang, S., Xu, K., Yi, M., He, Y., Wang, H.: Multi-task pre-training
language model for semantic network completion. ACM Trans. Asian Low Resour.
Lang. Inf. Process. **22** [(11), 250–125020 (2023) https://doi.org/10.1145/3627704](https://doi.org/10.1145/3627704)

[80] Shu, D., Chen, T., Jin, M., Zhang, Y., Zhang, C., Du, M., Zhang, Y.: Knowledge
[graph large language model (KG-LLM) for link prediction. CoRR (2024) https:](https://doi.org/10.48550/ARXIV.2403.07311)
[//doi.org/10.48550/ARXIV.2403.07311](https://doi.org/10.48550/ARXIV.2403.07311)

[81] Hao, S., Tan, B., Tang, K., Ni, B., Shao, X., Zhang, H., Xing, E.P., Hu, Z.: Bertnet:
Harvesting knowledge graphs with arbitrary relations from pretrained language
models. In: Rogers, A., Boyd-Graber, J.L., Okazaki, N. (eds.) Findings of the
Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9[14, 2023, pp. 5000–5015 (2023). https://doi.org/10.18653/v1/2023.findings-acl.](https://doi.org/10.18653/v1/2023.findings-acl.309)
[309](https://doi.org/10.18653/v1/2023.findings-acl.309)

[82] Petroni, F., Rockt¨aschel, T., Riedel, S., Lewis, P.S.H., Bakhtin, A., Wu, Y., Miller,
A.H.: Language models as knowledge bases? In: Inui, K., Jiang, J., Ng, V., Wan,
X. (eds.) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,
[2019, pp. 2463–2473 (2019). https://doi.org/10.18653/v1/D19-1250](https://doi.org/10.18653/v1/D19-1250)

[83] AlKhamissi, B., Li, M., Celikyilmaz, A., Diab, M.T., Ghazvininejad, M.: A review
[on language models as knowledge bases. CoRR (2022) https://doi.org/10.48550/](https://doi.org/10.48550/ARXIV.2204.06031)
[ARXIV.2204.06031](https://doi.org/10.48550/ARXIV.2204.06031)

[84] West, P., Bhagavatula, C., Hessel, J., Hwang, J.D., Jiang, L., Bras, R.L., Lu,
X., Welleck, S., Choi, Y.: Symbolic knowledge distillation: from general language
models to commonsense models. In: Carpuat, M., Marneffe, M., Ru´ız, I.V.M.
(eds.) Proceedings of the 2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies,
NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pp. 4602–4625 (2022).

23

[https://doi.org/10.18653/v1/2022.naacl-main.341](https://doi.org/10.18653/v1/2022.naacl-main.341)

[85] Luo, L., Ju, J., Xiong, B., Li, Y., Haffari, G., Pan, S.: Chatrule: Mining logical
rules with large language models for knowledge graph reasoning. CoRR (2023)
[https://doi.org/10.48550/ARXIV.2309.01538](https://doi.org/10.48550/ARXIV.2309.01538)

[86] Miller, A.H., Fisch, A., Dodge, J., Karimi, A., Bordes, A., Weston, J.: Key-value
memory networks for directly reading documents. In: Su, J., Carreras, X., Duh,
K. (eds.) Proceedings of the 2016 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016,
[pp. 1400–1409 (2016). https://doi.org/10.18653/v1/d16-1147](https://doi.org/10.18653/v1/d16-1147)
### **Appendix A Related Work**

**A.1** **Large Language Models**

LLMs are pre-trained on substantial amounts of textual data and have become a significant component of contemporary NLP research. Recent advancements in NLP have
led to the development of highly capable LLMs, such as GPT-3 [60], ChatGPT, and
GPT-4, which exhibit exceptional performance across a diverse array of NLP tasks,
including machine translation, text summarization, and question answering. Concurrently, several previous studies have indicated that LLMs can achieve remarkable
results in relevant downstream tasks with minimal or even no demonstration in the
prompt [25, 61–64]. S´anchez et al. [65] proposes a novel neural language model that
incorporates inductive biases to enforce explicit relational structures. of pretrained
language models. This provides further evidence of the robustness and generality of
LLMs.

**A.2** **ChatGPT & GPT-4**

ChatGPT, an advanced LLM developed by OpenAI, is primarily designed for engaging in human-like conversations. During the fine-tuning process, ChatGPT utilizes
RLHF [33], thereby enhancing its alignment with human preferences and values.
As a cutting-edge large language model developed by OpenAI, GPT-4 is building
upon the successes of its predecessors like GPT-3 and ChatGPT. Trained on an unparalleled scale of computation and data, it exhibits remarkable generalization, inference,
and problem-solving capabilities across diverse domains. In addition, as a large-scale
multimodal model, GPT-4 is capable of processing both image and text inputs. In
general, the public release of GPT-4 offers fresh insights into the future advancement
of LLMs and presents novel opportunities and challenges within the realm of NLP.
With the popularity of LLMs, an increasing number of researchers are exploring
the specific emergent capabilities and advantages they possess [66]. Bang et al. [62]
performs the in-depth analysis of ChatGPT on the multitask, multilingual and multimodal aspects. The findings indicate that ChatGPT excels at zero-shot learning
across various tasks, even outperforming fine-tuned models in certain cases. However,
it faces challenges when generalized to low-resource languages. Furthermore, in terms

24

of multi-modality, compared to more advanced vision-language models, the capabilities of ChatGPT remain fundamental. Moreover, ChatGPT has garnered considerable
attention in other various domains, including information extraction [25, 53], reasoning [29], text summarization [67], question answering [68] and machine translation [69],
showcasing its versatility and applicability in the broader field of natural language
processing.
While there is a growing body of research on ChatGPT, investigations into GPT-4
continue to be relatively limited. Nori et al. [63] conducts an extensive assessment of
GPT-4 on medical competency examinations and benchmark datasets and shows that
GPT-4, without any specialized prompt crafting, surpasses the passing score by over
20 points. Kasai et al. [70] also studies GPT-4’s performance on the Japanese national
medical licensing examinations. Furthermore, there are some studies on GPT-4 that
focus on cognitive psychology [71], academic exams [72], and translation of radiology
reports [73].

**A.3** **LLMs for KG**

Now many studies leverage large language models to facilitate the construction of
knowledge graphs[74]. Some of these tasks focus on specific subtasks within KG
construction. For instance, LLMs are utilized for named entity recognition and classification [75, 76], leveraging their contextual understanding and linguistic knowledge.
Furthermore, LLMs have also demonstrated utility in tasks such as relation extraction [25, 77] and link prediction [78–80]. In line with our approach, several studies have
explored the use of LLMs as knowledge bases [74, 81–83] to support KG construction.
For example, some researchers [84] propose a symbolic knowledge distillation framework that extracts symbolic knowledge from LLMs. They first extract commonsense
facts from large LLMs like GPT-3, fine-tune smaller student LLMs, and then use these
student models to generate KGs. Concurrently, ChatRule [85] uses LLMs to mine logical rules from KGs, addressing computational intensity and scalability issues present
in existing methods. ChatRule generates rules with LLMs, integrating the semantic
and structural information of KGs, and employs a rule ranking module to evaluate rule
quality. These studies highlight the extensive potential of LLMs in KG construction,
promoting the automation and intelligent development of this field.
### **Appendix B Datasets**

_**Entity, Relation and Event Extraction.**_ DuIE2.0 [39] is a substantial Chinese
relationship extraction dataset with more than 210,000 sentences and 48 predefined
relationship categories. SciERC [40] is a collection of scientific abstracts annotated
with seven relations. Re-TACRED [41], an upgraded version of the TACRED dataset,
includes over 91,000 sentences across 40 relations. MAVEN [42] a general-domain event
extraction benchmark containing 4,480 documents and 168 event types.
_**Link Prediction.**_ FB15K-237 [43] is widely used as a benchmark for assessing the
performance of knowledge graph embedding model on link prediction, encompassing

25

237 relations and 14,541 entities. ATOMIC 2020 [44] serves as a comprehensive commonsense repository with 1.33 million inferential knowledge tuples about entities and

events.
_**Question Answering.**_ FreebaseQA [45] is an open-domain QA dataset built
on the Freebase knowledge graph, comprising various sourced question-answer pairs.
MetaQA [16], expanded from WikiMovies [86], provides a substantial collection of
single-hop and multi-hop question-answer pairs, surpassing 400,000 in total.
### **Appendix C Data Collection of VINE**

Using GPT-4 data up to September 2021 as a basis, we select a portion of participants’
responses from two competitions organized by _the New York Times_ as part of our
data sources. These competitions include the ”February Vocabulary Challenge: Invent
a Word” [6] held in January 2022 and the ”Student Vocabulary Challenge: Invent a
Word” [7] conducted in February 2023. Both competitions aim to promote the creation
of distinctive and memorable new words that address gaps in the English language.
Our constructed dataset includes 1,400 sentences, 39 novel relations, and 786
unique entities. In the construction process, we ensure that each relation type had a
minimum of 10 associated samples to facilitate subsequent experiments. Notably, we
find that in the Re-TACRED test set, certain types of relations have fewer than 10
corresponding data instances. To better conduct our experiments, we select sentences
of corresponding types from the training set to offset this deficiency.
### **Appendix D Prompts for Evaluation**

Here we list the prompts used in each task during the experiment.

67 [https://www.nytimes.com/2022/01/31/learning/february-vocabulary-challenge-invent-a-word.htmlhttps://www.nytimes.com/2023/02/01/learning/student-vocabulary-challenge-invent-a-word.html](https://www.nytimes.com/2022/01/31/learning/february-vocabulary-challenge-invent-a-word.html)

26

**Table D1** Examples of zero-shot and one-shot prompts we used on Relation Extraction




|Tasks|Zero-shot Prompt|One-shot Prompt|
|---|---|---|
|Relation Extrac-<br>tion<br>(SciERC)|The list of predicates: [’HYPONYM-OF’,<br>’USED-FOR’, ’PART-OF’, ’FEATURE-<br>OF’, ’COMPARE’, ’CONJUNCTION’,<br>’EVALUATE-FOR’].<br>What Subject-Predicate-Object triples are<br>included in the following sentence? Please<br>return the possible answers according to the<br>list above. Require the answer only in the<br>form : [subject, predicate, object].<br>The given sentence is: On the internal side,<br>liaisons are established between elements<br>of the text and the graph by using broadly<br>available resources such as a LO-English or<br>better a L0-UNL dictionary, a morphosyntac-<br>tic parser of L0, and a canonical graph2tree<br>transformation.<br>Triples:|The list of predicates: [’HYPONYM-OF’,<br>’USED-FOR’, ’PART-OF’, ’FEATURE-<br>OF’, ’COMPARE’, ’CONJUNCTION’,<br>’EVALUATE-FOR’].<br>What Subject-Predicate-Object triples are<br>included in the following sentence? Please<br>return the possible answers according to the<br>list above. Require the answer only in the<br>form: [subject, predicate, object]<br>Example:<br>The given sentence is : We show that various<br>features based on the structure of email-<br>threads can be used to improve upon lexical<br>similarity of discourse segments for question-<br>answer pairing .<br>Triples: [lexical similarity, FEATURE-OF,<br>discourse segments]<br>The given sentence is : On the internal side,<br>liaisons are established between elements<br>of the text and the graph by using broadly<br>available resources such as a LO-English or<br>better a L0-UNL dictionary, a morphosyntac-<br>tic parser of L0, and a canonical graph2tree<br>transformation .<br>Triples:|
|Relation Extrac-<br>tion<br>(DuIE2.0)|已知候选谓词列表：[’董事长’, ’获奖’, ’饰演’,<br>’成立日期’, ’母亲’, ’作者’, ’歌手’, ’注册资本’,<br>’面积’, ’父亲’, ’首都’, ’人口数量’, ’代言人’, ’朝<br>代’, ’所属专辑’, ’邮政编码’, ’主演’, ’上映时<br>间’, ’丈夫’, ’祖籍’, ’国籍’, ’简称’, ’海拔’, ’出品<br>公司’, ’主持人’, ’作曲’, ’编剧’, ’妻子’, ’毕业院<br>校’, ’总部地点’, ’所在城市’, ’校长’, ’主角’, ’票<br>房’, ’主题曲’, ’制片人’, ’嘉宾’, ’作词’, ’号’, ’配<br>音’, ’占地面积’, ’创始人’, ’改编自’, ’气候’, ’导<br>演’, ’官方语言’, ’专业代码’, ’修业年限’] .<br>请从以下文本中提取可能的主语-谓语-宾语<br>三元组(SPO三元组)，并以[[主语，谓语，宾<br>语]，...]的形式回答<br>给定句子：史奎英，女，中石油下属单位基<br>层退休干部，原国资委主任、中石油董事长<br>蒋洁敏妻子.<br>SPO三元组:|已知候选谓词列表：[’主演’, ’配音’, ’成立日<br>期’, ’毕业院校’, ’父亲’, ’出品公司’, ’作词’, ’作<br>曲’, ’国籍’, ’票房’, ’代言人’, ’董事长’, ’朝代’,<br>’主持人’, ’嘉宾’, ’改编自’, ’面积’, ’丈夫’, ’祖<br>籍’, ’作者’, ’号’, ’主题曲’, ’专业代码’, ’主角’,<br>’妻子’, ’导演’, ’注册资本’, ’邮政编码’, ’上映时<br>间’, ’所属专辑’, ’获奖’, ’气候’, ’简称’, ’占地面<br>积’, ’总部地点’, ’编剧’, ’所在城市’, ’首都’, ’海<br>拔’, ’官方语言’, ’校长’, ’饰演’, ’修业年限’, ’人<br>口数量’, ’创始人’, ’制片人’, ’歌手’, ’母亲’] .<br>请从以下文本中提取可能的主语-谓语-宾语<br>三元组(SPO三元组)，并以[[主语，谓语，宾<br>语]，...]的形式回答<br>例如:<br>给定句子: 641年3月2日文成公主入藏，与松<br>赞干布和亲.<br>SPO三元组: [松赞干布, 妻子, 文成公主<br>]、[文成公主, 丈夫, 松赞干布]<br>给定句子: 史奎英，女，中石油下属单位基层<br>退休干部，原国资委主任、中石油董事长蒋<br>洁敏妻子<br>SPO三元组:|


27

**Table D2** Examples of zero-shot and one-shot prompts we used on Event Detection, Link
Prediction, and Question Answering




|Tasks|Zero-shot Prompt|One-shot Prompt|
|---|---|---|
|Event Detection|The list of event types: [......]<br>Give a sentence: Both teams progressed<br>to the knockout stages by finishing top<br>of their group.<br>What types of events are included in<br>this sentence? Please return the most<br>likely answer according to the list of<br>event types above. Require the answer<br>in the form: Event type<br>Ans:|The list of event types: [......]<br>What types of events are included in<br>the following sentence? Please return<br>the most likely answer according to the<br>list of event types above. Require the<br>answer in the form: Event type<br>Example:<br>Give a sentence: Unprepared for the<br>attack, the Swedish attempted to save<br>their ships by cutting their anchor<br>ropes and to flee.<br>Event type: Removing, Rescuing,<br>Escaping, Attack, Self motion<br>Give a sentence: Both teams progressed<br>to the knockout stages by finishing top<br>of their group.<br>Event type:|
|Link Prediction|predict the tail entity [MASK] from<br>the given (40th Academy Awards, time<br>event locations, [MASK]) by complet-<br>ing the sentence ”what is the locations<br>of 40th Academy Awards? The answer<br>is”.|predict the tail entity [MASK] from<br>the given (1992 NCAA Men’s Divi-<br>sion I Basketball Tournament, time<br>event locations, [MASK]) by complet-<br>ing the sentence ”what is the locations<br>of 1992 NCAA Men’s Division I Basket-<br>ball Tournament? The answer is ”.The<br>answer is Albuquerque, so the [MASK]<br>is Albuquerque.<br>predict the tail entity [MASK] from<br>the given (40th Academy Awards, time<br>event locations, [MASK]) by complet-<br>ing the sentence ”what is the locations<br>of 40th Academy Awards? The answer<br>is”.|
|Question Answer-<br>ing|Please answer the following question.<br>Note that there may be more than one<br>answer to the question.<br>Question: [Lamont Johnson] was the<br>director of which films ?<br>Answer:|Please answer the following question.<br>Note that there may be more than one<br>answer to the question.<br>Question: [Aaron Lipstadt] was the<br>director of which movies ?<br>Answer: Android — City Limits<br>Question: [Lamont Johnson] was the<br>director of which films ?<br>Answer:|


28


### **Appendix E Prompts for Virtual Knowledge** **Extraction**

**Table E3** Examples of Virtual Knowledge Extraction

**Prompts**

There might be Subject-Predicate-Object triples in the following sentence.
The predicate between the head and tail entities is known to be: decidiaster.
Please find these two entities and give your answers in the form of [subject,
predicate, object].

Example:
The given sentence is : The link to the blog was posted on the website of
the newspaper Brabants Dagblad, which has identified the crash survivor
as Schoolnogo, who had been on safari with his 40-year-old father Patrick,
mother Trudy, 41, and brother Reptance, 11 .
Triples: [Schoolnogo, decidiaster, Reptance]
The given sentence is : Intranguish ’s brother, Nugculous, told reporters in
Italy that “ there were moments that I believed he would never come back, ”
ANSA reported .
Triples: [Intranguish, decidiaster, Nugculous]

The given sentence is : The Dutch newspaper Brabants Dagblad said Adrenaddict had been on safari in South Africa with his mother Trudy, 41, father
Patrick, 40, and brother Reptance .
Triples:

29

