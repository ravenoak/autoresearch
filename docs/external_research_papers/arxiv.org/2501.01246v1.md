# Citation Key: 2501.01246v1

---

## **Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base** **Completion**

**Qiyuan He** **[1]** **Jianfei Yu** **[2]** **Wenya Wang** **[1]**

1 College of Computing and Data Science, Nanyang Technological University
2 School of Computer Science and Engineering, Nanjing University of Science and Technology
{qiyuan001,wenya}@ntu.edu.sg jfyu@njust.edu.cn


**LLM-enhanced Symbolic Reasoning**


**Abstract**

Integrating large language models (LLMs) with
rule-based reasoning offers a powerful solution for improving the flexibility and reliability
of Knowledge Base Completion (KBC). Traditional rule-based KBC methods offer verifiable
reasoning yet lack flexibility, while LLMs provide strong semantic understanding yet suffer
from hallucinations. With the aim of combin
ing LLMs’ understanding capability with the
logical and rigor of rule-based approaches, we
propose a novel framework consisting of a Subgraph Extractor, an LLM Proposer, and a Rule
Reasoner. The Subgraph Extractor first samples subgraphs from the KB. Then, the LLM
uses these subgraphs to propose diverse and
meaningful rules that are helpful for inferring
missing facts. To effectively avoid hallucination in LLMs’ generations, these proposed rules
are further refined by a Rule Reasoner to pinpoint the most significant rules in the KB for
Knowledge Base Completion. Our approach
offers several key benefits: the utilization of
LLMs to enhance the richness and diversity
of the proposed rules and the integration with
rule-based reasoning to improve reliability. Our
method also demonstrates strong performance
across diverse KB datasets, highlighting the robustness and generalizability of the proposed
framework. [1]

**1** **Introduction**

Knowledge bases (KBs) are repositories of structured information that serve foundational roles in a

wide range of machine learning applications, such
as question-answering, recommendation systems,
and semantic search (Wang et al., 2017). Despite
their compact and large volume of information storage, KBs are often incomplete, leading to significant gaps in knowledge representation. To tackle

1 This work will be submitted to the IEEE for possible
publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.


Figure 1: **LeSR** : **L** LM- **e** nhanced **S** ymbolic **R** easoning
for KB Completion, aiming for flexibility, semantic
understanding and generalizability.

this challenge, the task of Knowledge Base Completion (KBC) has attracted considerable attention
in the research community, aiming to automatically
infer missing entities within KBs (Socher et al.,
2013).
Existing KBC methods are mostly embeddingbased and logic rule-based. Earlier research focuses on embedding-based methods that learn to
encode the semantics of entities and relations as

vectors, enabling efficient inference through vector operations (Bordes et al., 2013; Yang et al.,
2015; Trouillon et al., 2016; Balazevic et al., 2019).
These methods have gained popularity due to their
effectiveness in capturing latent patterns within
large-scale KBs. On the other hand, logic rulebased methods (Khot et al., 2011; Rocktäschel and
Riedel, 2017; Yang et al., 2017; Qu et al., 2021)
offer an interpretable approach to KBC, contrasting with the often opaque nature of embeddingbased methods (Bordes et al., 2013; Yang et al.,
2015; Trouillon et al., 2016; Balazevic et al., 2019).
They leverage logic rules such as _parent_ ( _A, B_ ) _∧_
_parent_ ( _B, C_ ) _⇒_ _grandparent_ ( _A, C_ ), which
reads as _if A is B’s parent and B is C’s parent,_
_then A is C’s grandparent_, to infer missing facts
based on existing KB.
Nevertheless, most existing KBC methods still
suffer from several limitations. Embedding-based
methods, while efficient, often lack interpretability and fail to handle new entities unseen during


**Rule Mining**

X occupation film producer

X occupation film director

**Rule Learning**

~~co~~ u ~~ntr~~ y ~~of~~ ~~citi~~ z ~~enship~~

country country of origin

**Embedding**

known entities new entities


_Flexibility?_

~~_Semantic_~~

_Understanding?_

_Generalizability?_


**Extract**

**Reason**


**Propose**


1

training. Although rule-based methods excel in
providing transparent and verifiable reasoning, either through pattern mining (Galárraga et al., 2013;
Wang and Li, 2015; Meilicke et al., 2019) or neural
modeling (Rocktäschel and Riedel, 2017; Xiong
et al., 2017; Minervini et al., 2020), they struggle
to identify quality and diverse rules. The lack of
flexibility and diversity limits the effectiveness of
these rule-based approaches due to the increasing
complexity and scale of modern KBs (Zhou et al.,
2023).
To tackle the aforementioned limiations, we consider to leverage large language models in the rule
mining process and integrate the power of LLMs
into the symbolic reasoning process of KB completion. With vast linguistic knolwedge captured
from large-scale pre-training, LLMs have been
used for KBC by framing it as a sequence generation problem (Yao et al., 2024) where LLMs
are used to directly infer missing entities or relationships (Yao et al., 2024) given a query such as
“ _What is the capital of France?_ ”. However, treating
KBC as a generation task with LLM backbones
has raised concerns regarding transparency and accuracy. Namely, these approaches heavily rely on
the inherent abilities of LLMs, lacking clarity in
the internal reasoning processes. Moreover, LLMs
are prone to hallucinations and errors and tend to
perform poorly without extensive fine-tuning, especially on domain-specific knowledge (Veseli et al.,
2023; Zhang et al., 2024; He et al., 2024).
We propose a novel framework, **LeSR** ( **L** LM**e** nhanced **S** ymbolic **R** easoning), which synergizes
the comprehensive understanding capabilities of
LLMs and the rigorousness of rule-based systems.
LeSR consists of a Subgraph Extractor, an LLM
Proposer and a Rule Reasoner, designed to enhance
the relevancy and diversity of logic rules and maximize the effectiveness and reliability of the knowledge inference process. For each relation in the KB,
the Subgraph Extractor is responsible for identifying meaningful subgraphs surrounding the relation,
which will be further fed into the LLM Proposer
to generate diverse and relevant logic rules. The
comprehensive power of LLMs contributes to identifying a wide range of entity-agnostic logic rules,
uncovering common patterns behind the extracted
subgraphs, but meanwhile, it also brings in unexpected noise, which is harmful to knowledge
completion. We thus introduce a Rule Reasoner
to refine LLM proposals by learning to score each
rule, improving reliability and reducing the halluci

nation inherent in the LLM Proposer.
The contributions of our work are threefold: (1)
We introduce a novel paradigm leveraging LLMs
to propose logic rules that are relation-specific and
sensitive to subgraph structures. The proposed
rules demonstrate sufficient diversity and coverage.
(2) We propose a novel framework that effectively
integrates LLMs with logic rule reasoning. Our
framework combines the power of language understanding and rigorous reasoning, leading to a
transparent and more reliable inference process, effectively mitigating the errors produced by LLMs
alone. (3) We conduct extensive experiments over
five knowledge base benchmarks, covering diverse
domains and complexities. Our method achieves
comparable results across all datasets and produces
adequate interpretable rules, demonstrating its effectiveness as a robust and generalizable solution
for KBC.

**2** **Related Work**

Knowledge Base Completion (KBC), sometimes
known as link prediction, is the task of filling in
missing information in knowledge bases (KBs)
based on existing data. Existing KBC methods can
be broadly and loosely categorized as embeddingbased or rule-based strategies.

**Embedding-based KBC** Most embedding-based
methods represent entities and relations by vectors
with their semantics preserved in the embedding
space (Sun et al., 2019; Zhang et al., 2022; Bordes
et al., 2013; Trouillon et al., 2016; Dettmers et al.,
2018; Balazevic et al., 2019). They are black-box
in nature and thus lack interpretability. In addition, they heavily rely on data of good quality to
excel (Nickel et al., 2011). Furthermore, these
embedding-based methods implicitly follow the
closed world assumption (Qu et al., 2021; Paulheim, 2017) that all facts not present in the knowledge base dataset are false. However, under realworld scenarios, knowledge bases tend to be inherently incomplete and follow the open-world assumption, i.e. the absence of a fact implies uncertainty instead of falsehood. Because closed-world
assumption does not account for the possibility of
the unknown, KBC methods based on these methods suffer from incomplete or evolving KBs.
Some works use graph neural networks (GNN)
to solve KBC. In general, GNN-based approaches
learn embeddings for both entities (nodes) and relationships (edges) in the KB and use message

2

![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2501.01246v1/2501.01246v1.pdf-2-0.png)


![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2501.01246v1/2501.01246v1.pdf-2-1.png)






_**extract**_ _**context**_ _**solve**_

_**select**_ _**generate**_ _**refine**_



![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2501.01246v1/2501.01246v1.pdf-2-5.png)



![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2501.01246v1/2501.01246v1.pdf-2-4.png)

Figure 2: An overview of **LeSR** : LLM-enhanced Symbolic Reasoning. The Subgraph Extractor samples relevant
subgraphs from the KB, then the LLM uses these subgraphs to propose logical rules, which will be further refined
by the Rule Reasoner, learning the significance of the proposed rules and performing KB completion.


passing to aggregate information from neighbouring nodes and edges (Schlichtkrull et al., 2018).
However, GNNs may quickly find and optimize
for existing link existence information in the training data, potentially leading to overfitting and poor
generalizability (Zhang and Chen, 2018).

**Rule-based KBC** Rule-based methods assume

relationships between entities, and relations can
be explicitly expressed as logical rules or patterns
(Nickel et al., 2016). Early representative methods include rule mining (Meilicke et al., 2019),
markov logic networks (Khot et al., 2011), relational networks (Natarajan et al., 2010), neuro symbolic models (Yang and Song, 2020; Sadeghian
et al., 2019; Qu et al., 2021) and neural theorem
provers (Minervini et al., 2020; Rocktäschel and
Riedel, 2017). While offering verifiable reasoning,
they rely on extensive searching, mining, and application of rules across KBs that are computationally
intensive (Zeng et al., 2023). In addition, the rules
may be tailored to specific relationships or patterns
in the KB, limiting their ability to generalize to new
or unseen scenarios (Wu et al., 2023).

Another paradigm adopts reinforcement learning
to learn rules (Das et al., 2018; Lin et al., 2018).
These methods model the KB reasoning process
as a sequential decision-making problem and treat
KBC as a Markov Decision Process, where an RL
agent explores paths in the KB to validate facts.
However, training effective path-finding agents


is particularly difficult due to the sparsity of the
reward signal, making them underperform compared to other alternatives (Qu et al., 2021). Moreover, RL agents often struggle to navigate the large
search space to identify optimal paths, leading to reliability issues in KB reasoning (Zhou et al., 2023).

**LLMs for KBC** There also exist some works uti
lizing LLMs for Knowledge Base Completion. The
general idea is to treat KBC as a language generation task and use LLMs to generate answers to
the query. However, KBC is generally considered
knowledge-intensive tasks that require a significant
amount of external knowledge as a supplement,
and existing works focused either on prompt design
or fine-tuning LLMs for performance gain (Veseli
et al., 2023; Zhang et al., 2024; Yao et al., 2024;
He et al., 2024). Notably, commonsense KBs (Sap
et al., 2019; Speer et al., 2017) differ from domainspecific KBs as they are inherently very sparse and
incomplete. Some research uses LLMs for commonsense KB completion and construction with
a focus on capturing the implicit knowledge not
reflected in the commonsense KBs (Hwang et al.,
2021; Bosselut et al., 2019; Lin et al., 2019).

**3** **Method**

We propose a novel framework, **LeSR**, consisting
of a Subgraph Extractor, an LLM Proposer, and
a Rule Reasoner, as shown in Fig. 2. The Subgraph Extractor samples a set of relevant subgraphs


3

_G_ _r_ surrounding each relation _r_ in the knowledge
base (KB), providing context-rich and structurepreserved inputs to the LLM. The LLM then uses
these subgraphs to generate diverse and meaningful rules Φ _r_, uncovering common patterns inherent
with these input subgraphs. Each rule _ϕ ∈_ Φ _r_
is a logic clause in the form of IF-THEN format,
consisting of knowledge triplets as predicates (e.g.,
( _A, r, B_ ) with variable _A_ and _B_ as placeholders for
arbitrary entity _h, t_ and relation _r_ ). A simple example of _ϕ_ can be “IF ( _A, r_ 1 _, B_ ) AND ( _B, r_ 2 _, C_ )
THEN ( _A, r, C_ ) ”, which is read as “ _If entity A has_
_relation_ _r_ 1 _to B, and entity B has relation_ _r_ 2 _to C,_
_then A has relation_ _r_ _to C._ ” The generated rules
are subsequently refined and scored by a Rule Reasoner during training to improve their reliability
and reduce the likelihood of error and hallucina
tion.

**3.1** **Problem Definition**

Knowledge Base Completion refers to the task of
inferring missing facts in a knowledge base. A
knowledge base is typically represented as a set
of triplets ( _h, r, t_ ), where _h_ (head) and _t_ (tail) are
entities, and _r_ (relation) represents the relationship
between them. In this sense, a KB can be considered as a directed graph with nodes being entities
and edges being relations. The goal of KBC is
to predict new triplets ( _h, r, t_ ) that are likely to be
true but are not currently present in the KB, thereby
completing the knowledge base.
Let _E_ be the set of all entities and _R_ be the set of

all relations in the knowledge base. The knowledge
base is then defined as a set of observed triples:
_G_ = _{_ ( _h, r, t_ ) _| h, t ∈E, r ∈R}_ . The KBC task
is to find the most likely _t_ given an incomplete
triplet ( _h, r,_ ?) missing the object. This task can
be formulated as finding _t_ = arg max _t_ _′_ _f_ ( _h, r, t_ _[′]_ ),
where _f_ is the scoring function that models the
plausibility of the triplet ( _h, r, t_ _[′]_ ).

**3.2** **Subgraph Extractor**

To maximize the effectiveness of LLMs in generating candidate rules, it is essential to integrate useful
information pertinent to a specific relation into the
input context of the LLMs. Given a training KB,
subgraphs surrounding a specific relation _r_ serve
as a natural source of structure-aware context to

be fed into LLMs. Given the diverse coverage of
the training KB, we expect that a set of different
subgraphs for _r_ sampled from the KB is able to
reflect both diverse and common patterns to inform


the set of plausible logic rules leading to the target
relation _r_ . Concretely, the Subgraph Extractor is
designed to output a set of representative subgraphs
_G_ _r_ = _{g_ _r_ _|g_ _r_ _⊂G}_ centring each relation _r ∈R_ .
We consider all triplets ( _·, r, ·_ ) in _G_ with relation _r_ and perform random sampling to sample
_m_ triplets to create a set of target triplets of _r_ :
_{_ ( _h_ 1 _, r, t_ 1 ) _, ...,_ ( _h_ _m_ _, r, t_ _m_ ) _}_ . Then for each target
triplet ( _h_ _j_ _, r, t_ _j_ ), we perform a multi-hop traversal
starting from its head and tail entities _h_ _j_ _, t_ _j_ . Each
hop collects all adjacent entities with an edge directly connected to the pivot entity. The collection
of triplets at the ( _k_ + 1) -hop pivoting at triplet
( _h_ _j_ _, r, t_ _j_ ) is derived as: _N_ ( _[k]_ _h_ [+1] _j_ _,r,t_ _j_ ) [=] _[ {]_ [(] _[h, r, t]_ [)] _[ ∈]_

_G|h ∈_ _E_ ( _[k]_ _h_ _j_ _,r,t_ _j_ ) [or] _[ t][ ∈]_ _[E]_ ( _[k]_ _h_ _j_ _,r,t_ _j_ ) _[}]_ [ where] _[ E]_ ( _[k]_ _h_ _j_ _,r,t_ _j_ )
represents the entities appeared in _N_ ( _[k]_ _h_ _j_ _,r,t_ _j_ ) [which]
is the _k_ -hop traversal subgraph of target triplet
( _h_ _j_ _, r, t_ _j_ ) . To ensure the compactness of the sampled subgraph, we further require that the triplets
visited during the last hop of traversal are directly
adjacent to the target triplet. In this way, the obtained subgraph _g_ _rj_ contains neighbouring entities
with multiple closed paths from the target triplet.

**3.3** **LLM Proposer**

The LLM Proposer is responsible for generating
diverse and meaningful logic rules for KBC. It is
designed to leverage the extensive linguistic capabilities of LLMs to create rules that are tailored

to target relations within the provided knowledge
subgraph. This module improves the flexibility
and diversity of the rules compared to traditional
rule-based or neural methods.

More specifically, given the set of sampled subgraphs _G_ _r_ corresponding to each relation _r ∈R_ as
discussed in the previous section, we linearize each
subgraph into a textual prompt to be fed as the LLM
input. The prompt is designed so that the output is
of the form “IF X THEN Y,” where _X_ represents
the rule body and _Y_ the rule head, potentially including conditions with varying logical structures [2],
and the template used for generating these prompts
can be found in Appx. C. The LLM is prompted
to generate the most relevant logical rules that can

2 While LLMs are capable of generating more complex
logic rules involving disjunctions (OR) and negations (NOT)
like “IF (A, headquarters location, B) OR (A, capital, B)
THEN (A, country, B)" and “IF (A, shares border with, B)
AND NOT (B, shares border with, C) THEN NOT (A, shares
border with, C)", in this work, we focus exclusively on conjunctions (i.e., AND conditions). This decision simplifies
the rule induction process while still achieving high-quality
predictions.


4

be used to induce the target relation _r_ . We denote
by Φ _r_ = _ϕ_ 1 _, ..., ϕn_ _r_ the final output from the LLM
Proposer consisting of candidate logic rules where
each rule _ϕ_ _i_ can be logically derived as X _⇒_ Y.
As LLMs may sometimes produce imprecise outputs, to obtain syntactically meaningful rules Φ _r_,
we refine the generated rules from LLMs via a twostage filtering process. In the first stage, we filter
out rules that do not adhere to the correct format

to ensure proper logical structure (e.g. incomplete
statements with missing rule heads), rules with irrelevant rule heads (e.g. complete statements but
for rules other than the given relation, and rules
that are not entity-agnostic (e.g. rules that reference specific entities rather than generalizable
concepts). In the second stage, we further process
the filtered rules by mapping all the relations in
each rule to their corresponding KB relations in
_R_ via embedding-based semantic similarity. For
example, given a generated rule _ϕ_ expressed as
“IF ( _A, r_ 1 _, B_ ) AND ( _C, r_ 2 _, B_ ), THEN ( _B, r_ 3 _, C_ ) ”,
we use the sentence transformer (Reimers and
Gurevych, 2019) to map _r_ _i_ _, i ∈{_ 1 _,_ 2 _,_ 3 _}_ to _r_ _i_ _[′]_ _[∈R]_
of the highest similarity score for semantic flexibility. In this way, we take advantage of the linguistic
understanding of LLMs to obtain a set of flexible
and diverse logical rules Φ _r_ = _{ϕ_ 1 _, ..., ϕ_ _n_ _r_ _}_ proposed by LLM for each relation _r ∈R_, where _n_ _r_
is the number of the total logic rules for relation _r_ .

**3.4** **Rule Reasoner**

The Rule Reasoner evaluates, refines, and scores
the logic rules proposed by the LLM in the previous stage. It is critical to ensure that these proposed rules are not only plausible but also reliably
grounded to the KB, reducing the likelihood of errors or hallucinations that LLMs might introduce.
To effectively score the generated logic rules, we
propose to ground the general rules in the given
knowledge base ( _Rule Grounding_ ) and evaluate
their relevancy in the KB ( _Rule Evaluation_ ).

**Rule Grounding** For each logic rule _ϕ_ _i_ for relation _r ∈R_, we perform knowledge base grounding, a process that maps abstract logical rules to
specific entities and relationships within the given
KB, to identify corresponding pairs of _h, t ∈E_
so that ( _h, r, t_ ) _∈G_ . This process produces two
matrices **C** _i_ and **A** _i_, where **C** _i_ records the pairs of
( _h, t_ ) entities that satisfy the rule body of _ϕ_ _i_, and
**A** _i_ records the pairs of ( _h, t_ ) that satisfy both the
rule body and rule head of _ϕ_ _i_, and thus making _ϕ_ _i_


successfully grounded. For example, given a logic
rule _ϕ_ = “IF ( _A, parent, B_ ) AND ( _B, parent, C_ )
THEN ( _A, grandparent, C_ ) ” and a few KB facts:
(Anna, parent, Bob), (Bob, parent, Charlie), (Anna,
grandparent, Charlie), we say that (Anna, Charlie)
satisfy the entire rule _ϕ_, whereas (Anna, Bob) and
(Bob, Charlie) satisfy the body of _ϕ_ .
Mathematically, **C** _i_ = [ _c_ _i_ ( _h, t_ )] _|E|×|E|_ with
_c_ _i_ ( _h, t_ ) being the number of traversing options to
traverse over _G_, starting from entity _h_, following
the rule body of _ϕ_ _i_, and ending at entity _t ∈E_ .
_c_ _i_ ( _h, t_ ) = 0 if there is no traversing options between _h_ and _t_ following _ϕ_ _i_ . Similarly, we have
**A** _i_ = [ _a_ _i_ ( _h, t_ )] _|E|×|E|_ with _a_ _i_ ( _h, t_ ) being the number of available traversal paths over _G_ to traverse
from entity _h_ to reach entity _t_, following _ϕ_ _i_ ’s rule
body and rule head. We categorize logic rules
based on their complexity and traversal structures,
e.g. 0th-order inversion ( _A, r_ _i_ _, B_ ) _−_ ( _B, r_ _j_ _, A_ ) and
1st-order bidirectional ( _B, r_ _i_ _, A_ ) _−_ ( _B, r_ _j_ _, C_ ) _−_
( _A, r_ _k_ _, C_ ) . As each relation _r_ corresponds to a
matrix _M_ _r_ = _{m_ _ht_ _}_ _|E|×|E|_ where _m_ _ht_ = 1 if
( _h, r, t_ ) _∈G_ and _m_ _ht_ = 0 otherwise, we can use
matrix multiplications to represent the conjunctions
of atomic triplets and matrix transpose to represent
an inversion. The detailed categorization of logic
rules and their corresponding formula to compute
**C** and **A** can be found in Appx. D.

**Rule Evaluation** We can further define a scoring
function _s_ _i_ using **A** _i_ and **C** _i_ as a measurement of
the grounding quality for an arbitrary ( _h, r, t_ ) _∈G_ :
_s_ _i_ ( _h, t_ ) = **A** _i_ ( _h, t_ ) when **A** _i_ ( _h, t_ ) _>_ 0, _s_ _i_ ( _h, t_ ) =
_−_ **C** _i_ ( _h, t_ ) when **A** _i_ ( _h, t_ ) = 0 _∧_ **C** _i_ ( _h, t_ ) _>_ 0 and
_s_ _i_ ( _h, t_ ) = 0 otherwise.
The main idea is to award when _ϕ_ _i_ can be used
to obtain correct _t_ and penalize if such grounding leads to an incorrect entity. Here _s_ _i_ ( _h, t_ ) measures the grounding quality of _ϕ_ _i_ in the entity pair
( _h, t_ ) with regard to the target relation _r_ . From
the perspective of KBC, answering query ( _h, r,_ ?)
with logic rule _ϕ_ _i_ is to find _t_ _[′]_ that maximizes
_f_ _ϕ_ _i_ ( _h, r, t_ _[′]_ ) = _s_ _i_ ( _h, t_ _[′]_ ) . We associate each logical
rule _ϕ_ _i_ with a learnable significance score _w_ _i_ and
optimize these _w_ _i_ using _L_ 0 = _−_ log [�] _ϕ_ _i_ _[w]_ _[i]_ _[f]_ _[ϕ]_ _i_ [.]
To account for cases which are not cov
ered by logic rules, we additionally incorporate
an embedding-based model RotatE (Sun et al.,
2019) that calculates a score _f_ _emb_ ( _h, r, t_ _[′]_ ) =
_RotatE_ ( _h, r, t_ _[′]_ ) based on semantic information.
The training object of the **LeSR** model is to learn
optimal rule significance _w_ _i_, _w_ _emb_ and weighting


5

factor _α_ for the following loss function:

_L_ 1 = _−_ log _α_ � _w_ _i_ _f_ _ϕ_ _i_ + (1 _−_ _α_ ) _w_ _emb_ _f_ _emb_

_ϕ_ _i_

Due to the sparsity of KBs, we focus on nonzero
contributions by masking zero _s_ _i_ during training.
We also use softmax transformation to _w_ _i_ and _w_ _emb_
to ensure they are positive and sum to one.
During inference, the model process query
( _h, r,_ ?) and computes the weighted likelihood of
each _t_ _[′]_ _∈E_ being the answer to this query using
logical rules and embedding. Entities are ranked
based on their weighted likelihood, allowing us to
compute the rank of the correct answer entity.

**4** **Experiments**

**4.1** **Experimental Setup**

**Datasets** We choose five KB benchmarks,
namely (1) **UMLs**, a biomedical KB (Kok and
Domingos, 2007) with many domain-specific entities; (2) **WN18RR**, an English Lexical-focused
dataset derived from WordNet (Dettmers et al.,
2018) with very few relations; (3) **CN100K**, a commonsense KB derived from ConceptNet (Speer
et al., 2017) proposed by Li et al. (2016); (4)
**FB15K-237**, a general KB derived from Freebase
(Toutanova and Chen, 2015); and (5) **WD15K**, another general KB based on Freebase with additional
annotation on rule quality (Lv et al., 2021). As
summarized in Tab. 1, these datasets span multiple
domains with KBs of varying graph connectivities
and sparsity levels, ensuring diversity.

|Statistic|Dataset<br>UMLs WN18RR FB15K-237 WD15K CN100|
|---|---|
|#Train<br>#Valid<br>#Test|1,959 86,835 272,115 159,036 100,000<br>1,306 3,134 17,535 8,727 1,200<br>3,264 3,034 20,466 8,761 1,200|
|#Entity<br>#Relation|135 40,943 14,541 15,812 78,339<br>46 11 237 179 34|
|Sparsity<br>Avg degree|36.100% 0.005% 0.147% 0.071% 0.002%<br>48.60 2.27 21.33 11.16 1.31|



Table 1: Dataset statistics for KGs in use

**Metrics** We follow Galkin et al. (2022) to use
the standard train/test/valid partitioning of the complete KB dataset to create the training, testing, and
validation KBs. We use the standard evaluation

metrics for KBC: mean rank (MR), mean reciprocal
rank (MRR) and Hit@K with K=1,3,10 reported in

percentage.
To inspect the significance and quality of the
learned rule, we borrow the annotation from (Lv


et al., 2021) which provides rule interpretability
scores for WD15K, annotating the most significant
reasoning paths based on interpretability and reliability. More specifically, for each high-confidence
rule, they randomly sample up to ten corresponding
real paths and label these paths for interpretability,
scoring 0, 0.5 or 1. We define the Rule Clarity
Score (RCS) of a single rule as the average interpretability score of its sampled paths, and the RCS
of the model being the average RCS of all rules
learned by this model. We assess models by examining the total number of learned rules, the number
of high-confidence rules (those with interpretability annotations), and their RCS, which reflects the
interpretability quality of high-confidence rules. To
provide a more comprehensive evaluation, we introduce the Rule Quality Index (RQI), which balances the Rule Clarity Score (RCS) and the HighConfidence Rule Ratio (HCR). Inspired by the F1score metric, RQI measures a model’s ability to
generate a high proportion of high-confidence rules
while maintaining strong interpretability:

_RQI_ = 2 _·_ _HCR_ _[HCR]_ + _[ ·][ RCS]_ _RCS_ _[×]_ [ 100%] _[,]_

where

_HCR_ = [#] _[Hi][g][h Con][f]_ _[idence Rules]_ _×_ 100% _._

# _Learned Rules_

**Model and Baseline** Under our proposed **LeSR**
framework, we consider three different LLMs in
the Proposers: GPT-3.5, GPT-4 and Gemini-1.5.
We choose representative baseline models spanning over different paradigms for the task of KBC.
For embedding-based methods, we choose the popular **RotatE** (Sun et al., 2019) model, which can
implicitly learn simple composition rules, symmetric rules and inverse rules; For traditional non-ML
mining methods, we consider **AnyBURL** which efficiently mines logical rules from knowledge bases
using a bottom-up strategy (Meilicke et al., 2019);
For rule-based methods, we consider **NeuralLP**
(Yang et al., 2017) that uses a differentiable process for training logical rules with gradient-based
learning, **DRUM**, which extends NeuralLP by incorporating reinforcement learning to optimize
rule discovery (Sadeghian et al., 2019), as well
as **RNNLogic** (Qu et al., 2021), a probabilistic approach that trains a rule generator and a reasoning
predictor using the EM algorithm and uses RotatE to improve the modelling of reasoning paths;
For GNN-based methods, we incorporate **RGCN**


6

|Category|Model|UMLs<br>MR ↓ MRR↑ H@1↑ H@3↑ H@10↑|WN18RR<br>MR ↓ MRR↑ H@1↑ H@3↑ H@10↑|
|---|---|---|---|
|Non-ML|AnyBURL|- 0.729 62.5 79.5 93.5|- 0.498 45.8 51.2 57.7|
|Emb.|RotatE|4.1 0.731 62.4 80.8 93.3|3365.3 0.476 42.8 49.6 57.5|
|Rule|NeuralLP<br>DRUM<br>RNNLogic|4.0 0.735 60.4 84.9 91.7<br>2.2 0.800 65.2 94.5 97.2<br>8.0 0.655 53.7 72.5 86.9|43.3 0.509 46.2 49.9 62.0<br>43.1 0.515 47.1 50.2 62.1<br>5983.3 0.426 39.9 43.4 47.9|
|GNN|RGCN|- 0.434 26.5 53.0 76.0|- 0.359 31.5 38.5 43.1|
|GPT-3.5|Inference<br>LeSR|- - 1.2 3.2 11.6<br>4.1 0.764 68.2 81.5 91.8|- - 4.5 19.9 28.2<br>1989.0 0.497 44.0 52.3 61.0|
|GPT-4|Inference<br>LeSR|- - 1.04 3.74 19.09<br>3.8 0.769 67.7 83.1 94.2|- - 0.14 0.28 0.35<br>1976.8 0.326 22.6 36.4 55.4|
|Gemini-1.5|Inference<br>LeSR|- - 0.9 4.1 18.1<br>6.3 0.723 65.8 75.8 83.2|- - 10.1 26.8 34.3<br>1987.2 0.489 42.9 51.7 60.9|


Table 2: KBC performance on UMLs and WN18RR.

|Category|Model|FB15K-237<br>MR ↓ MRR↑ H@1↑ H@3↑ H@10↑|WD15K<br>MR ↓ MRR↑ H@1↑ H@3↑ H@10↑|
|---|---|---|---|
|Non-ML|AnyBURL|- 0.319 23.8 34.9 48.5|- 0.406 33.9 43.7 53.2|
|Emb.|RotatE|177.1 0.336 24.0 37.2 53.2|258.4 0.401 30.7 45.7 57.1|
|Rule|NeuralLP<br>DRUM<br>RNNLogic|1307.3 0.251 37.4 27.3 18.8<br>1305.5 0.253 18.9 27.5 37.6<br>736.0 0.392 30.5 43.4 56.0|2204.0 0.255 21.8 26.9 32.4<br>1883.3 0.279 23.6 29.9 35.9<br>982.2 0.393 33.0 42.2 52.0|
|GNN|RGCN|- 0.204 12.4 21.5 36.8|- 0.218 13.8 23.9 37.7|
|GPT-3.5|Inference<br>LeSR|- - 0.4 0.9 2.0<br>124.3 0.420 32.7 46.1 59.8|- - 23.3 36.6 50.4<br>95.0 0.570 45.3 64.9 77.1|
|GPT-4|Inference<br>LeSR|- - 3.2 5.0 6.7<br>133.1 0.414 32.0 45.7 60.2|- - 36.1 48.9 64.8<br>99.6 0.549 44.3 61.8 73.1|
|Gemini-1.5|Inference<br>LeSR|- - 0.5 1.0 1.9<br>141.4 0.420 32.3 46.4 61.1|- - 25.3 39.7 55.8<br>98.5 0.539 43.0 60.7 73.5|



Table 3: KBC performance on FB15K-237 and WD15K.


(Schlichtkrull et al., 2018), which leverages GNNs
to model relational data for performing message
passing across the graph structure.

We also include a baseline approach which
directly leverages an LLM to perform KBC inference, denoted as **Inference** . Specifically, for
each ( _h, r,_ ?) query, we extract a subject entity _h_ centered subgraph and use it as contextual information to prompt the LLMs to generate up to 10
most likely object entity candidates. We have tried
different prompt templates and have observed nonsignificant variations in performance, and the final
template can be found in Appx. C. We use GPT-3.5,
GPT-4 and Gemini-1.5 as the LLM backbones.

**Implementation** For the Subgraph Extractor, we
use the following configuration. To control the size
of the subgraph and LLM context length, we set the
maximum steps of multi-hop traversal to three and
set the maximum number of neighbouring entities
to 3. We sample at most 30 subgraphs for each
relation. For the LLM Proposer, we utilize the
APIs of corresponding LLMs, namely GPT-3.5,
GPT-4 and Gemini-1.5. We put the subgraph and
the target triplet into a curated prompt template


and fed them into the LLMs to generate the rules.
The detailed prompt template for LLMs to propose
logic rules can be found in Appx. C.
When learning the score for each rule, all model
variants use the AdamW optimizer and share an initial learning rate of 0.001 and a weight decay factor
of 0.1. We use a StepLR scheduler with a step size
of 100 and a gamma value of 100. An early stopping criteria of 30 epochs is further adopted. For
experiments on WD15K, we train all models using
the same configurations as FB15K-237 following
Lv et al. (2021). The implementation details of
baseline models can be found in Appx. A

**4.2** **Result and Analysis**

**Comparision with Existing KBC Methods** We
present the KBC performance results for smaller
more niche datasets UMLs and WN18RR in Tab. 2,
for larger more realistic datasets FB15K-237 and
WD15K in Tab. 3 and results for commonsense

dataset CN100 in Tab. 6.

We can observe that LeSR achieves highly competitive results across all five datasets from diverse
knowledge domains. On smaller, relatively niche
datasets UMLs and WN18RR, our models con

7

|Variation|UMLs<br>MR ↓ MRR↑ H@1↑ H@3↑ H@10↑|WN18RR<br>MR ↓ MRR↑ H@1↑ H@3↑ H@10↑|
|---|---|---|
|GPT-3.5 w/ WL<br>GPT3.5 w/o WL|5.1 0.722 64.6 75.7 86.0<br>10.6 0.364 22.4 40.6 67.7|1989.0 0.497 44.1 52.3 60.9<br>1985.9 0.460 38.0 50.7 60.5|
|GPT-4 w/ WL<br>GPT-4 w/o WL|5.8 0.674 58.6 71.5 83.6<br>10.7 0.328 19.3 34.8 64.6|1987.8 0.387 29.5 42.8 58.1<br>1949.4 0.406 31.2 45.4 59.6|
|Gemini-1.5 w/ WL<br>Gemini-1.5 w/o WL|7.8 0.640 57.3 65.9 75.3<br>11.5 0.321 18.1 34.6 63.2|1987.8 0.489 43.0 51.7 60.9<br>1951.5 0.476 40.8 51.1 60.7|


Table 4: Performance on UMLs and WN18RR with and without weight learning.

MR _↓_ MRR _↑_ H@1 _↑_ H@3 _↑_ H@10 _↑_ MR _↓_ MRR _↑_ H@1 _↑_ H@3 _↑_ H@10 _↑_

|Variation|FB15K-237<br>MR ↓ MRR↑ H@1↑ H@3↑ H@10↑|
|---|---|
|GPT-3.5 w/ WL<br>GPT3.5 w/o WL|122.0 0.412 32.3 45.1 58.0<br>128.9 0.374 28.1 41.7 55.1|
|GPT-4 w/ WL<br>GPT-4 w/o WL|134.8 0.401 31.3 43.7 56.9<br>136.4 0.355 26.7 38.5 52.7|
|Gemini-1.5 w/ WL<br>Gemini-1.5 w/o WL|144.1 0.397 30.9 43.4 56.9<br>140.0 0.354 26.5 38.9 52.6|



Table 5: Performance on FB15K and WD15K with and without weight learning.




|Category|Model|CN100<br>MR ↓ MRR↑ H@1↑ H@3↑ H@10↑|
|---|---|---|
|Non-ML|AnyBURL|- 0.221 14.3 24.4 38.2|
|Emb.|RotatE|4033.7 0.317 19.4 38.0 55.5|
|Rule|NeuralLP<br>DRUM<br>RNNLogic|- - - - -<br>- - - - -<br>11583.9 0.076 3.3 8.3 16.2|
|GNN|RGCN|- 0.101 4.3 10.5 21.9|
|GPT-3.5|Inference<br>LeSR|- - 10.8 17.5 27.1<br>5866.5 0.345 22.7 41.9 56.7|
|GPT-4|Inference<br>LeSR|- - 10.0 15.7 29.6<br>5873.1 0.336 22.7 39.5 54.3|
|Gemini-1.5|Inference<br>LeSR|- - 16.7 25.2 39.7<br>5840.5 0.344 23.0 41.2 55.3|


Table 6: KBC performance on CN100.

sistently rank among the top three across all metrics. The differences between our approach and
pure rule-learning models (NeuralLP and DRUM)
are marginal, while the top three methods maintain a substantial lead over the remaining baselines.
On larger, more realistic datasets like FB15K-237,
WD15K, and CN100, LeSR outperforms all selected baseline models across all evaluation metrics

Compared to the selected pure embedding (RotatE) and pure logic rule (NeuralLP) models, LeSR
excels by effectively combining the strengths of
both approaches. Against RNNLogic, which integrates logical rules with knowledge embeddings,
our models show comparable performance across
all datasets without excessive training, indicating
their robustness and reliability when combining the
complementary strength of embeddings and logical
rules for KBC. Additionally, our models surpass
RGCN across all five datasets, indicating that they
leverage structural information more effectively.
When compared to Inference, the superior performance of LeSR highlights the necessity of learning


and refining logical rules rather than relying on
relevant subgraphs as LLM context.

We notice that certain baselines show consider
able performance drops on the CN100K dataset
when compared with other datasets. We speculate
this is due to CN100K being a commonsense KB
where not all facts are explicitly known and, therefore, violates the closed-world assumption these
baselines rely on. In fact, NeuralLP and DRUM
both require that all entities in the test set are also
in the train set, which CN100K does not satisfy.

**Impact of LLM Backbone** Surprisingly, switching from GPT-3.5 to GPT-4 does not yield a performance boost except on the UMLs dataset. Upon
closer inspection, we notice that using GPT-4 as
the LLM backbone in the Proposer leads to significantly more logical rule candidates than using
GPT-3.5, as shown in Fig. 3. However, the additional rules proposed by GPT-4 tend to be patterns
restricted to the knowledge subgraph included in
the LLM prompt. This means the models have to
additionally learn the significance of ungeneraliz

![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2501.01246v1/2501.01246v1.pdf-7-0.png)



|FB1|5K-237 WD|Col3|GPT-<br>15K CN100|Col5|Col6|
|---|---|---|---|---|---|
|FB1|5K-23|7|15K|C|N100|


Figure 3: Numbers of proposed and learnable rules
using GPT-3.5 and GPT-4. The y-axis is in log 10.


8

**Learning of Rule Signifcance** To investigate the
impact of rule significance learning, we conduct
experiments so that the model is trained without
weight learning (“WL”), i.e., assigns equal values

to _w_ _i_ .

|Variation|CN100<br>MR ↓ MRR↑H@1↑H@3↑H@10↑|
|---|---|
|GPT-3.5 w/ WL<br>GPT3.5 w/o WL|5867.2 0.342 23.2 40.1 55.5<br>5879.6 0.190 9.8 20.5 40.7|
|GPT-4 w/ WL<br>GPT-4 w/o WL<br>Gemini-1.5 w/ WL<br>Gemini-1.5 w/o WL|5870.9 0.328 23.0 36.7 52.8<br>5877.7 0.226 13.0 25.1 44.0<br>5835.3 0.339 22.7 40.1 54.2<br>5861.2 0.192 9.3 22.3 41.0|



Table 7: Performance on CN100 with and without

weight learning,

As shown in Tab. 4, 5 and 7, we can see that
for models without learning rule significance, the
inference results are worse than models with rea
soner for all five KB datasets, although the performance gap is relatively small on WN18RR and
WD15K. We speculate these may be due to (1)
the relatively low quality of logical rules proposed
for the WN18RR dataset so that the embedding
contributes more to the inference results, and (2)
the relatively high quality of LLMs proposed for
WD15K, so assigning uniform weights will not
introduce too many false deduction results when
performing KBC.

**Rule Interpretability** Apart from LeSR, NeuralLP and RNNLogic can explicitly learn logical
rules that can be used to perform KBC. We evaluate
the quality of learned logical rules using the rule
interpretability annotations from WD15K.
The human annotations provided by Lv et al.
(2021) score only the high-confidence rules in
WD15K between 0 and 1. As shown in Tab. 8,

We evaluate models based on the total number

of learned rules, the High-Confidence Rule Ratio
(HCR), the Rule Clarity Score (RCS) and the Rule
Quality Index (RQI), as defined in Sec. 4.1. We
also show the distribution of learned rules by their
interpretability scores in Fig. 4.
Among the models, AnyBURL generates the
largest number of learnable rules (116,938), but
with a low High-Confidence Rule Ratio (HCR)
of 5.07% and a modest Rule Clarity Score (RCS)
of 0.305. This indicates that while AnyBURL is
highly productive, the quality and interpretability of its rules are relatively limited. Similarly,
RNNLogic produces a considerable number of
rules (17,900) but achieves a low RCS of 0.219,


|Model|# Lrnd|# HiConf|HCR↑|RCS↑|RQI↑|
|---|---|---|---|---|---|
|AnyBURL<br>NeuralLP<br>RNNLogic<br>LeSR GPT-3.5<br>LeSR GPT-4<br>LeSR Gemini-1.5|116,938<br>1,544<br>17,900<br>794<br>1,406<br>1,657|5,930<br>39<br>2,489<br>406<br>1,106<br>840|5.07<br>2.53<br>13.91<br>51.13<br>78.66<br>50.69|0.305<br>0.764<br>0.219<br>0.428<br>0.378<br>0.376|8.70<br>4.89<br>17.02<br>46.60<br>51.03<br>43.18|


Table 8: Rule quality evaluation on WD15K for AnyBURL, RNNLogic, NeuralLP and LeSR. Here “HiConf”
means high-confidence rules: logical rules considered
to be highly reliable and with interpretability annotation
in the dataset.


![](/Users/ravenoak/Projects/github.com/ravenoak/autoresearch/docs/external_research_papers/arxiv.org-bib4llm/2501.01246v1/2501.01246v1.pdf-8-0.png)

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||
||||||||||||||||
||||||||||||||||


Figure 4: Numbers of learned logical rules of different
interpretability scores. The y-axis is in log 10.

with only a small proportion of rules being highconfidence. In contrast, NeuralLP, with only 1,544
rules, excels in interpretability with an RCS of
0.764, focusing on producing a compact set of
highly interpretable rules.
Our LeSR models, particularly LeSR GPT-4 and
LeSR GPT-3.5, show a more balanced performance,
both in terms of the number of rules learned and

their interpretability. LeSR GPT-4 stands out with
the highest HCR of 78.66% and a solid RCS of
0.378, achieving the highest Rule Quality Index
(RQI) of 51.03, demonstrating its ability to generate high-quality, interpretable rules. LeSR GPT-3.5
also performs well with a high HCR of 51.13%
and an RCS of 0.428, resulting in a strong RQI of
46.60.

Overall, the LeSR models strike a commendable balance between the quantity and quality of
rules, achieving higher RQI scores than other models, which tend to focus too much on generating
large rule sets (e.g., AnyBURL) or on producing a
smaller set of highly interpretable rules (e.g., NeuralLP). The LeSR models, especially GPT-4, effectively blance the number of rules learned and the
quality of rules learned.

**5** **Conclusion**

In this work, we present a novel framework that
improves the transparency and reliability of Knowledge Base Completion by combining LLMs with
symbolic reasoning. Our approach leverages the





9

linguistic capability and rule-generating ability of
LLMs in conjunction with the verifiable reasoning of rule-based approaches. Experimental results
show that our proposed method not only improves
accuracy but also provides generalizability across
diverse KB datasets. By bridging the gap between
the interpretability of rule-based reasoning and the
adaptability of LLMs, our work offers a potential
future direction on enhancing the reliability of KBC
while providing a transparent and scalable solution
for handling large-scale knowledge bases.

**References**

Ivana Balazevic, Carl Allen, and Timothy Hospedales.
[2019. TuckER: Tensor factorization for knowledge](https://doi.org/10.18653/v1/D19-1522)
[graph completion. In](https://doi.org/10.18653/v1/D19-1522) _Proceedings of the 2019 Con-_
_ference on Empirical Methods in Natural Language_
_Processing and the 9th International Joint Confer-_
_ence on Natural Language Processing (EMNLP-_
_IJCNLP)_, pages 5185–5194, Hong Kong, China. Association for Computational Linguistics.

Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko.
[2013. Translating embeddings for modeling multi-](https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf)
[relational data. In](https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf) _Advances in Neural Information_
_Processing Systems_, volume 26. Curran Associates,
Inc.

Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi.
[2019. COMET: Commonsense transformers for auto-](https://doi.org/10.18653/v1/P19-1470)
[matic knowledge graph construction. In](https://doi.org/10.18653/v1/P19-1470) _Proceedings_
_of the 57th Annual Meeting of the Association for_
_Computational Linguistics_, pages 4762–4779, Florence, Italy. Association for Computational Linguistics.

Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,
Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy,
[Alex Smola, and Andrew McCallum. 2018. Go for](https://arxiv.org/abs/1711.05851)
[a walk and arrive at the answer: Reasoning over](https://arxiv.org/abs/1711.05851)
[paths in knowledge bases using reinforcement learn-](https://arxiv.org/abs/1711.05851)
[ing.](https://arxiv.org/abs/1711.05851) _Preprint_, arXiv:1711.05851.

Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,
[and Sebastian Riedel. 2018. Convolutional 2d knowl-](https://doi.org/10.1609/aaai.v32i1.11573)
[edge graph embeddings.](https://doi.org/10.1609/aaai.v32i1.11573) _Proceedings of the AAAI_
_Conference on Artificial Intelligence_, 32(1).

Luis Antonio Galárraga, Christina Teflioudi, Katja Hose,
[and Fabian Suchanek. 2013. Amie: association rule](https://doi.org/10.1145/2488388.2488425)
[mining under incomplete evidence in ontological](https://doi.org/10.1145/2488388.2488425)
[knowledge bases. In](https://doi.org/10.1145/2488388.2488425) _Proceedings of the 22nd In-_
_ternational Conference on World Wide Web_, WWW
’13, page 413–422, New York, NY, USA. Association
for Computing Machinery.

Michael Galkin, Zhaocheng Zhu, Hongyu Ren, and
[Jian Tang. 2022. Inductive logical query answer-](https://proceedings.neurips.cc/paper_files/paper/2022/file/6246e04dcf42baf7c71e3a65d3d93b55-Paper-Conference.pdf)
[ing in knowledge graphs. In](https://proceedings.neurips.cc/paper_files/paper/2022/file/6246e04dcf42baf7c71e3a65d3d93b55-Paper-Conference.pdf) _Advances in Neural_


_Information Processing Systems_, volume 35, pages
15230–15243. Curran Associates, Inc.

Zhongmou He, Jing Zhu, Shengyi Qian, Joyce Chai,
[and Danai Koutra. 2024. Linkgpt: Teaching large](https://arxiv.org/abs/2406.04640)
[language models to predict missing links.](https://arxiv.org/abs/2406.04640) _Preprint_,
arXiv:2406.04640.

Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras,
Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and
[Yejin Choi. 2021. (comet-) atomic 2020: On sym-](https://doi.org/10.1609/aaai.v35i7.16792)
[bolic and neural commonsense knowledge graphs.](https://doi.org/10.1609/aaai.v35i7.16792)
_Proceedings of the AAAI Conference on Artificial_
_Intelligence_, 35(7):6384–6392.

Tushar Khot, Sriraam Natarajan, Kristian Kersting, and
[Jude Shavlik. 2011. Learning markov logic networks](https://doi.org/10.1109/ICDM.2011.87)
[via functional gradient boosting. In](https://doi.org/10.1109/ICDM.2011.87) _2011 IEEE 11th_
_International Conference on Data Mining_, pages 320–
329.

[Stanley Kok and Pedro Domingos. 2007. Statistical](https://doi.org/10.1145/1273496.1273551)
[predicate invention. In](https://doi.org/10.1145/1273496.1273551) _Proceedings of the 24th In-_
_ternational Conference on Machine Learning_, ICML
’07, page 433–440, New York, NY, USA. Association
for Computing Machinery.

Xiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel.
[2016. Commonsense knowledge base completion.](https://doi.org/10.18653/v1/P16-1137)
In _Proceedings of the 54th Annual Meeting of the_
_Association for Computational Linguistics (Volume_
_1: Long Papers)_, pages 1445–1455, Berlin, Germany.
Association for Computational Linguistics.

Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang
[Ren. 2019. KagNet: Knowledge-aware graph net-](https://doi.org/10.18653/v1/D19-1282)
[works for commonsense reasoning. In](https://doi.org/10.18653/v1/D19-1282) _Proceedings_
_of the 2019 Conference on Empirical Methods in Nat-_
_ural Language Processing and the 9th International_
_Joint Conference on Natural Language Processing_
_(EMNLP-IJCNLP)_, pages 2829–2839, Hong Kong,
China. Association for Computational Linguistics.

Xi Victoria Lin, Richard Socher, and Caiming Xiong.
[2018. Multi-hop knowledge graph reasoning with](https://doi.org/10.18653/v1/D18-1362)
[reward shaping. In](https://doi.org/10.18653/v1/D18-1362) _Proceedings of the 2018 Con-_
_ference on Empirical Methods in Natural Language_
_Processing_, pages 3243–3253, Brussels, Belgium.
Association for Computational Linguistics.

Xin Lv, Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu,
[Yichi Zhang, and Zelin Dai. 2021. Is multi-hop rea-](https://doi.org/10.18653/v1/2021.emnlp-main.700)
[soning really explainable? towards benchmarking](https://doi.org/10.18653/v1/2021.emnlp-main.700)
[reasoning interpretability.](https://doi.org/10.18653/v1/2021.emnlp-main.700) In _Proceedings of the_
_2021 Conference on Empirical Methods in Natural_
_Language Processing_, pages 8899–8911, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.

Christian Meilicke, Melisachew Wudage Chekol, Daniel
[Ruffinelli, and Heiner Stuckenschmidt. 2019. Any-](https://doi.org/10.24963/ijcai.2019/435)
[time bottom-up rule learning for knowledge graph](https://doi.org/10.24963/ijcai.2019/435)
[completion. In](https://doi.org/10.24963/ijcai.2019/435) _Proceedings of the Twenty-Eighth_
_International Joint Conference on Artificial Intel-_
_ligence, IJCAI-19_, pages 3137–3143. International
Joint Conferences on Artificial Intelligence Organization.


10

Pasquale Minervini, Sebastian Riedel, Pontus Stenetorp,
Edward Grefenstette, and Tim Rocktäschel. 2020.
[Learning reasoning strategies in end-to-end differen-](https://arxiv.org/abs/2007.06477)
[tiable proving.](https://arxiv.org/abs/2007.06477) _Preprint_, arXiv:2007.06477.

Sriraam Natarajan, Tushar Khot, Kristian Kersting,
[Bernd Gutmann, and Jude W. Shavlik. 2010. Boost-](https://api.semanticscholar.org/CorpusID:15889400)
[ing relational dependency networks. In](https://api.semanticscholar.org/CorpusID:15889400) _International_
_Conference on Inductive Logic Programming_ .

Maximilian Nickel, Kevin Murphy, Volker Tresp, and
[Evgeniy Gabrilovich. 2016. A review of relational](https://doi.org/10.1109/JPROC.2015.2483592)
[machine learning for knowledge graphs.](https://doi.org/10.1109/JPROC.2015.2483592) _Proceedings_
_of the IEEE_, 104(1):11–33.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In _Proceedings of_
_the 28th International Conference on Machine Learn-_
_ing (ICML-11)_, ICML ’11, pages 809–816, New
York, NY, USA. ACM.

[Heiko Paulheim. 2017. Knowledge graph refinement:](https://doi.org/10.3233/SW-160218)
[A survey of approaches and evaluation methods.](https://doi.org/10.3233/SW-160218) _Se-_
_mantic web_, 8(3):489–508.

Meng Qu, Junkun Chen, Louis-Pascal Xhonneux,
Yoshua Bengio, and Jian Tang. 2021. [Rnnlogic:](https://arxiv.org/abs/2010.04029)
[Learning logic rules for reasoning on knowledge](https://arxiv.org/abs/2010.04029)
[graphs.](https://arxiv.org/abs/2010.04029) _Preprint_, arXiv:2010.04029.

Nils Reimers and Iryna Gurevych. 2019. [Sentence-](https://doi.org/10.18653/v1/D19-1410)
[BERT: Sentence embeddings using Siamese BERT-](https://doi.org/10.18653/v1/D19-1410)
[networks. In](https://doi.org/10.18653/v1/D19-1410) _Proceedings of the 2019 Conference on_
_Empirical Methods in Natural Language Processing_
_and the 9th International Joint Conference on Natu-_
_ral Language Processing (EMNLP-IJCNLP)_, pages
3982–3992, Hong Kong, China. Association for Computational Linguistics.

[Tim Rocktäschel and Sebastian Riedel. 2017. End-to-](https://proceedings.neurips.cc/paper_files/paper/2017/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf)
[end differentiable proving. In](https://proceedings.neurips.cc/paper_files/paper/2017/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf) _Advances in Neural_
_Information Processing Systems_, volume 30. Curran
Associates, Inc.

Ali Sadeghian, Mohammadreza Armandpour, Patrick
[Ding, and Daisy Zhe Wang. 2019. Drum: End-to-end](https://proceedings.neurips.cc/paper_files/paper/2019/file/0c72cb7ee1512f800abe27823a792d03-Paper.pdf)
[differentiable rule mining on knowledge graphs. In](https://proceedings.neurips.cc/paper_files/paper/2019/file/0c72cb7ee1512f800abe27823a792d03-Paper.pdf)
_Advances in Neural Information Processing Systems_,
volume 32. Curran Associates, Inc.

Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin,
Brendan Roof, Noah A. Smith, and Yejin Choi. 2019.
[Atomic: An atlas of machine commonsense for if-](https://doi.org/10.1609/aaai.v33i01.33013027)
[then reasoning.](https://doi.org/10.1609/aaai.v33i01.33013027) _Proceedings of the AAAI Conference_
_on Artificial Intelligence_, 33(01):3027–3035.

Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem,
Rianne van den Berg, Ivan Titov, and Max Welling.
[2018. Modeling relational data with graph convolu-](https://doi.org/10.1007/978-3-319-93417-4_38)
[tional networks. In](https://doi.org/10.1007/978-3-319-93417-4_38) _The Semantic Web: 15th Inter-_
_national Conference, ESWC 2018, Heraklion, Crete,_
_Greece, June 3–7, 2018, Proceedings_, page 593–607,
Berlin, Heidelberg. Springer-Verlag.


Richard Socher, Danqi Chen, Christopher D Manning,
[and Andrew Ng. 2013. Reasoning with neural tensor](https://proceedings.neurips.cc/paper_files/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf)
[networks for knowledge base completion. In](https://proceedings.neurips.cc/paper_files/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf) _Ad-_
_vances in Neural Information Processing Systems_,
volume 26. Curran Associates, Inc.

Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.

[Conceptnet 5.5: An open multilingual graph of gen-](https://doi.org/10.1609/aaai.v31i1.11164)
[eral knowledge.](https://doi.org/10.1609/aaai.v31i1.11164) _Proceedings of the AAAI Conference_
_on Artificial Intelligence_, 31(1).

Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian
[Tang. 2019. Rotate: Knowledge graph embedding](https://arxiv.org/abs/1902.10197)
[by relational rotation in complex space.](https://arxiv.org/abs/1902.10197) _Preprint_,
arXiv:1902.10197.

[Kristina Toutanova and Danqi Chen. 2015. Observed](https://doi.org/10.18653/v1/W15-4007)
[versus latent features for knowledge base and text](https://doi.org/10.18653/v1/W15-4007)
[inference. In](https://doi.org/10.18653/v1/W15-4007) _Proceedings of the 3rd Workshop on_
_Continuous Vector Space Models and their Composi-_
_tionality_, pages 57–66, Beijing, China. Association
for Computational Linguistics.

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Eric
[Gaussier, and Guillaume Bouchard. 2016. Complex](https://proceedings.mlr.press/v48/trouillon16.html)
[embeddings for simple link prediction. In](https://proceedings.mlr.press/v48/trouillon16.html) _Proceed-_
_ings of The 33rd International Conference on Ma-_
_chine Learning_, volume 48 of _Proceedings of Ma-_
_chine Learning Research_, pages 2071–2080, New
York, New York, USA. PMLR.

Blerta Veseli, Sneha Singhania, Simon Razniewski,
[and Gerhard Weikum. 2023. Evaluating language](https://doi.org/10.1007/978-3-031-33455-9_14)
[models for knowledge base completion. In](https://doi.org/10.1007/978-3-031-33455-9_14) _The Se-_
_mantic Web: 20th International Conference, ESWC_
_2023, Hersonissos, Crete, Greece, May 28–June 1,_
_2023, Proceedings_, page 227–243, Berlin, Heidelberg. Springer-Verlag.

Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.
[2017. Knowledge graph embedding: A survey of](https://doi.org/10.1109/TKDE.2017.2754499)
[approaches and applications.](https://doi.org/10.1109/TKDE.2017.2754499) _IEEE Transactions_
_on Knowledge and Data Engineering_, 29(12):2724–
2743.

[Zhichun Wang and Juanzi Li. 2015. Rdf2rules: Learn-](https://arxiv.org/abs/1512.07734)
[ing rules from rdf knowledge bases by mining fre-](https://arxiv.org/abs/1512.07734)
[quent predicate cycles.](https://arxiv.org/abs/1512.07734) _Preprint_, arXiv:1512.07734.

Hong Wu, Zhe Wang, Kewen Wang, Pouya Ghias[nezhad Omran, and Jiangmeng Li. 2023. Rule Learn-](https://doi.org/10.4230/TGDK.1.1.7)
[ing over Knowledge Graphs: A Review.](https://doi.org/10.4230/TGDK.1.1.7) _Transac-_
_tions on Graph Data and Knowledge_, 1(1):7:1–7:23.

Wenhan Xiong, Thien Hoang, and William Yang Wang.
[2017. DeepPath: A reinforcement learning method](https://doi.org/10.18653/v1/D17-1060)
[for knowledge graph reasoning. In](https://doi.org/10.18653/v1/D17-1060) _Proceedings of_
_the 2017 Conference on Empirical Methods in Nat-_
_ural Language Processing_, pages 564–573, Copenhagen, Denmark. Association for Computational Linguistics.

Bishan Yang, Wen tau Yih, Xiaodong He, Jianfeng Gao,
[and Li Deng. 2015. Embedding entities and rela-](https://arxiv.org/abs/1412.6575)
[tions for learning and inference in knowledge bases.](https://arxiv.org/abs/1412.6575)
_Preprint_, arXiv:1412.6575.


11

Fan Yang, Zhilin Yang, and William W Cohen. 2017.

[Differentiable learning of logical rules for knowledge](https://proceedings.neurips.cc/paper_files/paper/2017/file/0e55666a4ad822e0e34299df3591d979-Paper.pdf)
[base reasoning. In](https://proceedings.neurips.cc/paper_files/paper/2017/file/0e55666a4ad822e0e34299df3591d979-Paper.pdf) _Advances in Neural Information_
_Processing Systems_, volume 30. Curran Associates,
Inc.

[Yuan Yang and Le Song. 2020. Learn to explain effi-](https://arxiv.org/abs/1910.02481)
[ciently via neural logic inductive learning.](https://arxiv.org/abs/1910.02481) _Preprint_,
arXiv:1910.02481.

Liang Yao, Jiazhen Peng, Chengsheng Mao, and
Yuan Luo. 2024. [Exploring large language mod-](https://arxiv.org/abs/2308.13916)
[els for knowledge graph completion.](https://arxiv.org/abs/2308.13916) _Preprint_,
arXiv:2308.13916.

[Zefan Zeng, Qing Cheng, and Yuehang Si. 2023. Logi-](https://doi.org/10.3390/math11214486)
[cal rule-based knowledge graph reasoning: A com-](https://doi.org/10.3390/math11214486)
[prehensive survey.](https://doi.org/10.3390/math11214486) _Mathematics_, 11(21).

[Muhan Zhang and Yixin Chen. 2018. Link prediction](https://proceedings.neurips.cc/paper_files/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf)
[based on graph neural networks. In](https://proceedings.neurips.cc/paper_files/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf) _Advances in_
_Neural Information Processing Systems_, volume 31.
Curran Associates, Inc.

Wen Zhang, Jiaoyan Chen, Juan Li, Zezhong Xu, Jeff Z.
Pan, and Huajun Chen. 2022. [Knowledge graph](https://arxiv.org/abs/2202.07412)
[reasoning with logics and embeddings: Survey and](https://arxiv.org/abs/2202.07412)
[perspective.](https://arxiv.org/abs/2202.07412) _Preprint_, arXiv:2202.07412.

Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu,
[Wen Zhang, and Huajun Chen. 2024. Making large](https://arxiv.org/abs/2310.06671)
[language models perform better in knowledge graph](https://arxiv.org/abs/2310.06671)
[completion.](https://arxiv.org/abs/2310.06671) _Preprint_, arXiv:2310.06671.

Fangfang Zhou, Jiapeng Mi, Beiwen Zhang, Jingcheng
Shi, Ran Zhang, Xiaohui Chen, Ying Zhao, and Jian
[Zhang. 2023. Reliable knowledge graph fact predic-](https://doi.org/10.1186/s42492-023-00150-7)
[tion via reinforcement learning.](https://doi.org/10.1186/s42492-023-00150-7) _Visual Computing_
_for Industry, Biomedicine, and Art_, 6.

**A** **Baseline Implementations**

For the baseline models, we utilize the official implementations provided by the authors. Specifically, for RotatE [3], NeuralLP [4], DRUM [5] and AnyBURL [6], we employed the respective official codebases and ran the models using the best hyperparameters reported by the authors. For the RNNLogic setup, we combined the default hyperparameters provided by RNNLogic [7] with the best RotatE
configuration from the RotatE models. Lastly, for
RGCN, we re-implemented the model in PyTorch
based on the original TensorFlow implementation [8]

and executed the models using the best hyperparameters reported by the authors.

3 [https://github.com/DeepGraphLearning/](https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding)
[KnowledgeGraphEmbedding](https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding)

4 [https://github.com/fanyangxyz/Neural-LP](https://github.com/fanyangxyz/Neural-LP)
5 [https://github.com/alisadeghian/DRUM](https://github.com/alisadeghian/DRUM)
6 [https://web.informatik.uni-mannheim.de/](https://web.informatik.uni-mannheim.de/AnyBURL)
[AnyBURL](https://web.informatik.uni-mannheim.de/AnyBURL)

7 [https://github.com/DeepGraphLearning/RNNLogic](https://github.com/DeepGraphLearning/RNNLogic)
8 [https://github.com/tkipf/relational-gcn](https://github.com/tkipf/relational-gcn)


**B** **Example Rules Learned**

Here we present some logical rules learned by the
Rule Reasoner, followed by their rule significance
score in percentage.

**Relation: “separated from” (** _α_ = 0 _._ 980 **)**

  - “IF (A, shares border with, B) AND (B, separated from, C) THEN (A, separated from, C)”
(0 _._ 32%)

  - “IF (A, country, B) AND (B, shares border with, C) THEN (A, separated from, C)”
(0 _._ 43%)

  - “IF (A, country, B) AND (B, diplomatic relation, C) AND (C, shares border with, D)
THEN (A, separated from, D)” (25 _._ 66%)

  - “IF (A, country, B) AND (B, shares border
with, C) AND (C, diplomatic relation, D)
THEN (A, separated from, D)” (0 _._ 25%)

  - “IF (A, diplomatic relation, B) AND (B,
country, C) THEN (A, separated from, C)”
(0 _._ 43%)

  - “IF (A, shares border with, B) AND (B, shares
border with, C) THEN (A, separated from, C)”
(72 _._ 48%)

  - “IF (A, shares border with, B) AND (B,
country, C) THEN (A, separated from, C)”
(0 _._ 43%)

**Relation: “performer” (** _α_ = 0 _._ 878 **)**

  - “IF (X, creator, Y) AND (Y, professional or
sports partner, Z) THEN (X, performer, Z)”
(0 _._ 21%)

  - “IF (A, present in work, B) AND (B, cast member, C) THEN (A, performer, C)” (99 _._ 80%)

**Relation: “language used” (** _α_ = 0 _._ 707 **)**

  - “IF (A, diplomatic relation, B) AND (B, official language, C) THEN (A, language used,
C)” (99 _._ 29%)

**C** **LLM Prompt Templates**

The bolded parts are places to insert relevant subgraphs and target triplets.

**Prompt Template for Rule Proposing** The following template is designed to guide LLMs to generate logical rules that can deduce a specific target
fact ( _h, r, t_ ) based on a given knowledge subgraph.
The prompt provides example rules to illustrate the
format where logical operators are used to describe
how multiple conditions involving entities lead to
the deduction of a new triplet.


12

A knowledge subgraph describes relationships
between entities using a set of triplets. Each
triplet is written in the form of triplet (SUBJ,
REL, OBJ), which states that entity SUBJ is
of relation REL to entity OBJ.
A logic rule can be applied to known triplets
to deduce new ones. Each rule is written in

the form of a logical implication, which states
that if the conditions on the right-hand side are
satisfied, then the statement on the left-hand
side holds true. Here are some example rules
where A, B, C are entities:
IF (A, parent, B) AND NOT (A, father, B)
THEN (A, mother, B)
IF (A, father, B) OR (A, mother, B) THEN (A,
parent, B)
IF (A, mother, B) AND (A, sibling, C) THEN
(C, mother, B)
Now we have the following triplets: _**knowl-**_
_**edge subgraph**_
Please generate as many of the most important
logical rules based on the above knowledge
subgraph to deduce triplet _**target fact**_ . The
rules provide general logic implications instead of using specific entities. Return the
rules only without any explanations.

**Prompt Template for Direct KBC Inferring**
This prompt template is designed to instruct LLMs
to predict the most likely entity candidates that can
complete a given query ( _h, r,_ ?) query given a list
of triplets from its relevant knowledge subgraph.
We use the generated entity candidates to compute
Hit@K metrics for evaluation.

A knowledge subgraph describes relationships
between entities using a set of triplets. Each
triplet is written in the form of triplet (SUBJ,
REL, OBJ), which states that entity SUBJ is
of relation REL to entity OBJ.
Now we have the following triplets:
_**relevant knowledge subgraph**_
Please generate 10 most likely OBJ candidates
to complete _**query**_ . Return only the entity
candidates without any additional text.

**D** **Logic Rules by Traversal Structure**

The logical rule structures are categorized into three
main groups based on the complexity and traversal
structure of relations: two-node (0th-order), threenode (1st-order), and four-node (2nd-order) cases.
These categories encapsulate different relational


patterns, where relation direction and node connectivity play crucial roles in defining the logic.
Each logical rule structure can be represented using a matrix-based approach, where _M_ _r_ denotes
the matrix for ( _h, t_ ) pairs satisfying relation _r_ .
The computation of **C**, representing the satisfiability of the rule body, involves matrix multiplication for conjunction ( _M_ _r_ _i_ _M_ _r_ _j_ represents ( _h, t_ )
pairs so that there exists some _e ∈E_ so that
( _h, r_ _i_ _, e_ ) _,_ ( _e, r_ _j_ _, t_ ) _∈G_ ) and matrix transpose for
relation inversion ( _M_ _r_ _[⊤]_ [represents] [ (] _[h, t]_ [)] [ pairs so]
that ( _t, r, h_ ) _∈G_ ). Elementwise multiplication _◦_
is used to compute **A** by evaluating ( _h, t_ ) pairs
against the rule head.

**0th-order Structures** These two cases represent
simple rules involving two entities with direct or
inversed relations.
**Case 0-1** : ( _A, r_ _i_ _, B_ ) _−_ ( _A, r_ _j_ _, B_ ) with example rule _ϕ_ = “IF ( _A, place of birth, B_ )) THEN
( _A, country of citizenship, B_ )”:

**C** = _M_ _r_ _i_, and **A** = **C** _◦_ _M_ _r_ _j_

**Case 0-2** : ( _A, r_ _i_ _, B_ ) _−_ ( _B, r_ _j_ _, A_ ) with example rule _ϕ_ = “IF ( _A, award received, B_ ) THEN
( _B, winner, A_ )”:

**C** = _M_ _r_ _[⊤]_ _i_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _j_

**1st-order Structures** The following cases introduce a third entity node, adding complexity with
various chain structures that either converge or diverge, providing greater diversity in logical struc
tures.
**Case 1-1** : ( _A, r_ _i_ _, B_ ) _−_ ( _B, r_ _j_ _, C_ ) _−_ ( _A, r_ _k_ _, C_ )
with example rule _ϕ_ = “IF ( _A, genre, B_ ) AND
( _B, subclass of, C_ ) THEN ( _A, genre, C_ )”:

**C** = _M_ _r_ _i_ _M_ _r_ _j_, and **A** = **C** _◦_ _M_ _r_ _k_

**Case 1-2** : ( _B, r_ _i_ _, A_ ) _−_ ( _B, r_ _j_ _, C_ ) _−_ ( _A, r_ _k_ _, C_ )
with example rule _ϕ_ = “IF ( _B, composer, A_ )
AND ( _B,_ _country of_ _origin,_ _C_ ) THEN
( _A, country of citizenship, C_ )”:

**C** = _M_ _r_ _[⊤]_ _i_ _[M]_ _[r]_ _j_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _k_

**Case** **1-3** : ( _A, r_ _i_ _, B_ ) _−_ ( _C, r_ _j_ _, B_ ) _−_
( _A, r_ _k_ _, C_ ) with example rule _ϕ_ = “IF
( _A,_ _nominated_ _for,_ _B_ ) AND
( _C,_ _award_ _received,_ _B_ ) THEN
( _A, performer, C_ )”:

**C** = _M_ _r_ _i_ _M_ _r_ _[⊤]_ _j_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _k_


13

**Case 1-4** : ( _B, r_ _i_ _, A_ ) _−_ ( _C, r_ _j_ _, B_ ) _−_ ( _A, r_ _k_ _, C_ )
with example rule _ϕ_ = “IF ( _A, owner of, B_ )
AND ( _B, parent organization, C_ ) THEN
( _C, owned by, A_ )”:

**C** = _M_ _r_ _[⊤]_ _i_ _[M]_ _r_ _[⊤]_ _j_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _k_

**2nd-order Structures** These four-node struc
tures exhibit the highest structural diversity. Eight
distinct patterns include sequences of relations that
form complex structures that cannot be easily captured using simple chain and relation inversion.
**Case** **2-1** : ( _A, r_ _i_ _, B_ ) _−_ ( _B, r_ _j_ _, C_ ) _−_
( _C, r_ _k_ _, D_ ) _−_ ( _A, r_ _l_ _, D_ ) with example rule _ϕ_ = “IF
( _A, twinned administrative body, B_ ) AND
( _B, twinned administrative body, C_ ) AND
( _C, country, D_ ) THEN ( _A, country, D_ )”:

**C** = _M_ _r_ _i_ _M_ _r_ _j_ _M_ _r_ _k_, and **A** = **C** _◦_ _M_ _r_ _l_

**Case** **2-2** : ( _A, r_ _i_ _, B_ ) _−_ ( _B, r_ _j_ _, C_ ) _−_
( _D, r_ _k_ _, C_ ) _−_ ( _A, r_ _l_ _, D_ ) with example rule
_ϕ_ = “IF ( _A, diplomatic relation, B_ ) AND
( _B, continent, C_ ) AND ( _D, continent, C_ )
THEN ( _A, shares border with, D_ )”:

**C** = _M_ _r_ _i_ _M_ _r_ _j_ _M_ _r_ _[⊤]_ _k_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _l_

**Case** **2-3** : ( _A, r_ _i_ _, B_ ) _−_ ( _C, r_ _j_ _, B_ ) _−_
( _C, r_ _k_ _, D_ ) _−_ ( _A, r_ _l_ _, D_ ) with example rule
_ϕ_ = “IF ( _A,_ _country_ _for_ _sport,_ _B_ )
AND ( _C,_ _country_ _of_ _citizenship,_ _B_ )
AND ( _C,_ _participant_ _of,_ _D_ ) THEN
( _A, participant of, D_ )”:

**C** = _M_ _r_ _i_ _M_ _r_ _[⊤]_ _j_ _[M]_ _[r]_ _k_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _l_

**Case** **2-4** : ( _B, r_ _i_ _, A_ ) _−_ ( _B, r_ _j_ _, C_ ) _−_
( _C, r_ _k_ _, D_ ) _−_ ( _A, r_ _l_ _, D_ ) with example

“
rule _ϕ_ = IF ( _A,_ _country,_ _B_ ) AND
( _A,_ _shares_ _border_ _with,_ _C_ ) AND
( _C, located on terrain feature, D_ ) THEN
( _B, located on terrain feature, D_ )”:

**C** = _M_ _r_ _[⊤]_ _i_ _[M]_ _[r]_ _j_ _[M]_ _[r]_ _k_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _l_

**Case** **2-5** : ( _A, r_ _i_ _, B_ ) _−_ ( _C, r_ _j_ _, B_ ) _−_
( _D, r_ _k_ _, C_ ) _−_ ( _A, r_ _l_ _, D_ ) with example rule

“
_ϕ_ = IF ( _A,_ _participant,_ _B_ ) AND
( _C,_ _diplomatic_ _relation,_ _B_ ) AND
( _D,_ _diplomatic_ _relation,_ _C_ ) THEN
( _A, participant, D_ )”:

**C** = _M_ _r_ _i_ _M_ _r_ _[⊤]_ _j_ _[M]_ _r_ _[⊤]_ _k_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _l_


**Case** **2-6** : ( _B, r_ _i_ _, A_ ) _−_ ( _B, r_ _j_ _, C_ ) _−_
( _D, r_ _k_ _, C_ ) _−_ ( _A, r_ _l_ _, D_ ) with example rule
_ϕ_ = “IF ( _B,_ _characters,_ _A_ ) AND
( _B, cast member, C_ ) AND ( _D, performer, C_ )
THEN ( _A, partner, D_ )”:

**C** = _M_ _r_ _[⊤]_ _i_ _[M]_ _[r]_ _j_ _[M]_ _r_ _[⊤]_ _k_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _l_

**Case** **2-7** : ( _B, r_ _i_ _, A_ ) _−_ ( _C, r_ _j_ _, B_ ) _−_
( _C, r_ _k_ _, D_ ) _−_ ( _A, r_ _l_ _, D_ ) with example rule
_ϕ_ = “IF ( _B,_ _diplomatic_ _relation,_ _A_ )
AND ( _C,_ _diplomatic_ _relation,_ _B_ ) AND
( _C, currency, D_ ) THEN ( _A, currency, D_ )”:

**C** = _M_ _r_ _[⊤]_ _i_ _[M]_ _r_ _[⊤]_ _j_ _[M]_ _[r]_ _k_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _l_

**Case** **2-8** : ( _B, r_ _i_ _, A_ ) _−_ ( _C, r_ _j_ _, B_ ) _−_
( _D, r_ _k_ _, C_ ) _−_ ( _A, r_ _l_ _, D_ ) with example rule
_ϕ_ = “IF ( _B, diplomatic relation, A_ ) AND
( _C, country, B_ ) AND ( _D, educated at, C_ )
THEN ( _A, head of government, D_ )”:

**C** = _M_ _r_ _[⊤]_ _i_ _[M]_ _r_ _[⊤]_ _j_ _[M]_ _r_ _[⊤]_ _k_ [, and] **[ A]** [ =] **[ C]** _[ ◦]_ _[M]_ _[r]_ _l_


14

