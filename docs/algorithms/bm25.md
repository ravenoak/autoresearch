# BM25 Ranking Formula

The BM25 ranking function scores a document *D* for query *Q* as:

\[
score(D, Q) = \sum_{q_i \in Q} IDF(q_i)
  \cdot \frac{f(q_i, D) (k_1 + 1)}
         {f(q_i, D) + k_1 \left(1 - b + b \cdot \frac{|D|}{avgdl}\right)}
\]

where:

- `f(q_i, D)` is the term frequency of `q_i` in `D`
- `|D|` is the length of the document in tokens
- `avgdl` is the average document length in the corpus
- `k1` and `b` are hyperparameters, typically `k1` in `[1.2, 2.0]` and
  `b` around `0.75`
- `IDF(q_i) = \log\left(\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} + 1\right)`
  with `N` documents and `n(q_i)` containing `q_i`

Autoresearch uses the `rank-bm25` Python implementation and normalizes
scores by scaling them into the `[0, 1]` range. Each score is divided by the
maximum so BM25 values can be combined with other ranking strategies.

With a weight vector `w = [w_bm, w_sem, w_cred]` on the simplex, the
final document score is

\[
s(d) = w_{bm} b(d) + w_{sem} m(d) + w_{cred} c(d)
\]

where `b(d)` is the normalized BM25 value, `m(d)` the semantic score,
and `c(d)` the source credibility.

## Correctness

BM25 approximates the log odds that a document is relevant for a query. For a
term \(q_i\), the inverse document frequency \(IDF(q_i)\) grows as the term
appears in fewer documents, so rare matches carry more weight. The saturation
term

\[
\frac{f(q_i, D) (k_1 + 1)}{f(q_i, D) + k_1 (1 - b + b |D| / avgdl)}
\]

is monotone in the term frequency \(f(q_i, D)\) and bounded above by
\(k_1 + 1\). Thus increasing matches never decreases the score. Summing over
query terms preserves the ordering induced by these relevance estimates.

## Complexity

With an inverted index, scoring a query of length \(|Q|\) touches postings
lists for its terms. The runtime is
\(O(\sum_{q_i \in Q} df(q_i))\) where \(df(q_i)\) is document frequency. In
the worst case \(df(q_i) = N\) for all terms, giving \(O(|Q| N)\) time. The
index requires \(O(N)\) space.

## References

- Stephen Robertson and Hugo Zaragoza. "The Probabilistic Relevance Framework:
  BM25 and Beyond." *Foundations and Trends in Information Retrieval* 3(4),
  2009.[^robertson]
- `rank-bm25` library[^rbm25]

[^robertson]: https://doi.org/10.1561/1500000019
[^rbm25]: https://github.com/dorianbrown/rank_bm25

## Simulation

Automated tests confirm bm25 behavior.

- [Spec](../specs/search.md)
- [Tests](../../tests/unit/test_bm25_scoring.py)
