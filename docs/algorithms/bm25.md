# BM25 Ranking Formula

The BM25 ranking function scores a document *D* for query *Q* as:

\[
score(D, Q) = \sum_{q_i \in Q} IDF(q_i)
  \cdot \frac{f(q_i, D) (k_1 + 1)}
         {f(q_i, D) + k_1 \left(1 - b + b \cdot \frac{|D|}{avgdl}\right)}
\]

where:

- `f(q_i, D)` is the term frequency of `q_i` in `D`
- `|D|` is the length of the document in tokens
- `avgdl` is the average document length in the corpus
- `k1` and `b` are hyperparameters, typically `k1` in `[1.2, 2.0]` and
  `b` around `0.75`
- `IDF(q_i) = \log\left(\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} + 1\right)`
  with `N` documents and `n(q_i)` containing `q_i`

Autoresearch uses the `rank-bm25` Python implementation and normalizes
scores by scaling them into the `[0, 1]` range. Each score is divided by the
maximum so BM25 values can be combined with other ranking strategies.

With a weight vector `w = [w_bm, w_sem, w_cred]` on the simplex, the
final document score is

\[
s(d) = w_{bm} b(d) + w_{sem} m(d) + w_{cred} c(d)
\]

where `b(d)` is the normalized BM25 value, `m(d)` the semantic score,
and `c(d)` the source credibility.

## References

- Stephen Robertson and Hugo Zaragoza. "The Probabilistic Relevance Framework:
  BM25 and Beyond." *Foundations and Trends in Information Retrieval* 3(4),
  2009.[^robertson]
- `rank-bm25` library[^rbm25]

[^robertson]: https://doi.org/10.1561/1500000019
[^rbm25]: https://github.com/dorianbrown/rank_bm25
