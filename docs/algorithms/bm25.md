# BM25 Ranking Formula

The BM25 ranking function scores a document *D* for query *Q* as:

\[
score(D, Q) = \sum_{q_i \in Q} IDF(q_i)
  \cdot \frac{f(q_i, D) (k_1 + 1)}
         {f(q_i, D) + k_1 \left(1 - b + b \cdot \frac{|D|}{avgdl}\right)}
\]

where:

- `f(q_i, D)` is the term frequency of `q_i` in `D`
- `|D|` is the length of the document in tokens
- `avgdl` is the average document length in the corpus
- `k1` and `b` are hyperparameters, typically `k1` in `[1.2, 2.0]` and
  `b` around `0.75`

Autoresearch uses the `rank-bm25` Python implementation and normalizes
scores by scaling them into the `[0, 1]` range. Each score is divided by the
maximum so BM25 values can be combined with other ranking strategies.

## References

- Stephen Robertson and Hugo Zaragoza. "The Probabilistic Relevance Framework:
  BM25 and Beyond." *Foundations and Trends in Information Retrieval* 3(4),
  2009.[^robertson]
- `rank-bm25` library[^rbm25]

[^robertson]: https://doi.org/10.1561/1500000019
[^rbm25]: https://github.com/dorianbrown/rank_bm25
