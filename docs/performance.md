# Performance Monitoring

Autoresearch includes tools to inspect system performance and resource usage.

## CPU and Memory Tracking

Use the following command to record CPU and memory statistics:

```bash
autoresearch monitor resources --duration 10
```

This collects metrics for ten seconds and displays them in a table. The
duration can be adjusted as needed.

You can also run `autoresearch monitor` to view a live stream of basic metrics.

## GPU Monitoring

If your system has NVIDIA GPUs, Autoresearch will attempt to collect GPU
utilization and memory usage. Metrics are gathered using `pynvml` when
available or by invoking `nvidia-smi`. When neither is present the GPU values
remain zero.

Running `autoresearch monitor resources` will therefore include ``GPU %`` and
``GPU MB`` columns when supported.

## Budget-Aware Model Routing

The orchestration layer now records per-agent token and latency samples so the
model router can steer high-usage roles toward cost-efficient backends without
violating service-level objectives. The router consults:

- the moving-average token spend for each agent,
- the 95th-percentile latency derived from recent executions, and
- model profiles that describe per-1K token pricing and latency targets.

When an agent consumes more than 80% of its allocated share of the global token
budget, the router downgrades to a cheaper profile that still satisfies the
agent's latency SLO. The decision is logged with before/after cost estimates so
operators can audit the savings. You can visualise the routing behaviour by
exporting the new `agent_token_samples` and `agent_timings` series via
`metrics.get_summary()` or the Prometheus counters exposed by the API gateway.

## Telemetry Dashboards

The metrics payload now includes latency percentiles per agent role alongside
aggregate cost estimates derived from the routing profiles. Dashboards should
plot the following series to track performance regressions:

- `agent_latency_p95_ms`: 95th-percentile latency per agent, surfaced through
  the orchestration summary payload.
- `agent_avg_tokens`: moving-average token consumption emitted as
  ``avg_tokens_per_call`` in the orchestration logs.
- `model_routing_cost_savings`: difference between the baseline and routed cost
  stored in the log event `Budget router selecting cost-efficient model`.

These signals make it easy to confirm that cost savings materialise without
raising the latency envelope for latency-sensitive agents.

## Distributed Coordination Benchmarks

ResourceMonitor captured CPU and memory usage while coordinating a simple
CPU-bound task across multiple processes. Average CPU rose from roughly 0%
with one node to about 30% with two nodes and 40% with four nodes. Memory
remained near 45â€“49 MB. These measurements were generated by
`tests/analysis/distributed_coordination_analysis.py`.

## Token Usage Heuristics

The orchestration metrics module provides helpers to automatically compress
prompts and adjust token budgets. After each cycle the orchestrator uses
`suggest_token_budget` to expand or shrink the configured budget. The heuristic
tracks both the overall token usage and per-agent historical averages so the
budget gradually converges toward typical usage without starving any agent. It
applies expansion when usage spikes and contracts when cycles run lean, never
allowing the budget to drop below one token. `compress_prompt_if_needed`
likewise tracks prompt lengths and lowers its compression threshold when the
average length exceeds the available budget. This causes later prompts to be
compressed earlier if recent prompts were long. If a prompt still exceeds the
budget after compression, a summarization step can be supplied to
``compress_prompt`` to further reduce the text. This adaptive behaviour helps
prevent runaway token consumption.

See the Token Budget Helpers specification for the precise expected
behaviour of these algorithms and the accompanying unit tests.

When a token budget is set, the orchestrator applies this compression step
inside ``_capture_token_usage`` before passing prompts to the LLM adapter.
Any remaining excess is trimmed by the adapter so prompts never exceed the
configured budget.

## Connection Pooling

HTTP requests to LLM and search backends reuse shared `requests.Session`
instances. The pool size for LLMs is controlled by `llm_pool_size` while search
backends use `http_pool_size`. When a session is created an `atexit` hook is
registered to close it automatically on program exit. Reusing sessions reduces
connection overhead during heavy query loads.

### Polars Metrics Analysis

When the `analysis` extra is installed you can transform collected metrics into
a Polars DataFrame:

```python
from autoresearch.data_analysis import metrics_dataframe
df = metrics_dataframe(metrics, polars_enabled=True)
print(df)
```

This provides convenient aggregation and export capabilities for further
analysis.

