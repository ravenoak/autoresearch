# Performance Monitoring

Autoresearch includes tools to inspect system performance and resource usage.

## CPU and Memory Tracking

Use the following command to record CPU and memory statistics:

```bash
autoresearch monitor resources --duration 10
```

This collects metrics for ten seconds and displays them in a table. The
duration can be adjusted as needed.

You can also run `autoresearch monitor` to view a live stream of basic metrics.

## GPU Monitoring

If your system has NVIDIA GPUs, Autoresearch will attempt to collect GPU
utilization and memory usage. Metrics are gathered using `pynvml` when
available or by invoking `nvidia-smi`. When neither is present the GPU values
remain zero.

Running `autoresearch monitor resources` will therefore include ``GPU %`` and
``GPU MB`` columns when supported.

## Distributed Coordination Benchmarks

ResourceMonitor captured CPU and memory usage while coordinating a simple
CPU-bound task across multiple processes. Average CPU rose from roughly 0%
with one node to about 30% with two nodes and 40% with four nodes. Memory
remained near 45â€“49 MB. These measurements were generated by
`tests/analysis/distributed_coordination_analysis.py`.

## Token Usage Heuristics

The orchestration metrics module provides helpers to automatically compress
prompts and adjust token budgets. After each cycle the orchestrator uses
`suggest_token_budget` to expand or shrink the configured budget. The heuristic
tracks both the overall token usage and per-agent historical averages so the
budget gradually converges toward typical usage without starving any agent. It
applies expansion when usage spikes and contracts when cycles run lean, never
allowing the budget to drop below one token. `compress_prompt_if_needed`
likewise tracks prompt lengths and lowers its compression threshold when the
average length exceeds the available budget. This causes later prompts to be
compressed earlier if recent prompts were long. If a prompt still exceeds the
budget after compression, a summarization step can be supplied to
``compress_prompt`` to further reduce the text. This adaptive behaviour helps
prevent runaway token consumption.

See the Token Budget Helpers specification for the precise expected
behaviour of these algorithms and the accompanying unit tests.

When a token budget is set, the orchestrator applies this compression step
inside ``_capture_token_usage`` before passing prompts to the LLM adapter.
Any remaining excess is trimmed by the adapter so prompts never exceed the
configured budget.

## Budget-Aware Model Routing

``OrchestrationMetrics`` now tracks token deltas and latency per agent role.
The new ``get_role_usage_snapshot`` helper provides average prompt tokens,
completion tokens, latency, and model frequency for each role. These
snapshots feed ``select_model_for_role`` in ``autoresearch.token_budget`` which
scores candidate models against optional latency and cost ceilings. The
heuristic defaults to the configured model when no candidates improve on the
baseline, otherwise it chooses the lowest-cost option that satisfies the
constraints. Routing decisions persist rationale strings so telemetry
dashboards can visualise cost savings and latency percentiles per role.

Each call to ``record_agent_timing`` logs role, model, latency, and token
usage, enabling Grafana or Prometheus boards to chart P50/P95 latency by role
alongside projected token spend. The telemetry summary emitted after every
cycle now includes a ``role_usage`` section that can be ingested by downstream
dashboards without extra aggregation.

## Shared Retrieval Cache Controls

External search requests now honour ``search.shared_retrieval_cache`` and
``search.parallel_backends`` toggles. When the cache toggle is enabled all
``Search`` instances reuse an in-memory retrieval cache on top of the existing
TinyDB persistence, preventing redundant HTTP calls once a backend has already
served a query. Clearing the persistent cache no longer forces repeated
network calls within the same run.

Parallelism can be disabled by setting ``search.parallel_backends`` to
``false`` which skips the thread pool entirely. This is useful when testing
rate-limited backends or benchmarking serial behaviour. The shared cache and
parallel toggles are surfaced in the runtime summary so observability stacks
can flag when the knobs flip between runs.

## Connection Pooling

HTTP requests to LLM and search backends reuse shared `requests.Session`
instances. The pool size for LLMs is controlled by `llm_pool_size` while search
backends use `http_pool_size`. When a session is created an `atexit` hook is
registered to close it automatically on program exit. Reusing sessions reduces
connection overhead during heavy query loads.

### Polars Metrics Analysis

When the `analysis` extra is installed you can transform collected metrics into
a Polars DataFrame:

```python
from autoresearch.data_analysis import metrics_dataframe
df = metrics_dataframe(metrics, polars_enabled=True)
print(df)
```

This provides convenient aggregation and export capabilities for further
analysis.

